{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequences\n",
    "\n",
    "### Pick **one** of these two topics.\n",
    "\n",
    "The lecture notebooks are adapted from the [official repository](https://github.com/fchollet/deep-learning-with-python-notebooks) and can be used as starter code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Time Series with RNNs\n",
    "\n",
    "\n",
    "1. Download and prepare the timeseries data from listings 10.1 to 10.7.\n",
    "    ```\n",
    "    # !wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
    "    # !unzip jena_climate_2009_2016.csv.zip\n",
    "    ```\n",
    "     \n",
    "2. Build an LSTM network, importing code from the first recurrent baseline section (DLWP, 10.2.5). \n",
    "\n",
    "3.  Experiment with different hyperparameters to improve the results of advanced RNN models in DLWP 10.4.\n",
    "\n",
    "4. Suggestions for experiments:\n",
    "\n",
    "    - Adjust the number of units in each recurrent layer;\n",
    "    - Try different learning rates;\n",
    "    - Substitute LSTM layers for GRU layers;\n",
    "    - Try adding a recurrent regularizer;\n",
    "    - Try bigger densely connected regressors on top of the recurrent base, or even a stack of dense layers;\n",
    "    - Try stacking more layers, like in listing 10.23, or make it bidirectional (listing 10.24);    \n",
    "    - Try and reproduce the non-recurrent results from DLWP 10.2.2 to 10.2.5 and compare those with what you have:\n",
    "      - The commonsense baseline (listing 10.9);\n",
    "      - The feedforward net (listing 10.10);\n",
    "      - The 1D convnet from DLWP p.291.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 2. Text classification with a Transformer\n",
    "\n",
    "1. Download the data and load it into datasets as is done in section 11.3.1 (there is a modified version of the dataset-building code in the lecture notebooks): \n",
    "    ```bash\n",
    "    !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    !tar -xf aclImdb_v1.tar.gz\n",
    "    !rm -r aclImdb/train/unsup\n",
    "    ```  \n",
    "       \n",
    "    Note that if you already downloaded this data for previous experiments, you can create a symlink:\n",
    "    ```bash\n",
    "    ln -s path/to/aclImdb [newName]\n",
    "    ```\n",
    "     \n",
    "2. Import the code for text classification with the Transformer Encoder from listings 11.21, 11.24 and 11.25 and train it.\n",
    "\n",
    "3. Experiment with different hyperparameters. How do the results compare with the first method we used in Topic 3 (you can find experiments in the lecture notebook `3-getting-started/04.1.classifying-movie-reviews-imdb.ipynb`)? \n",
    "\n",
    "4. Suggestions for experiments:\n",
    "\n",
    "    - Reproduce the steps from DLPW listings 11.12 and 11.17, and compare the performance of your bidirectional LSTM with the transformer you built above.\n",
    "    - Test pretrained embeddings: download the *GloVe* file as described in listing 11.18 and 11.19. The comment after listing 11.20 mentions that in this case, the *GloVe* embeddings do not make much of a difference. Can you reproduce this result? If you reduce the size of your dataset, do you notice a threshold at which the model pretrained embedding works better than the one trained from scratch?\n",
    "    - Reproduce the steps from DLWP section 11.3 (using the model-building utility from listing 11.5), and compare your results with what you have:\n",
    "        - Create a bag-of-word model (listing 11.3);\n",
    "        - Create a n-grams (n > 1) model (listings 11.7 and 11.8).\n",
    "    - Advanced: can you modify and encapsulate your Transformer model so that it is easier to change its architecture (number of layers, number of heads, etc.)?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Remember to run your best performing models on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
