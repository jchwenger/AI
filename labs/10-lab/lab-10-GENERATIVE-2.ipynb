{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4dfc59-2498-41fd-878b-8d9317ce3308",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. VAEs\n",
    "\n",
    "Experiment with VAE image generation. Instead of MNIST, it is possible to train on FashionMNIST, or the 200,000 celebrity portraits in the free [celebA dataset](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) (also on [Kaggle](https://www.kaggle.com/jessicali9530/celeba-dataset))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4729af28-9315-4624-b54f-090041cb69d1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. GANs\n",
    "\n",
    "Experiment with GANs image generation (beware: training can be quite long). The main difficulty with GANs is the instability of training (the Discriminator loss going to zero, and the Generator one blowing up, which has to do with the fact that the generating task is much harder than the discriminating one!). Chollet has two recommendations for this: lowering the Discriminator learning rate, increasing its dropout. Experiment with various learning rates and dropout rate, as well as batch sizes, so as to find a way to train your GAN in a stable manner.\n",
    "\n",
    "Another common strategy is to train the Generator more often than the Discriminator. The `train_step` function can be modified to include a loop that trains "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
