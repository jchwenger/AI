{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0a4a5b0-7a8b-4b5c-acf7-c11aac6a58d2",
   "metadata": {},
   "source": [
    "# Generative Deep Learning 2\n",
    "\n",
    "### Pick **one** of these topics.\n",
    "\n",
    "The lecture notebooks should be used as starter code: a good first step is to remove all that is not necessary for your enquiry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a303e10-e342-4e58-90fa-9f4a25ace691",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. VAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7423cad-0609-4403-91f5-ba90949c4b39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Experiment with VAE image generation.\n",
    "\n",
    "There are two main alleyways when approaching this particular model: training, and sampling.\n",
    "\n",
    "First, Instead of MNIST, it is possible to train on FashionMNIST, or the 200,000 celebrity portraits in the free [celebA dataset](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) (also on [Kaggle](https://www.kaggle.com/jessicali9530/celeba-dataset)). It might even be interesting to try and combine two datasets (say, MNIST + FashionMNIST, since they are already in the same format), and see the results?\n",
    "\n",
    "Another line of research is to explore and control sampling from the latent space. In the lecture code, Chollet creates a grid of numbers linearly distributed between -1 and 1. Another way of doing this is to sample from a Gaussian, and do interpolation using the [great circle distance](https://en.wikipedia.org/wiki/Great-circle_distance) (see [this thread](https://github.com/soumith/dcgan.torch/issues/14)). What it means is that you can start with two random vectors $z_1, z_2$, and use a function to create $n$ equally spaced vectors between them. Using $z_1, z_2$ and the intermediate vectors as seeds for your **Decoder**, you should be able to generate a series of images transitioning from the image corresponding to $z_1$ to the one corresponding to $z_2$.\n",
    "\n",
    "It is also possible to isolate specific, meaningful vectors, and use vector maths to manipulate images. The method is the following:\n",
    "- gather a number of pictures with and without a feature (in the canonical example, with/without smile), the more the better;\n",
    "- encode all these into latent vectors using the **Encoder**;\n",
    "- take the mean of the latent vectors for each group;\n",
    "- subtract one by the other to obtain the vector representing the feature!\n",
    "\n",
    "Finally, you can use that vector to modify an image, like so:\n",
    "- encode the image of choice using the **Encoder**;\n",
    "- subtract the feature vector from the image latent vector (literally `image_modified_z = image_z + feature_z`;\n",
    "- use the modified image latent vector and the **Decoder** to generate an image without the feature!\n",
    "\n",
    "[This notebook](https://github.com/eduhrami/Hands-On-Image-Generation-with-TensorFlow-2.0/blob/master/Chapter02/ch2_vae_faces.ipynb) implements some of these (see also [this file](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/03_vae/03_vae_faces/vae_utils.py))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f921f1-3723-48a0-a9ae-7a9e2e253580",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348849c8-9112-43a9-ad2b-f0273232acc2",
   "metadata": {},
   "source": [
    "Experiment with GANs image generation (beware: training can be quite long). The main difficulty with GANs is the instability of training (the Discriminator loss going to zero, and the Generator one blowing up, which has to do with the fact that the generating task is much harder than the discriminating one!). Chollet has two recommendations for this: \n",
    "- lowering the Discriminator learning rate,\n",
    "- increasing its dropout.\n",
    "\n",
    "Experiment with various learning rates and dropout rate, as well as batch sizes, so as to find a way to train your GAN in a stable manner. It is quite tricky to achieve the right balance, and building a system that would try various architectures automatically could be an interesting project!\n",
    "\n",
    "Another common strategy is to train the Generator more often than the Discriminator. The `train_step` function can be modified to include a loop that trains the Generator for a few steps!\n",
    "\n",
    "Note that just like VAEs, it is possible to do some interesting things with the (opaque) latent space. For instance, you could start with two random vectors, each of which creates a specific image when passed through the generator, and then use a function to generate [interpolated vectors between the two original ones](https://github.com/soumith/dcgan.torch/issues/14#issuecomment-525160139): if you then convert the series of vectors into images, you should see a smooth-ish transition between images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e3c811-7822-4286-89b2-f4bc3a599987",
   "metadata": {},
   "source": [
    "## 3. Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de808e-8e9a-434d-8ac6-9670c2ce3182",
   "metadata": {},
   "source": [
    "This section is still at a somewhat experimental stage! But one thing that could definitely be done is use the notebook to train models on different datasets (MNIST, FashionMNIST, CelebA, your own dataset).\n",
    "\n",
    "One relatively straightforward thing that could be quite nice to implement is to save each of the 1000 steps of the denoising process as an image, and combine those to make a video (ChatGPT will happily help you with the code to generate a GIF/video from a folder of images). Remember that it is not necessary always to plot a grid of images: you could also simplify the code to plot only one image at a time (using `plt` or `PIL`), which would allow you to save single images more easily.\n",
    "\n",
    "As mentioned at the end of the ddpm notebook, one thing that could be really interesting to learn to implement is a class-conditional U-Net, that also takes in a label, so that it is possible to guide the generation process that way! Here's a [tutorial](https://huggingface.co/learn/diffusion-course/en/unit2/3) that focusses on this (in PyTorch)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
