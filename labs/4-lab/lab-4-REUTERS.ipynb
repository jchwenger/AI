{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Reuters Dataset\n",
    "\n",
    "---\n",
    "\n",
    "### Colab Note\n",
    "\n",
    "Don't forget that you can link your notebook to your drive and save your work there. Then you can download and backup your models, reload them to keep training them, or upload datasets to your drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir('drive/My Drive/') # 'My Drive' is the default name of Google Drives\n",
    "    os.listdir()\n",
    "    \n",
    "# use os.chdir(\"my-directory\") # to change directory, and\n",
    "# os.listdir()                 # to list its contents\n",
    "# os.getcwd()                  # to get the name of the current directory\n",
    "# os.mkdir(\"my-new-dir\")       # to create a new directory\n",
    "# See: https://realpython.com/working-with-files-in-python/\n",
    "\n",
    "# You can also use bash commands directly, preceded by a bang\n",
    "# !ls\n",
    "# However, the following will *not* change the Python directory \n",
    "# the notebook points to (use os.chdir for that)!\n",
    "# !cd my-directory    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For reproducible results\n",
    "\n",
    "```python\n",
    "tf.random.set_seed(42) # can be any number\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "# load\n",
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.reuters.load_data(num_words=10000)\n",
    "\n",
    "# preprocess\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "one_hot_train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "one_hot_test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "# split training set into train & validation\n",
    "partial_x_train=x_train[1000:]\n",
    "partial_y_train=one_hot_train_labels[1000:]\n",
    "x_val=x_train[:1000]\n",
    "y_val=one_hot_train_labels[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(64, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(46, activation = 'softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check, how does our network perform before training (use `.evaluate` on `partial_x_train, partial_y_train`). Is the accuracy a value you would expect?\n",
    "\n",
    "Do you remember what other baselines were discussed in the lecture? You can import the code and test it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    partial_x_train,\n",
    "    partial_y_train,\n",
    "    epochs=20,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_val,y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise your results\n",
    "\n",
    "Import `matplotlib` code to visualise the result of your experiments (from lab 3 or the lectures).\n",
    "\n",
    "Can you think of ways to make the code easy to reuse (when running several experiments)?\n",
    "\n",
    "As usual, we are after the epoch where best *validation* result (accuracy) has been achieved. Can you think of a way to retrieve that optimal epoch programmatically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "- Plot training and validation loss and accuracy. Can you spot the optimal epoch (where the best *validation accuracy* has been achieved)?\n",
    "- Deliberately cause an information bottleneck by building a 64-4-46 network. Compare the validation accuracy with a 64-64-46 network;\n",
    "- Try using larger layers – 128 units and smaller layers – 32 units;\n",
    "- Try less and more hidden layers;\n",
    "- Tabulate your results / organise your experiments as much as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Conclusion\n",
    "\n",
    "Take your best network and train on **all the training data** (`x_train`, `one_hot_train_labels`), without a train/validation split, using the same hyperparameters (optimizer, learning rate, network size, etc.) as your best run, for the optimal number of epochs (looking at your best validation curves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this last model on the test set (`x_test, one_hot_test_labels`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Use your model (optional) \n",
    "\n",
    "Can you import the lecture code used to test the model on a newswire, and see if you agree with its prediction?\n",
    "\n",
    "AS shown in the lecture, the 46 topics are:\n",
    "```python\n",
    "# https://github.com/keras-team/keras/issues/12072\n",
    "# and here: https://martin-thoma.com/nlp-reuters/\n",
    "topics = [\n",
    "    'cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',\n",
    "    'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',\n",
    "    'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',\n",
    "    'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',\n",
    "    'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead'\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load models\n",
    "\n",
    "To save and load models locally, you can use [the high-level API](https://www.tensorflow.org/tutorials/keras/save_and_load):\n",
    "```python\n",
    "model.save(\"my_reuters_model.keras\")\n",
    "```\n",
    "Later one, to reload it, use:\n",
    "```python\n",
    "reloaded_model = tf.keras.models.load_model('my_reuters_model.keras')\n",
    "```\n",
    "\n",
    "It is also possible to save not just the model, but also the state of your optimiser, and every variable used during training, using the morer involved [checkpoints](https://www.tensorflow.org/guide/checkpoint#create_the_checkpoint_objects)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
