{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Deep Learning\n",
    "\n",
    "### Pick **one** of these topics.\n",
    "\n",
    "The lecture notebooks should be used as starter code: a good first step is to remove all that is not necessary for your enquiry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Text generation\n",
    "\n",
    "Try building different language models. There are tens of thousands of texts in [Project Gutenberg](https://www.gutenberg.org). There is small but wide range of texts (and other data) [here](https://introcs.cs.princeton.edu/java/data/). Are you able to train models that generate realistic texts?\n",
    "\n",
    "[Karpathy](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) noted in his blog post that language models also work with code. What happens if you try and train a language model on the [TensorFlow codebase](https://github.com/tensorflow/tensorflow)?\n",
    "\n",
    "Other common sampling strategies on top of `temperature` are `top_k`, sampling only from the most probable `k` logits, and `top_p`, nucleus sampling, sampling only from the most probable logits which taken together amount for `p` probability (see [here](https://huggingface.co/blog/how-to-generate) for a more complete discussion). Can you implement these generation functionalities, for instance importing them from [here](https://github.com/akanyaani/gpt-2-tensorflow2.0/blob/master/sample.py#L11) and [here](https://github.com/akanyaani/gpt-2-tensorflow2.0/blob/master/sample.py#L27)?\n",
    "\n",
    "Note that if you add special markers into your dataset, e.g. a special character, like `❡`, into the dataset, that allows you to control the behaviour of the trained net: if that character is used as an end point, splitting the dataset into parts, then in your generation loop that would allow you to stop generation, for instance, once that specific character has been produced. This logic is what fuels chatbots: if you have markers for the end of a reply/question by a user or by a bot, and you have a dataset of that, then you can train your model to output the 'end of reply' token after it has answered your question, which produces the _effect_ of conversation.\n",
    "\n",
    "Another approach is to think of what to do with a trained model: can you implement a typewriter effect when generating, perhaps adding a bit of randomness, using `time.sleep()`, to make it a bit irregular?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Deep Dream\n",
    "\n",
    "Produce dream-like images with DeepDream. Experiment with different input images – faces, landscapes etc. Experiment with lower and upper layers – the lower layers should produce more geometric patterns, the upper layers, more abstract (noses, feathers, eyes etc.). The layers that are maximised for activation are specified in the `layer_contributions` dictionary. Get the layer names from `model.summary()`. Finally, experiment with the number of octaves, loss limit and step size, and, if you feel curious, different pretrained models shipping with TF.\n",
    "\n",
    "One good approach for this could be to start by packaging the code in a way that makes it easier to run the code multiple times with different hyperparameters: encapsulate it all into a class or function(s). Then, one could even imagine running many different experiments, in a quasi scientific fashion, showing the results alongside the parameters, allowing the user to decide which parameters give the best results. Another way would be to build a web app (like Gradio, see the Style Transfer coursework example from last year) where it's possible for the user to test various hyperparameters. Another path would be to make it easier to handle a folder of images, rather than just one.\n",
    "\n",
    "As mentioned in the class, an advanced path of experimentation would be to modify `compute_loss` so that instead of computing the norm with the entire layer, one could select only one feature map in the net? The hope is that if we select a high-level feature map detecting a particular kind of object, we would see hallucinations resembling that object in the modified image? Another way of being more specific about the effects in the image is to change models: if the classifier model was trained, say, exclusively on buildings, or cats/dogs, that's the kinds of hallucinations you would see emerge in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Style Transfer\n",
    "\n",
    "Experiment with mixing a content image with a style image using Style Transfer. Experiment with different input images – faces, landscapes etc. Experiment with various layers, and how this affects the result (strength of the style, preservation of the content). The layers selected for the content are listed in `content_layer_names`, and in `style_layer_names` for style. As with Deep Dream, you can get the layer names from `model.summary()`. The key thing here is the tuning of the three loss weights, `total_variation_weight`, `style_weight` and `content_weight`. Beware the `nan` values you sometimes get depending on the values (still need to implement the proper checks)!  If you feel curious, different pretrained models shipping with TF.\n",
    "\n",
    "Just like for Deep Dream, one good approach for this could be to start by packaging the code in a way that makes it easier to run the code multiple times with different hyperparameters: encapsulate it all into a class or function(s). Then, one could even imagine running many different experiments, in a quasi scientific fashion. Similarly, it would be interesting to organise the code so that it's easy to run it on many different combinations of content and style images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
