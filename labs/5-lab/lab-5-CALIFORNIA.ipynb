{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6gb6X1pzNp1"
   },
   "source": [
    "# The California Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6gb6X1pzNp1"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6gb6X1pzNp1"
   },
   "source": [
    "### Colab Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget that you can link your notebook to your drive and save your work there. Then you can download and backup your models, reload them to keep training them, or upload datasets to your drive. \n",
    "\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir('drive/My Drive/') # 'My Drive' is the default name of Google Drives\n",
    "    os.listdir()\n",
    "    \n",
    "# use os.chdir(\"my-directory\") # to change directory, and\n",
    "# os.listdir()                 # to list its contents\n",
    "# os.getcwd()                  # to get the name of the current directory\n",
    "# os.mkdir(\"my-new-dir\")       # to create a new directory\n",
    "# See: https://realpython.com/working-with-files-in-python/\n",
    "\n",
    "# You can also use bash commands directly, preceded by a bang\n",
    "# !ls\n",
    "# However, the following will *not* change the Python directory \n",
    "# the notebook points to (use os.chdir for that)!\n",
    "# !cd my-directory\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxAMektPDyQf"
   },
   "source": [
    "### For reproducible results\n",
    "\n",
    "```python\n",
    "tf.random.set_seed(42) # can be any number\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_a-FRQSH84B"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ee-bCfzGkCq"
   },
   "source": [
    "## Method 1: Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGOAu29gD1Jn"
   },
   "outputs": [],
   "source": [
    "(train_data, train_targets), (test_data, test_targets) = keras.datasets.california_housing.load_data(version=\"small\")\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uy3GsqAbJTZV",
    "outputId": "3b3b732e-0ac5-4ade-b630-35fe5085aa14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500001.0\n",
      "74100.0\n"
     ]
    }
   ],
   "source": [
    "print(test_targets.max())\n",
    "print(test_targets.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Compute `mean` and `std` from `train_data`, and apply those to both `train_data` and `test_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQvMWYSdzNp8"
   },
   "outputs": [],
   "source": [
    "# compute the mean, and std from `train_data`\n",
    "# create an `x_train` that is normalised using those statistics\n",
    "\n",
    "# create an `x_test from `test_data`  using `mean` and `std` computed above\n",
    "\n",
    "# create `y_train` and `y_test` by dividing \n",
    "# `train_targets` and `test_targets` by 100_000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building & callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_5i-iAKFCKR"
   },
   "outputs": [],
   "source": [
    "def build_model(clear=True):\n",
    "    if clear:\n",
    "        tf.keras.backend.clear_session()\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.Input((x_train.shape[1],)))\n",
    "    model.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.compile(\n",
    "        optimizer='rmsprop',\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynWYwJg7FCKR"
   },
   "source": [
    "##### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynWYwJg7FCKR"
   },
   "source": [
    "Can you make the function above more modular? You could modify it so that it accepts arguments changing the architecture of the network, and other hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOCOzAjOzNqC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, epochs):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.epochs = epochs\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        c = ['|', '/', '-', '\\\\']\n",
    "        print(f\"\\r{c[epoch % 4]} epoch: {epoch+1}/{self.epochs}\", end=\"\")\n",
    "    def on_train_end(self, logs=None):\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The K-fold algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "2cubqyRFzNqG",
    "outputId": "166d813c-4428-462d-edac-143ceb09f7de"
   },
   "outputs": [],
   "source": [
    "K = 4\n",
    "num_val_samples = len(x_train) // K\n",
    "num_epochs = 100\n",
    "all_mae_histories = []\n",
    "\n",
    "for i in range(K):\n",
    "    print('processing fold', i)\n",
    "    \n",
    "    # Prepare the validation data: data from partition i\n",
    "    a, b =        # prepare indices\n",
    "    val_data =    # extract validation samples from x_train\n",
    "    val_targets = # extract validation targets from train_targets\n",
    "    \n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_x_train =    # extract partial train samples from x_train\n",
    "    partial_train_targets = # # extract partial train samples from  train_targets\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model = # use your function\n",
    "    \n",
    "    # Train the model using .fit()\n",
    "    # - train: partial_x_train, partial_train_targets\n",
    "    # - validation: val_data, val_targets\n",
    "    # - number of epochs, batch_size=1 for now\n",
    "    # - silent mode recommended, verbose=0\n",
    "    # collect the returned object in `history`\n",
    "\n",
    "    mae_history = # collect val_mae from history\n",
    "    # add mae_history to all_mae_histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ5Fp--7FCKS"
   },
   "source": [
    "### Visualise your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ5Fp--7FCKS"
   },
   "source": [
    "Can you think of a way to automate the visualisation once the training is done? This would mean encapsulating the plotting code into a function, and calling it once the K-fold loop is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dNFOVfdzNqJ"
   },
   "outputs": [],
   "source": [
    "average_mae_history = np.array(all_mae_histories).mean(axis=0)\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8I58FoiazNqM"
   },
   "outputs": [],
   "source": [
    "def smooth_curve(points, beta = 0.9):       # beta must be between 0 and 1!\n",
    "    smoothed_points = []\n",
    "    for current in points:\n",
    "        if smoothed_points:                 # (an nonempty list is 'True')\n",
    "            previous = smoothed_points[-1]  # the last appended point\n",
    "                                            # ↓ a weighted sum of previous & point, controlled by beta\n",
    "            smoothed_points.append(beta * previous + (1 - beta) * current)\n",
    "        else:\n",
    "            smoothed_points.append(current) # at the start, the list is empty, we just add the first point\n",
    "    return smoothed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IaEOcb8zNqO"
   },
   "outputs": [],
   "source": [
    "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oV50wyjqLd6v"
   },
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oV50wyjqLd6v"
   },
   "source": [
    "- Run k-fold validation on the California dataset;\n",
    "- Notice that the mini-batch size is set to 1. Experiment with different mini-batch sizes. What do you observe? Can you account for your observation?\n",
    "- Run a series of experiments to find the best model, like in previous labs.\n",
    "- A more advanced experiment could be to implement *iterated K-fold validation with shuffling*, as mentioned in the lecture.\n",
    "\n",
    "The obvious thing to be done here is to compare the results between the small random subset and the full dataset, if you were to train models on it (don't forget to split into train, validation and test sets when you work on the full data!). Varying the size of the test set could also be of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfwCUU0uIMEP"
   },
   "source": [
    "### 2. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfwCUU0uIMEP"
   },
   "source": [
    "Retrain the best model:\n",
    " - with the same hyperparameters\n",
    " - on the entire the training data (`train_data` and `train_targets`)\n",
    " - without validation data\n",
    " - up until the best epoch (lowest average MAE)\n",
    " - and then evaluate on the test data (`test_data`, `test_targets`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIW2NMUGIMEP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use your model to make predictions by selecting one data point in `x_test`, and compare the prediction to the equivalent price in `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPLEO30ODyQg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJMH6z9GLcgt"
   },
   "source": [
    "## Extra: Manual Download and data processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRmlPyG4IBS1"
   },
   "source": [
    "[California Housing](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html), original website. (Also available on [Kaggle](https://www.kaggle.com/datasets/camnugent/california-housing-prices).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRmlPyG4IBS1"
   },
   "source": [
    "### 1. Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRmlPyG4IBS1"
   },
   "source": [
    "The terminal commands to download it. (Add a `!` in front of them to use them from Jupyter or Colab.)\n",
    "\n",
    "```bash\n",
    "wget https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.tgz\n",
    "tar -xvf cal_housing.tgz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAeSnHizDyQg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the name of the file `cal_housing.data` to:\n",
    "- open it \n",
    "- read the lines \n",
    "- strip the final newline `\\n` \n",
    "- split on commas\n",
    "- turn the data into a numpy array, casting it as floats\n",
    "\n",
    "**Note**\n",
    "\n",
    "You can see the features by loading `cal_housing.domain`, read its lines, and print its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flVUBl9uDyQg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e5EjEmJDyQh"
   },
   "source": [
    "### 3. Separate the features and the targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e5EjEmJDyQh"
   },
   "source": [
    "The price is the last feature, so you need to use NumPy to slice all the `targets` in the last dimension, and the rest of the `features` in another array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFXwQ9G-DyQh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nC-zUspdDyQh"
   },
   "source": [
    "### 4. Scale the prices to a more manageable range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nC-zUspdDyQh"
   },
   "source": [
    "You can print the `min()` and the `max()` of your `targets` to see the kind of range we are dealing with.\n",
    "\n",
    "Then a division by `100000` will give us similar numbers to the Boston Housing Dataset (and the `scikit-learn` version, as above).\n",
    "\n",
    "Once you have your reduced targets, you may want to print again the `min()` and the `max()` as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g46-rbBZDyQh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bM_Iu278DyQh"
   },
   "source": [
    "### 5. Reduce the dataset to Boston Housing size, or split your data into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bM_Iu278DyQh"
   },
   "source": [
    "Use `.shape` on your `features` (and/or `targets`) to check how many samples this dataset has.\n",
    "\n",
    "Either use random indices as above to select only 504 samples, or train normally, but splitting into train and test sets.\n",
    "\n",
    "Slice both `features` and `targets` to obtain `train_data`, `test_data`, and `train_targets`, `test_targets` respectively.\n",
    "\n",
    "This is actually a potential subject of experiment. You could slice it roughly in the middle, or have more in your training than your testing set.\n",
    "\n",
    "As a sanity check, your shapes should look like this:\n",
    "```\n",
    "# n_train: number of training samples\n",
    "# n_train: number of testing samples\n",
    "train_data (n_train, 8)\n",
    "train_targets (n_train,)\n",
    "test_data (n_test, 8)\n",
    "test_targets (n_test,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFpKpHYZDyQh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BwauS6YDyQh"
   },
   "source": [
    "### 6. Normalisation/scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BwauS6YDyQh"
   },
   "source": [
    "Use the mean and standard deviation of the **train data** to normalise it, and apply the same transform to test data, exactly as above with the Boston Housing Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-s8FLct8DyQh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7wvRPCCDyQi"
   },
   "source": [
    "### 7. Everything is now set up for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the procedure (define the model, train, plot) is now the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1kyCcaADyQi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtOLngZXDyQi",
    "tags": []
   },
   "source": [
    "### Experiments\n",
    "\n",
    "The obvious thing to be done here is to compare the results between the small random subset and the full dataset. Varying the size of the test set could also be of interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnRx7UKTDyQi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmGGVwnaDyQi"
   },
   "source": [
    "### 8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmGGVwnaDyQi"
   },
   "source": [
    "Retrain the best model (with the same hyperparameters on the entire the training data (`train_data` and `train_targets`) **without validation data** and instead evaluate on the test data (`test_data`, `test_targets`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1V5AkAyDyQi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtyRXlyPDyQi",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Visualisations\n",
    "\n",
    "Three examples of how people use visualisations for this dataset:\n",
    "- [California Housing Modelling and Map Visualisation](https://www.kaggle.com/code/qixuan/california-housing-modelling-and-map-visualisation)\n",
    "- [California Housing Prices: EDA and Visualization](https://www.kaggle.com/code/ujwalkandi/california-housing-prices-eda-and-visualization)\n",
    "- [The California housing dataset](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
