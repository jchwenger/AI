{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6gb6X1pzNp1"
   },
   "source": [
    "# The California Housing Dataset\n",
    "\n",
    "---\n",
    "\n",
    "### Colab Note\n",
    "\n",
    "Don't forget that you can link your notebook to your drive and save your work there. Then you can download and backup your models, reload them to keep training them, or upload datasets to your drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir('drive/My Drive/') # 'My Drive' is the default name of Google Drives\n",
    "    os.listdir()\n",
    "    \n",
    "# use os.chdir(\"my-directory\") # to change directory, and\n",
    "# os.listdir()                 # to list its contents\n",
    "# os.getcwd()                  # to get the name of the current directory\n",
    "# os.mkdir(\"my-new-dir\")       # to create a new directory\n",
    "# See: https://realpython.com/working-with-files-in-python/\n",
    "\n",
    "# You can also use bash commands directly, preceded by a bang\n",
    "# !ls\n",
    "# However, the following will *not* change the Python directory \n",
    "# the notebook points to (use os.chdir for that)!\n",
    "# !cd my-directory    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For reproducible results\n",
    "\n",
    "```python\n",
    "tf.random.set_seed(42) # can be any number\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## The California Housing dataset (optional)\n",
    "\n",
    "[California Housing](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html), original website. (Also available on [Kaggle](https://www.kaggle.com/datasets/camnugent/california-housing-prices).)\n",
    "\n",
    "### 1. Download\n",
    "\n",
    "The terminal commands to download it. (Add a `!` in front of them to use them from Jupyter or Colab.)\n",
    "\n",
    "```bash\n",
    "wget https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.tgz\n",
    "tar -xvf cal_housing.tgz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the data\n",
    "\n",
    "Use the name of the file `cal_housing.data` to:\n",
    "- open it \n",
    "- read the lines \n",
    "- strip the final newline `\\n` \n",
    "- split on commas\n",
    "- turn the data into a numpy array, casting it as floats\n",
    "\n",
    "#### Note\n",
    "\n",
    "You can see the features by loading `cal_housing.domain`, read its lines, and print its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Separate the features and the targets\n",
    "\n",
    "The price is the last feature, so you need to use NumPy to slice all the `targets` in the last dimension, and the rest of the `features` in another array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scale the prices to a more manageable range\n",
    "\n",
    "You can print the `min()` and the `max()` of your `targets` to see the kind of range we are dealing with.\n",
    "\n",
    "Then a division by `100000` will give us similar numbers to the Boston Housing Dataset.\n",
    "\n",
    "Once you have your reduced targets, you may want to print again the `min()` and the `max()` as a sanity check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Split your data into train and test\n",
    "\n",
    "Use `.shape` on your `features` (and/or `targets`) to check how many samples this dataset has.\n",
    "\n",
    "Slice both `features` and `targets` to obtain `train_data`, `test_data`, and `train_targets`, `test_targets` respectively.\n",
    "\n",
    "This is actually a potential subject of experiment. You could slice it roughly in the middle, or have more in your training than your testing set.\n",
    "\n",
    "As a sanity check, your shapes should look like this:\n",
    "```\n",
    "# n_train: number of training samples\n",
    "# n_train: number of testing samples\n",
    "train_data (n_train, 8)\n",
    "train_targets (n_train,)\n",
    "test_data (n_test, 8)\n",
    "test_targets (n_test,) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Normalisation/scaling\n",
    "\n",
    "Use the mean and standard deviation of the **train data** to normalise it, and apply the same transform to test data, exactly as above with the Boston Housing Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Everything is now set up for training\n",
    "\n",
    "The rest of the procedure (define the model, train, plot) is now the same.\n",
    "\n",
    "#### Note\n",
    "\n",
    "This dataset is not small like the Boston Housing Dataset, so you may find that it's taking too long to do many epochs with K-fold given the compute you have. This doesn't matter *too* much, the important thing is to understand the K-fold logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiments\n",
    "\n",
    "You could artificially reduce your training set to a similar size as the Boston Housing Dataset (~400 samples), and see what performance you manage to get on your (then much, much larger) test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "Don't forget to retrain on the entire training set using the best hyperparamemters, and evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Visualisations\n",
    "\n",
    "Three examples of how people use visualisations for this dataset:\n",
    "- [California Housing Modelling and Map Visualisation](https://www.kaggle.com/code/qixuan/california-housing-modelling-and-map-visualisation)\n",
    "- [California Housing Prices: EDA and Visualization](https://www.kaggle.com/code/ujwalkandi/california-housing-prices-eda-and-visualization)\n",
    "- [The California housing dataset](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
