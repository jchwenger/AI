{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7.310 Advanced architecture\n",
    "\n",
    "Some advanced architectures power state-of-the-art DL models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. ##### Residual connections\n",
    "\n",
    "Explained in 7.140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. ##### Batch normalisation\n",
    "\n",
    "The output from a layer might have some large values - and these will dominate training\n",
    "\n",
    "Batch normalisation is like data preprocessing at a hidden layer\n",
    "\n",
    "=>  normalise layer output to quench unusually high values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "BN operates by keeping an exponentially moving average of batch mean and std during training\n",
    "\n",
    "Assists the optimiser and allows deeper networks\n",
    "\n",
    "Some very deep networks would be impossible to train without batch normalisation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The idea, therefore is to normalise between layers:\n",
    "\n",
    "```\n",
    "model.add(layers.Conv2D(32, 3, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "...\n",
    "```\n",
    "\n",
    "The application to dense layers is identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3. ##### Depthwise separable convolution\n",
    "\n",
    "A replacement for `Conv2D` that is\n",
    "\n",
    "    - lighter (fewer trainable weights)\n",
    "    - a few per cent better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The idea is to use a spatial convolution on each channel under the assumption that\n",
    "- spatial locations are highly correlated\n",
    "- different channels are fairly independent\n",
    "\n",
    "The parallel convolutions are mixed via a max poolinglayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.add(layers.SeperableConv2D(64, 3, activation='relu'))\n",
    "model.add(layers.SeperableConv2D(128, 3, activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Depthwise seperable convolutions are the basis of the high-performing Xception architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7.320 Hyperparameter optimisation\n",
    "\n",
    "DL engineers have many seemingly arbitrary decisions:\n",
    "\n",
    "- How many layers?\n",
    "- The size of each layer\n",
    "- Activation?\n",
    "- Batch normalise?\n",
    "- How much dropout?\n",
    "- Learning rate?\n",
    "- etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Apart from intuition, how do they make these decisions?\n",
    "\n",
    "Is there a way of exploring the hyperparameter space and the space of hypotheses spaces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hyperparameter optimisation \n",
    "\n",
    "1. Choose a set of hyperparameters\n",
    "2. Build the model\n",
    "3. Fit to training data and measure performance on validation data\n",
    "4. If good enough\n",
    "    - Evaluate on test data\n",
    "    - Stop\n",
    "5. Go to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The particular optimisation scheme will specify step 1.\n",
    "\n",
    "Bayesian, genetic optimisation, random search etc.\n",
    "\n",
    "But, not gradient descent!!!\n",
    "\n",
    "Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The hyperparameter space is discrete in some dimensions (e.g. number of layers) - so the loss function on this space is typically neither continuous or differentiable\n",
    "\n",
    "Furthermore, the process is computationally very expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Feature engineering, a costly and difficult human process, has been replaced by deep learning - features are automatically tuned by the feedback signal and not by hand\n",
    "\n",
    "We can hope the same speed-up will happen for hyperparameter engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7.330 Model ensembling\n",
    "\n",
    "Ensembling means pooling predictions from a number of independent models\n",
    "\n",
    "The idea is based on the observation that any model can only grasp part of the truth, but a suitably *diverse* set of models might access much of the truth \n",
    "\n",
    "Ensembling generally produces the most competitive ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pooling can proceed by averaging model predictions \n",
    "\n",
    "But there is a disadvantage - a poor model can worsen the average, even dragging it below the performace of the best model\n",
    "\n",
    "The predictions can be weighted, with higher weights to better models\n",
    "\n",
    "The weights can even be automatically optimised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The models in the ensemble need to be as diverse as possible\n",
    "\n",
    "Avoid training the same model from different initialisations/order of exposure to data and then ensembling the result since the runs are not diverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A model in the ensemble might be comparatively poor and have low weight, but can nevertheless make a quantitative difference to the overall prediction because it was distant from the other models and provided exclusive information\n",
    "\n",
    "The concern is for a diversity of models rather than how well the best model performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7.340 Wrapping up\n",
    "\n",
    "- High performing convnets will utilise one or more of\n",
    "    - residual connections\n",
    "    - batch normalisation\n",
    "    - depthwise separable convolutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "    \n",
    "- DL engineering requires exploration in hyperparameter space\n",
    "    - currently guided by intuition\n",
    "    - automation is desirable\n",
    "    - human or automatic search should be systematic\n",
    "    - random search might be the only feasible option for extremely unstructured spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Ensembling with a well weighted average is very powerful\n",
    "- Component models are as dissimilar as possible"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
