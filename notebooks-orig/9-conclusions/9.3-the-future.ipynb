{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our knowledge of DL models and their limitations, F. Chollet has made some medium-term predictions:\n",
    "\n",
    "- *Models closer to computer programs*. These models would be built on primitives richer than the current differentiable layers. These new models might attain powers of abstraction and reasoning.\n",
    "- These new models would require *new forms of learning*.\n",
    "- *Models that require less human involvement* i.e. more autonomy for tuning.\n",
    "- *Greater and systematic reuse of previously learnt features and architectures* such as meta-learning with rerusable and modular program subroutines.\n",
    "\n",
    "These projections are appplicable to unsupervised, self-supervised and reinforcement learning.\n",
    "\n",
    "# Models as programs\n",
    "\n",
    "Current AI reasonong relies on hardcoding e.g. search algorithms, graph manipulation and formal logic.\n",
    "\n",
    "AlphaGo, for example, the intelligence is hardcoded as a Monte-Carlo search tree. Learning from data only occurs in specialised submodules.\n",
    "\n",
    "RNNs are less restricted than feedforward nets becuase they apply simple geometric transformations within a feedback loop. \n",
    "\n",
    "But what if the feedback loop is replaced by programming primitives such as `if`, `while`, variables, disk storage, sorting operations, datastructures (lists, hash tables etc.)? \n",
    "\n",
    "The hypothesis space would far exeed the hyp. space of current DL models.\n",
    "\n",
    "Program synthesis refers to the automatic search for simple programs, perhaps by evolutionary techniques. The search terminates when input-output pairs have been matched. But instead of modifying a hardcoded model, program syntheis geneates *source code*.\n",
    "\n",
    "There is a possibilty of merging DL with program synthesis: neural networks (data-processing flows) augmented with a rich set of algorithmic primitives. \n",
    "\n",
    "# Beyond backpropagation\n",
    "\n",
    "The DL/program synthesis hybrids will no longer be differentiable.\n",
    "\n",
    "The weighted DL layers will still be differentiablke but the hybrid as a whole won't be.\n",
    "\n",
    "So, backpropagation cannot train any model involving programs becuase programs aren't dofferentiable.\n",
    "\n",
    "But what alternatives are there?\n",
    "\n",
    "Optimisers that make little assumption about the loss function (or, at least, do not assume continuity and smoothness) have been researched for a long time: genetic algorithms, simulated annealing, swarm intelligence...\n",
    "\n",
    "Furthermore, backpropagation does not take advantage of the modularity of deep nets. Instead, perhaps decoupled, sychronised, heierarchical training modules might implement backpropagation more efficiently.\n",
    "\n",
    "# Automated ML\n",
    "\n",
    "Perhaps architectures will be learnt rather than handcrafted.\n",
    "\n",
    "The DL engineer data-munges with Python scripts followed by architecture and hperparameter tuning.\n",
    "\n",
    "Data-munging is hard to automate, but hyperparameter tuning is a simple search procedure. These tuning systems already exist. Even architecture search algrithms are feasible.\n",
    "\n",
    "But learning architectures in conjunction with model weights would be mre desirable. This would be more efficient because at the moment each architecture has to be trained from scratch.\n",
    "\n",
    "# Lifelong learning and modular subroutine reuse\n",
    "\n",
    "There is a lot of wasted effort in DL: every dataset/model/task requires training from scratch.\n",
    "\n",
    "But future hybrid DL/synthetic progams would require higher modular reuse. This is because many datasets are insufficiently informative - it will be necessary to use information from previously encountered datasets (we don't relearn English every time we read a new book).\n",
    "\n",
    "Furthermore, training models from scratch is inefficient because tasks overlap.\n",
    "\n",
    "For example, the same DL model trained to translate English/German and French/Italian is better at each indivdual translation. And, training an image segmetation model in conjunction with a an image classification model yields a model that is better at both tasks.\n",
    "\n",
    "This is due to information overlap.\n",
    "\n",
    "We already use pretained models - i.e. a fixed model with weights trained an a large database -  in computer vision, but the pretraining could be generalised to architectures and even training procedures.\n",
    "\n",
    "If models become more like programs, them maybe program subroutines could be collected into software libraries, like the large class libraries of modermn OOP languages.\n",
    "\n",
    "This is one step along the path to abstraction. (?)\n",
    "\n",
    "# The long-term vision\n",
    "\n",
    "- Models will begin to resemble programs enabling more general, non-differentiable transformations between input and output spaces. Some argue that program-models are closer to abstract human mental models than current models are, and will be capable of greater generalisation. \n",
    "\n",
    "\n",
    "- Models will blend algorithmic modules (for formal reasoning) with geometric modules (informal intuition and pattern recognition). AlphaGo is a paradigmatic example, but require much human engineering.\n",
    "\n",
    "\n",
    "- Models will be grown rather than hardcoded (as AlphaGo was) from global libraries of reusable subroutines. \n",
    "   - This library will have evolved by training high-performing models on thousands of different tasks and datasets. \n",
    "   - Frequent problem-solving patterns will be identified by the meta-learning system and turned into reusable subroutines, like classes in OOP libraries, and added to the global library.\n",
    "\n",
    "\n",
    "- The global library and model-growing system will achieve extreme generalisation, able to generate new working models in novel situations from scant data(!?). \n",
    "\n",
    "\n",
    "- We will have arrived at the initial, 1950s goal of AI: an artificial general intelligence.\n",
    "\n",
    "\n",
    "- Doom-laden predictions of singularities, robot apocalypses and aggresive paperclip machines misunderstand intelligence and technology and the relation between the two. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR DOES IT????"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
