{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "DL cannot input raw text - the text must be vectorised\n",
    "- words to vectors\n",
    "- characters to vectors\n",
    "- n-grams of words or characters to vectors.\n",
    "\n",
    "In each case, the units are called tokens. Two main tokenisations:\n",
    "- one-hot\n",
    "- token (usually word) embeddings.\n",
    "\n",
    "##### n-grams \n",
    "\n",
    "are sets of consecutive words with maximum length n. \n",
    "\n",
    "- For example\n",
    "\"The cat sat on the mat\" -> {\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \"the cat\", \"cat sat\", \"sat on\", \"on the\", \"the mat\"} is a *bag of 2-grams*.\n",
    "- order is lost\n",
    "- a kind of feature engineering\n",
    "- used in shallow learning (logistic regression, random forests)\n",
    "- DL uses RNNs and convnets to learn representations for ordered groups of tokens without any priors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-hot encoding\n",
    "- associate a unique integer $n$ with each word in a dictionary of $N$ \n",
    "- then form an $N$ dimensional vector $v$ with components $v_i = \\delta_{i, n}$ i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "v_i = \\begin{cases} &1 \\text{ if } i == n \\\\\n",
    "                    &0 \\text{ otherwise } \\\\\n",
    "                    \\end{cases}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word embeddings\n",
    "\n",
    "Dense and low dimensional e.g. 256, 512, 1024), unlike one hot vectors, which are sparse and high dimensional e.g. 20,000.\n",
    "\n",
    "- Learn word embeddings with the main task - start with random vectors - or\n",
    "- use pre-trained embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The embedding layer\n",
    "\n",
    "The embedding layer learns embeddings with every new task.\n",
    "\n",
    "A structured embedding space is desired.\n",
    "- words of similar meanings are close\n",
    "- geometric and semantic relationships match\n",
    "    - vector wolf-to-dog = vector lion-to-cat and lion-to-wolf = cat-to-dog\n",
    "- expect different geometries for different tasks (sentiment analysis is very different from classification of legal documents).\n",
    "\n",
    "This code makes an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorlow.keras.layers import Embedding\n",
    "embedding_layer = Embedding(10000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with a 10000 word dictionary and an embedding space of 64 dimensions.\n",
    "\n",
    "It accepts batches of shape `(samples, sequence_length)` and returns a 3D float tensor of shape `(samples, sequence_length, embedding_dimensionality)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "# Number of words to consider as features\n",
    "max_features = 10000\n",
    "# Cut texts after this number of words \n",
    "# (among top max_features most common words)\n",
    "maxlen = 20\n",
    "\n",
    "# Load the data as lists of integers.\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# This turns our lists of integers\n",
    "# into a 2D integer tensor of shape `(samples, maxlen)`\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "# We specify the maximum input length to our Embedding layer\n",
    "# so we can later flatten the embedded inputs\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "# After the Embedding layer, \n",
    "# our activations have shape `(samples, maxlen, 8)`.\n",
    "\n",
    "# We flatten the 3D tensor of embeddings \n",
    "# into a 2D tensor of shape `(samples, maxlen * 8)`\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add the classifier on top\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But merely flattening the embeddings produces a model that treats each word separately. The order is lost. \n",
    "\n",
    "We will meet order sensitive models such as RNNs and 1D convnets in a few minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-trained embeddings\n",
    "\n",
    "- Word-to-vec and GLoVe\n",
    "- Listings 6.8 to 6.11 demonstrate how to tokenise raw text and form the embedding weight array.\n",
    "- Then, the model is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "max_words = 10000\n",
    "embedding_dim = 100\n",
    "maxlen = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer is frozen for the same reason as the freezing of the convnet base in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained on just 200 reviews because pre-trained embeddings are thought to be useful on problems with small training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs\n",
    "\n",
    "The dense, fully connected layers and the convnet layers have no memory. These *feedforward networks* process each input in turn with no information transfer.\n",
    "\n",
    "A recurrent neural network (RNN), on the other hand, retains information of past inputs.\n",
    "\n",
    "The RNN has a feedback loop - the activation $a(t - 1)$ at time $t - 1$ is inputted, along with the sequence term $x(t)$, at time $t$. \n",
    "\n",
    "DLWP: the previous activation is called a 'state'. \n",
    "\n",
    "The previous activation is weighted and the network learns patterns correlations with earlier inputs by adjusting these weights, in the usual manner.\n",
    "\n",
    "(Here, we imagine that sequence terms $x(1), x(2), x(3), \\ldots$ etc. occur at successive times, even if the sequence is not chronological.)\n",
    "\n",
    "For example, successive words $x(1), x(2), x(3), \\ldots$ of a single movie review.\n",
    "\n",
    "The state is reset before a new sequence (new movie review) is processed.\n",
    "\n",
    "Previously, in *feed forward networks*, the entire sequence (review) was treated as a single input vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense Feedforward layer - recap\n",
    "\n",
    "Suppose the input to a layer is the vector $x = [x_1, x_2, \\ldots x_N]^T$ and the output, the activation, is the vector $a = [a_1, a_2, \\ldots, a_M]^T$.\n",
    "\n",
    "There are $M$ output features (one per neuron) and $N$ input features.\n",
    "\n",
    "The weight matrix is\n",
    " \\begin{bmatrix}\n",
    "  W_{11} & W_{12} & \\ldots & W_{1N} \\\\\n",
    "  W_{21} & W_{22} & \\ldots & W_{2N} \\\\\n",
    "  \\ldots \\\\\n",
    "  W_{M1} & W_{M2} & \\ldots & W_{MN} \\\\\n",
    " \\end{bmatrix}\n",
    " \n",
    "and the bias is $b = [b_1, b_2, \\ldots, b_M]$: there are $M \\times N + M$ adjustable parameters.\n",
    "\n",
    "The layer activation, as a vector, is \n",
    "\n",
    "$\n",
    "a = \\sigma(W \\cdot x + b)\n",
    "$\n",
    "\n",
    "which means, in terms of components, \n",
    "\n",
    "$\n",
    "a_i = \\sigma\\left(\\sum_{j = 1}^{N} W_{ij}x_j + b_i\\right).\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The simple recurrent layer\n",
    "\n",
    "There are two inputs: the current input, $x(t)$, and the previous activation, $a(t - 1)$. \n",
    "\n",
    "The activation from the RNN layer is \n",
    "\n",
    "$\n",
    "a(t) = \\sigma(W \\cdot x(t) + U \\cdot a(t - 1) + b).\n",
    "$\n",
    "\n",
    "The idea is that the previous activation, $a(t - 1)$, can have more or less effect on the current activation. The influence of the previous activation and input is controlled by the weights. The equation for $a(t)$ is recursive,\n",
    "\n",
    "$\n",
    "a(t) = f(x(t), a(t-1)) = f(x(t), f(x(t - 1), a(t-2))) = \\ldots,\n",
    "$\n",
    "\n",
    "so, in principle, the distant past can influence the present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN layer in pseudocode is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_output = 0\n",
    "for input_t in input_sequence:\n",
    "    output_t = activation(dot(W, input_t) + dot(U, prev_output) + b)\n",
    "    prev_output = output_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U is an $M \\times M$ matrix - or, as a tensor, it has shape (output_features, output_features).\n",
    "\n",
    "So, there are $M \\times N + M \\times M + M$ cofficients.\n",
    "\n",
    "Another way of seeing it: each neuron has $N$ weights for the $N$ input features, $M$ weights for the previous output, and a bias: that's $(N + M + 1)$ coefficients. But there are $M$ units, giving $(N + M + 1) \\times M$ coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a keras recurrent model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 322,080\n",
      "Trainable params: 322,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of parameters in the `SimpleRNN` layer, from the above calculation is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2080"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(32 + 32 + 1) * 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which agrees with `model.summary()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model processes inputs in mini-batches, just as in the feedforward networks, and `SimpleRNN` layers can be stacked just like FF networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listings 6.22-6.24 show a simple RNN for IMDB, with the restriction to the first 500 words of each review.\n",
    "\n",
    "The simple RNN falls below the FF MNIST model of chapter 3 (85% cf. 88%).\n",
    "\n",
    "This is in part due to the short reviews, but also because the simple RNN is just *too* simple. There is a fundamental pathology: *vanishing gradients*. The backpropagated error signal progressively weakens.\n",
    "\n",
    "Peanuts per hour = peanuts per mile times miles per hour - the  products of gradients\n",
    "\n",
    "The simple RNN is never used in practice. Instead, engineers turn to more refined RNNs such as the LTSM network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The LSTM layer\n",
    "\n",
    "The LSTM layer is really a kind of multi-layer with two different activation functions.\n",
    "\n",
    "Let's quickly revise three common activation functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Activation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    y = np.zeros(len(x))\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(-10, 10, 0.01)\n",
    "\n",
    "plt.plot(x, sigmoid(x), 'k')\n",
    "plt.title('sigmoid')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, tanh(x), 'k')\n",
    "plt.title('tanh')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, relu(x), 'k')\n",
    "plt.title('relu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM pseudocode \n",
    "\n",
    "```\n",
    "memory = gated_prev_memory + gated_simple_RNN\n",
    "output_t = gated_tanh-activation(memory)\n",
    "```\n",
    "\n",
    "##### Explanation\n",
    "\n",
    "- The LSTM layer output is `output_t`. The LSTM layer output at $t - 1$ is `prev_output`.\n",
    "- A gate is a simple RNN layer with sigmoid activation,\n",
    "```\n",
    "sigmoid-activation(dot(input_t, W) + dot(prev_output, U) + b)\n",
    "```\n",
    "- 'Gated' means multiply by a gate. \n",
    "- All gates and simple RNN layers have their own parameters $(W, U, b)$.\n",
    "- Simple_RNN: \n",
    "```tanh-activation(dot(input_t, W) + dot(prev_output, U) + b)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The LSTM layer in detail*\n",
    "\n",
    "[A popular blog account](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) and [there's always Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory)\n",
    "\n",
    "##### Memory\n",
    "\n",
    "The LTSM layer has a memory, called 'carry' by Chollet and, confusingly, 'state' by other authors. \n",
    "\n",
    "----\n",
    "\n",
    "1. A forget gate:\n",
    "\n",
    "$\n",
    "f(t) = \\sigma_g(W_f \\cdot x(t) + U_f \\cdot a(t - 1) + b_f)\n",
    "$\n",
    "\n",
    "$\\sigma_g$ is a sigmoid, outputing in $[0, 1]$, a kind of smoothed 'gate'.\n",
    "\n",
    "----\n",
    "\n",
    "2. An input gate:\n",
    "\n",
    "$\n",
    "i(t) = \\sigma_g(W_i \\cdot x(t) + U_i \\cdot a(t - 1) + b_i)\n",
    "$\n",
    "\n",
    "which we can also imagine as open or closed or in-between.\n",
    "\n",
    "----\n",
    "\n",
    "3. A simple RNN layer - to inject new information into the memory\n",
    "\n",
    "$\n",
    "k(t) = \\sigma_h(W_k \\cdot x(t) + U_k \\cdot a(t - 1) + b_k)\n",
    "$\n",
    "\n",
    "where $\\sigma_h$ is the $\\tanh$ activation.\n",
    "\n",
    "----\n",
    "\n",
    "4. Putting all this together:\n",
    "\n",
    "```\n",
    "memory = gated_prev_memory + gated_simple_RNN\n",
    "```\n",
    "\n",
    "$\n",
    "c(t) = f(t) * c(t - 1) + i(t) * k(t)\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "##### Output\n",
    "\n",
    "----\n",
    "\n",
    "1. An output gate\n",
    "\n",
    "$\n",
    "o(t) = \\sigma_d(W_o \\cdot x(t) + U_o \\cdot a(t - 1) + b_o)\n",
    "$\n",
    "\n",
    "----\n",
    "2. which multiplies, or gates, the memory\n",
    "\n",
    "```\n",
    "output_t = output_gate(t) * tanh-activation(memory)\n",
    "```\n",
    "\n",
    "$\n",
    "a(t) = o(t) * \\sigma_h(c(t))\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\\sigma_h$ is a hyperbolic tangent (or, in a 'peephole' LSTM, the identity function $\\sigma(x) = x$). \n",
    "\n",
    "\n",
    "##### Parameters\n",
    "\n",
    "There are four $(W, U, b)$ units - 3 gates and the new information RNN, $k$. Each set of $(W, U, b)$ units has $M times (M + N + 1)$ trainable parameters. Hence an LTSM layer has $4M(M + N + 1)$ tuneable parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params:  416 \n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 8)                 416       \n",
      "=================================================================\n",
      "Total params: 416\n",
      "Trainable params: 416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.models as models, tensorflow.keras.layers as layers\n",
    "\n",
    "time_steps = 10\n",
    "input_feats = 4\n",
    "output_feats = 8\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(output_feats, input_shape=(time_steps, input_feats)))\n",
    "print('Trainable params: ', 4 * output_feats * (output_feats + input_feats + 1), '\\n')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM comparison\n",
    "\n",
    "A very simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scores 89% validation accuracy, improving on the simple RNN accuracy (85%) for the MNIST dataset (because it suffers less from vanishing gradients) and is slightly better than the fully connected model of DLWP Chapter 3 (where the reviews were not truncated at 500 words). \n",
    "\n",
    "LSTM might perform better with hyper-parameter tuning and regularisation but the main issue is that the salient information in the reviews is not sequential - frequencies of positive and negative words are enough for good prediction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence processing with convnets\n",
    "\n",
    "Convnets are successful at (some) visual problems because they exploit 2D locality.\n",
    "\n",
    "We expect that 1D convnets might also have applications in sequence data.\n",
    "\n",
    "In fact they offer a fast alternative to RNNs for text processing and time-series forcasting.\n",
    "\n",
    "Imagine a 1D convolutional filter of length 5 tokens sliding across a sequence. The same transformation is performed on each patch - so the filter can recognise patterns anywhere in the input sequence.\n",
    "\n",
    "1D convolutions can be pooled, as well, with receptive fields of 5, 7 or 9.\n",
    "\n",
    "Here is the model of Listing 6.46:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The global max-pooling layer finds the max over the entire input. \n",
    "- Validation accuracy is less than the previous LSTM model, but the training run is quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RNN's\n",
    "\n",
    "Three techniques: recurrent dropout, stacked layers and bidirectional layers are applied to the temperature-forecasting problem: given 14 weather measurements taken at ten minute intervals over several years (the Jena dataset), can we predict (for example) the next temperature reading from 24 hours worth of data.\n",
    "\n",
    "###### Common sense baseline\n",
    "\n",
    "Simply suppose that the temperature 24 hours from now is just the current temperature. This baseline has an MAE of 0.29 or 2.57C. \n",
    "\n",
    "###### DL baseline \n",
    "\n",
    "```\n",
    "model = Sequential()\n",
    "model.add(layers.Flatten...)\n",
    "model.add(layers.Dense(32, activation='relu')\n",
    "model.add(layers.Dense(1))\n",
    "```\n",
    "Note the single neuron final layer - no activation - this is a regression problem. \n",
    "\n",
    "The DL baseline is worse than the common-sense baseline.\n",
    "\n",
    "The common sense solution might lie in the hypothesis space but might be effectively unlearnable. Parameter learning is not infallible and it can fail to find simple solutions to simple problems.\n",
    "\n",
    "###### RNN baseline\n",
    "\n",
    "Listing 6.39 shows a Gated Recurrent Unit model (GRU).\n",
    "```\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps)\n",
    "```\n",
    "\n",
    "The GRU is similar, but represetationally weaker than, the LSTM. However, it runs faster.\n",
    "\n",
    "MAE: 2.35C, better than common sense (2.57C), but can we do better?\n",
    "\n",
    "##### Dropout\n",
    "\n",
    "Remembert the idea is to break chance correlations by zero'ing random units.\n",
    "\n",
    "The same dropout mask, in RNNs, is applied to each step rather than a different mask at each timestep.\n",
    "\n",
    "```\n",
    "model.add(layers.GRU(32,\n",
    "                     dropout=0.2,\n",
    "                     recurrent_dropout=0.2,\n",
    "                     input_shape=(None, float_data.shape[-1])))\n",
    "```\n",
    "\n",
    "The GRU with dropout at 0.2 shows improvement over the baseline GRU on the Jena data - the model does not overfit.\n",
    "\n",
    "##### Stacked recurrent layers\n",
    "\n",
    "Remember the DL workflow: find the simplest model that doesn't overfit and then enlarge the model until it overfits. Finally, regularise the overfitting model.\n",
    "\n",
    "Stacking is very easy in keras - Listing 6.41.\n",
    "\n",
    "```\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,\n",
    "                     dropout=0.1,\n",
    "                     recurrent_dropout=0.5,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu',\n",
    "                     dropout=0.1, \n",
    "                     recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "```\n",
    "\n",
    "The two recurrent layer model still does not overfit too badly, so another layer could be added\n",
    "- but there will be a computational cost\n",
    "- and adding a second layer didn't have a great effect, so the returns might be diminishing.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bidirectional RNNs\n",
    "\n",
    "RNNs are applied to ordered data - but which order? Chronological or anti-chronological? Or, for non-chronological sequences, from left to right or right to left?\n",
    "\n",
    "Training a one layer GRU with anti-chronological Jena data underperforms the same network with chronological data. That makes sense: the recent past is more salient to temperature prediction than the more distant past.\n",
    "\n",
    "But a reverse-direction LSTM equals the performance normal direction LSTM on IMDB data.\n",
    "\n",
    "It seems word order is important in natural language processing - but *which* order is irrelevant!\n",
    "\n",
    "In general we expect reversed order training to yield different representation from forward order training. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representations that are different but useful are worth exploiting in DL modeling.\n",
    "\n",
    "The bidirectional RNN merges the forward and backward order trained RNN layers.\n",
    "\n",
    "This is an example of ensembling - merging different models.\n",
    "\n",
    "We find some improvement with the bidirectional LSTM over the regular LSTM on IMDB - overfitting sooner (there are twice as many parameters). Now, we can try regularisation.\n",
    "\n",
    "The bidirectional GRU does not perform better than the regular model on the temperature task - because the anti-chronological half is not, in this case, sufficiently infomative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Even more advanced\n",
    "\n",
    "- Adjust the number of units in each recurrent layer\n",
    "- Try different learning rates\n",
    "- Substitute LSTM layers for GRU layers\n",
    "- Try a bigger densely connected regressor on top of the recurrent base, or even a stack of dense layers\n",
    "- Remember to run the best performing models on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 500, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,315,937\n",
      "Trainable params: 1,315,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "max_features = 10000  # number of words to consider as features\n",
    "max_len = 500  # cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The global max-pooling layer finds the max over the entire input. \n",
    "- Validation accuracy is less than the previous LSTM model, but the training run is quicker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining CNN and RNNs for long sequence processing\n",
    "\n",
    "1D convnets insensitive to order at lengths greater than the convolution window.\n",
    "\n",
    "So, stack convnet layers, as we did with vision tasks?\n",
    "\n",
    "Stacked 1D CNNs don't always work so well: Listing 6.47 shows a stacked model on the Jena data. \n",
    "- The results are worse than common sense\n",
    "- because upper layers access the distant past and we know the distant past is less informative than the recent past in this data set.\n",
    "\n",
    "Solution: use a convnet, which is computationally cheap, to shorten the sequence and place an RNN, which recognises time-ordering, on top.\n",
    "- Especially relevant with very long sequences (downsampling is necessary before input to RNN)\n",
    "- Still not as good as the GRU model on the Jena data, but is significantly faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- Techniques for sequence data:\n",
    "    - Tokenise text\n",
    "    - Word embeddings\n",
    "    - Recurrent networks\n",
    "    - Stacked and bidrirectional RNNs\n",
    "    - 1D convnets for sequence orocessing\n",
    "    - Combination of convnets and RNNs\n",
    "- RNNs applicable to\n",
    "    - timeseries regression/classifiaction, anomaly dertection, sequence labeling (e.g. identifying names or dates in sentences.)\n",
    "- 1D convnets applicable to\n",
    "    - machine translation, document classification and spelling correction\n",
    "- Use an RNN if the sequence data has important global ordering (epsecially for timeseries where the recent past is more predictively informative than the distant past)\n",
    "- Otherwise, if global order is not important, as in text where keywords are equally informative at the beginning or end of the setence, prefer 1D convnets since they work as well as RNNs but are computationally cheaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
