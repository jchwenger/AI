{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But the loss function is tensor-valued?\n",
    "\n",
    "What is the derivative of a tensor operation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "But the loss function is tensor-valued? What is the derivative of a tensor operation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An affine transformation of $(x_1, x_2)^T$ is \n",
    "\n",
    "\\begin{eqnarray}\n",
    "y = \\left(\\begin{array}{c} y_1 \\\\\n",
    "                       y_2 \\end{array} \\right)\n",
    "=\n",
    "\\left( \\begin{array}{c} w_{11} & w_{12} \\\\ \n",
    "                        w_{21} & w_{22} \\end{array} \\right)\n",
    "\\left( \\begin{array}{c} x_1  \\\\ \n",
    "                        x_2  \\end{array}  \\right)\n",
    "+\n",
    "\\left( \\begin{array}{c} b_1  \\\\ \n",
    "                        b_2         \\end{array}  \\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "For example:\n",
    "\\begin{eqnarray} \n",
    "\\left( \\begin{array}{c} 1 & -2 \\\\ \n",
    "                                 2 & 1  \\end{array}  \\right)\n",
    "\\left( \\begin{array}{c} 1  \\\\ \n",
    "                        2  \\end{array}  \\right)\n",
    "+\n",
    "\\left( \\begin{array}{c} 0  \\\\ \n",
    "                        -1\\end{array}  \\right)\n",
    "=\n",
    "\\left( \\begin{array}{c}-3  \\\\ \n",
    "                        3         \\end{array}  \\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We will demonstrate tensor operator differentiation by considering a 2D affine transformation. What is the derivative - the gradient function - of an affine transformation? And how can we use this gradient to move our point 'downhill'? \n",
    "\n",
    "The affine transformation has six parameters - four w's and two b's. The slide shows the transformation function and the result of transforming the point $x_1 = 1, x_2 = 2$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.]\n",
      " [ 3.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "w = tf.Variable([[1., -2.], [2., 1.]])\n",
    "b = tf.Variable([[0], [-1.]])\n",
    "x = tf.Variable([[1.], [2.]])\n",
    "\n",
    "y = w @ x + b\n",
    "\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here is the example as TensorFlow code. The 'at' between $w$ and $x$ is the matrix multiplication operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Differentiation rules\n",
    "\n",
    "\n",
    "|$f$ |$\\frac{df}{dx}$ | | \n",
    "|:---|:---|:---|\n",
    "|$x^n$|$nx^{n-1}$| Power |\n",
    "|$g(x) + h(x)$ | $\\frac{dg}{dx} + \\frac{dh}{dx}$| Linearity |\n",
    "|$af(x)$ | $a\\frac{df}{dx}$ | Linearity |\n",
    "|$f(y(x))$ | $\\frac{df}{dy}\\frac{dy}{dx}$ | Chain rule | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We just need to say a few more things about differentiation. The first line of this table repeats the rule we saw previously - the derivative of a power. Differentiation is linear: the second and third row tell us that the derivative of a sum of functions is the sum of the derivatives and the derivative of a constant times a function is a constant times the derivative. The chain rule in the last row tells us how to differentiate a composite function. Let's clarify with examples.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Differentiation examples\n",
    "\n",
    "|$f$ |$\\frac{df}{dx}$ |\n",
    "|:---|:---|\n",
    "| $5x^3$ | $15 x^2$\n",
    "| $5x^2 - 2x + 3$ | $10x - 2$ |\n",
    "| $(3x + 2x^2)^2$ | $2(3x + 2x^2)\\times(3 + 4x)$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The derivative of $5x^3$ is $15x^2$. \n",
    "\n",
    "The second row shows the derivative of the sum of three functions, $5x^2$, $- 2x$ and the constant function $3$. The derivative of $5x^2$ is $10x$, the derivative of $-2x$ is $-2$ and the derivative of $3$ is 0 because $3 = 3x^0$. We see that the derivatives of the separate parts have been summed.\n",
    "\n",
    "\n",
    "$(3x + 2x^2)^2$ is a composition of $f = y^2$ and $y = 3x + 2x^2$. The next slide spells out the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Writing $(3x + 2x^2)^2$ as a composition:\n",
    "\n",
    "\\begin{align*}\n",
    "f &= y^2 \\text{ where } y = 3x + 2x^2  \\\\\n",
    "\\end{align*}\n",
    "\n",
    "$f$ is differentiated with respect to y, and then $y$ is differentiated with respect to $x$: \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{dx} &= \\frac{df}{dy}\\frac{dy}{dx} \\\\\n",
    "               &= 2y \\  \\times \\  (3 + 4x) \\\\\n",
    "               &= 2(3x + 2x^2)(3 + 4x)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Writing $(3x + 2x^2)^2$ as a composition:\n",
    "\n",
    "\\begin{align*}\n",
    "f &= y^2 \\text{ where } y = 3x + 2x^2  \\\\\n",
    "\\end{align*}\n",
    "\n",
    "$f$ is differentiated with respect to y, and then $y$ is differentiated with respect to $x$: \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{dx} &= \\frac{df}{dy}\\frac{dy}{dx} \\\\\n",
    "               &= 2y \\  \\times \\  (3 + 4x) \\\\\n",
    "               &= 2(3x + 2x^2)(3 + 4x)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The gradient of a two variable function, $f = f(x, y)$, with respect to either variable, is written $\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We almost have all we need to explain automatic gradient descent - how the optimiser tweaks layer weights and biases in an efficient implementation known as backpropagation. The final bit if theory is partial differentiation. \n",
    "\n",
    "Partial differentiation means the differentiation of a function of several variables. Here, $f$ is a function of two variables. The gradient with respect to either function is written with 'curly' d's. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Differentiation follows the single-variable rules \n",
    "\n",
    "The undifferentiated variable is held constant\n",
    "\n",
    "| $f$|$\\frac{\\partial f}{\\partial x}$ |$\\frac{\\partial f}{\\partial y}$ | \n",
    "|:---|:---|:---|\n",
    "|$xy$|$y$| $x$ |\n",
    "| $ax^n + by^m$|$anx^{n-1}$| $bmy^{m-1}$ | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In fact there is not too much extra: the undifferentiated variables are treated as constants. Then the normal rules of differentiation - powers, linearity and the chain rule - are applied. The table shows the partial derivatives with respect to $x$ and $y$.\n",
    "\n",
    "Now we can demonstrate tensor differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose we have a very simple network that implements a single 2D affine transformation\n",
    "\n",
    "Input: $(x_1, x_2)$, prediction: $(y_1, y_2)$, target: $(0, 0)$ \n",
    "\n",
    "Loss: $(y_1 - 0)^2 + (y_2 - 0)^2 = y_1^2 + y_2^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a very simple layer that performs a 2D affine transformation on the input $(x_1, x_2)$. We suppose that the target is the vector $(0, 0)$. The loss is the sum of the squares of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We wish to adjust the weights in order to lower the loss on the input $(1, 2)^T$ \n",
    "\n",
    "We need to find the gradients in $w, b$-space \n",
    "\n",
    "Differentiate with respect to $w_{11}, w_{12}, w_{21}, w_{22}, b_1, b_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We wish to adjust the weights in order to lower the loss on the input $(1, 2)^T$. We need to find the gradients in $w, b$-space and move in the direction of the negative gradient - differentiate with respect to $w_{11}, w_{12}, w_{21}, w_{22}, b_1, b_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "y_1 &= w_{11}x_1 + w_{12}x_2 + b_1 \\\\\n",
    "y_2 &= w_{21}x_1 + w_{22}x_2  + b_2 \\\\\n",
    "\\\\\n",
    "f &= y_1^2 + y_2^2 \\\\\n",
    "  &= (w_{11}x_1 + w_{12}x_2 + b_1)^2 + (w_{21}x_1 + w_{22}x_2  + b_2)^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The loss is a fairly horrible function of the weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial w_{11}} &= \\frac{\\partial f}{\\partial y_1} \\frac{\\partial y_1}{\\partial w_{11}} \n",
    "                                   = 2y_1 \\frac{\\partial y_1}{\\partial w_{11}} \n",
    "                                   = 2y_1 x_1\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "x = \\left( \\begin{array}{c} 1  \\\\ \n",
    "                            2  \\end{array}  \\right),\\quad y = \\left( \\begin{array}{c} -3 \\\\ 3 \\end{array} \\right)\n",
    "\\end{eqnarray}\n",
    "then\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial w_{11}} = -6\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The gradient of the loss with respect to the weight $w_{11}$ is easily calculated by the chain rule. In this case we find that $\\frac{\\partial f}{\\partial w_{11}} = -6$. $f$ is differentiated with respect to $y_1$ and then multiplied by the derivative of $y_1$ with respect to $w_{11}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial w}\n",
    "&=\n",
    "\\left( \n",
    "\\begin{array}{c} \\frac{\\partial f}{\\partial w_{11}} & \\frac{\\partial f}{\\partial w_{12}} \\\\\n",
    "                 \\frac{\\partial f}{\\partial w_{21}} & \\frac{\\partial f}{\\partial w_{22}} \\end{array} \n",
    "\\right) \\\\\n",
    "&=\n",
    "\\left( \n",
    "\\begin{array}{c} 2 y_1 \\frac{\\partial y_1}{\\partial w_{11}} & 2y_1\\frac{y_1}{\\partial w_{12}} \\\\\n",
    "                 2y_2\\frac{\\partial y_2}{\\partial w_{21}}   & 2y_2\\frac{\\partial y_2}{\\partial w_{22}} \\end{array} \n",
    "\\right) \n",
    "=\n",
    "\\left( \n",
    "\\begin{array}{c} -6 \\frac{\\partial y_1}{\\partial w_{11}} & -6\\frac{y_1}{\\partial w_{12}} \\\\\n",
    "                  6 \\frac{\\partial y_2}{\\partial w_{21}} &  6\\frac{\\partial y_2}{\\partial w_{22}} \\end{array} \n",
    "\\right) \\\\\n",
    "&=\n",
    "\\left( \n",
    "\\begin{array}{r} -6 x_1 & -6 x_2 \\\\\n",
    "                  6 x_1 &  6 x_2 \\end{array} \n",
    "\\right) \n",
    "=\n",
    "\\left( \n",
    "\\begin{array}{r} -6  & -12 \\\\\n",
    "                  6  &  12 \\end{array} \n",
    "\\right)\n",
    "\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The weights $w_{11}, w_{12}, w_{21} \\text{ and } w_{22}$ are elements of the weight tensor - in this case a matrix. The tensor derivative is just the tensor of partial derivatives. I've written it out here but you will not never need to write code for this thing - that's why we have TensorFlow and other similar deep learning frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -6. -12.]\n",
      " [  6.  12.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  y = w @ x + b\n",
    "  f = tf.reduce_sum(y**2)\n",
    "\n",
    "[df_dw, df_db] = tape.gradient(f, [w, b])\n",
    "print(df_dw.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here is how TensorFlow can be used to find tensor derivatives. Don't panic: we won't need this code - everything happens automatically underneath `network.fit`.  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
