{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The dataset is small and we risk overfitting if the network is too big\n",
    "\n",
    "The fit would be *too* close\n",
    "\n",
    "So, a small network with two hidden layers is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The dataset is small and we risk overfitting if the network is too big - strangely, the fit can be *too* close. the network models the twists and turns of the data too well and fails to generalise on similar data with different idiosyncrasies. So, a small network with two hidden layers is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, \n",
    "                           activation = 'relu', \n",
    "                           input_shape = (train_data.shape[1], )))\n",
    "    model.add(layers.Dense(64, activation = 'relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A function assembles and compiles the network - useful since we will need to instantiate the network several times. The final layer has a single linear unit (i.e. no activation function). This is necessary since the purpose of the model is the prediction of a continuous variable and a non-linear activation function would constrain the output in some way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The mean squared error is a common regression loss: \n",
    "\n",
    "\\begin{align*}\n",
    "\\text{MSE} = \\frac{1}{N} \\sum (y_{pred} - y)^2\n",
    "\\end{align*}\n",
    "\n",
    "$y$ is the target, $y_{pred}$ is the network output and $N$ is the mini-batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We choose a mean squared error for the loss. This loss is continuous in $y_{pred}$ and is differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The metric for monitoring is the mean absolute error, \n",
    "\n",
    "\\begin{align*}\n",
    "\\text{MAE} = \\frac{1}{N}\\sum |y_{pred} - y |\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The mean `absolute` error is our monitoring metric."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
