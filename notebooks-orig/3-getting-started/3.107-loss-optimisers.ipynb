{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Neural networks have arbitrary model architecture (number of layers, size of each layer) and layer activation\n",
    "\n",
    "The loss function and optimiser, specified at compilation is also arbitrary\n",
    "\n",
    "Hyperparmeters, such as the SGD learning rate, are not trained: they are tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Neural networks have arbitrary model architecture (depth, size of each layer) and layer activation, but the training scheme is also arbitrary. The loss function and optimiser are chosen rather than determined. The parameters that govern training such as the SGD learning rate and depth of the network are not trained: they are 'tuned' - by humans, although automatic tuning is a possibility. These arbitrary parameters - the ones that *we* choose - are called 'hyperparameters'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The choice of loss (objective/cost) function is very important\n",
    "\n",
    "Luckily there are some standard loss functions for common problems\n",
    "- many-class classification: categorical cross-entropy\n",
    "- regression: mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The choice of loss function is very important. Luckily there are some standard loss functions for common problems. \n",
    "Categorical cross-entropy is used for many-class classification and mean squared error is a common regression loss, We will explain the meaning of these terms later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The optimiser is always a variety of stochastic gradient descent (SGD)\n",
    "\n",
    "`tensorflow.keras` offers several SGD variants\n",
    "\n",
    "Bespoke gradient free methods are also possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Finally, although TensorFlow Keras provides a choice of optimiser, they are all SGD variants. The framework is versatile enough for non-gradient optimisation techniques such as genetic algorithms and swarm optimisation - as long as you are willing to write the code.   "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
