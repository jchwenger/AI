{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Experiments:\n",
    "\n",
    "- more or fewer hidden layers - 32 units, 64 units etc.\n",
    "\n",
    "- replace  `binary_cross-entropy` loss function with `mse`\n",
    "\n",
    "- replace `relu` with an early neural network favourite, `tanh`\n",
    "\n",
    "- try different learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here are some ideas for experiments. You could experiment with network archiotecure - with the depth of the network and layer size, and with different losses and activations and learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "\n",
    "- the example architecture is a reasonable choice, although there is room for improvement\n",
    "\n",
    "- data usually has to be preprocessed\n",
    "\n",
    "- sequences of words are often encoded as binary vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The example architecture is a reasonable choice, although there is room for improvement. Data usually has to be preprocessed: sequences of words are often encoded as binary vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- dense `relu` layers with an `rmsprop` optimiser solve some common problems\n",
    "\n",
    "- although neural networks improve in long training runs, performance on unseen data can deteriorate - overfitting\n",
    "\n",
    "- always monitor training on non-training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Dense `relu` layers with an `rmsprop` optimiser solve some common problems. We have seen that although neural networks improve in long training runs, performance on unseen data can deteriorate. This phenomenon is called overfitting. We note that training is monitored by evaluating separate validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- binary classification\n",
    "    \n",
    "    - the final layer is a single dense sigmoid unit\n",
    "\n",
    "    - The loss function is binary_crossentropy\n",
    "    \n",
    "    - the model outputs a probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "More specifically we have seem that the final layer in a binary classification network is a single dense sigmoid unit - so that the model outputs a 'probability', and that the preferred loss is binary_crossentropy.\n",
    "\n",
    "Our next network will solve a multiclass problem - where samples belong to one of several possible classes. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
