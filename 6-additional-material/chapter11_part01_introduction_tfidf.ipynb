{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54fd3280-5c76-4e96-8a3c-218fec930e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os, pathlib, shutil, random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef03e5fc-a971-4931-a888-06e5b72e0985",
   "metadata": {},
   "source": [
    "### Downloading the data\n",
    "\n",
    "Commands to download and uncompress the imdb dataset:\n",
    "\n",
    "```bash\n",
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz\n",
    "!rm -r aclImdb/train/unsup\n",
    "```\n",
    "\n",
    "#### Reminder\n",
    "\n",
    "In Colab, use a bang (!) to run a bash command. You can also run them in a terminal (without the initial !).\n",
    "\n",
    "#### Other reminder\n",
    "\n",
    "You can use symlinks in Linux to avoid downloading things twice!\n",
    "\n",
    "```bash\n",
    "# will create a folder called 'linked_dataset' in the current dir, linked to the other one\n",
    "!ln -s /path/to/already/downloaded/dataset linked_dataset\n",
    "```\n",
    "\n",
    "In Google drive, the same can be achieved by selecting the directory you want to link to, and pressing Shift+Z, then selecting the target directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7718009-10dc-4428-aef3-660f94261912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to split the data into train/val folders\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "train_dir = base_dir / \"train\"\n",
    "val_dir = base_dir / \"val\"\n",
    "test_dir = base_dir / \"test\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    if not os.path.isdir(val_dir / category):    # do this only once\n",
    "        os.makedirs(val_dir / category)          # make 'neg'/'pos' dir in validation\n",
    "        files = os.listdir(train_dir / category) # list files in 'train'\n",
    "        random.Random(1337).shuffle(files)       # shuffle using a seed\n",
    "        num_val_samples = int(0.2 * len(files))  # 2% of our samples for validation\n",
    "        val_files = files[-num_val_samples:]\n",
    "        for fname in val_files:                  # move our files\n",
    "            shutil.move(train_dir / category / fname,\n",
    "                        val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ac3ff05-4c93-42c5-9501-8b2092fb076d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    train_dir, batch_size=batch_size\n",
    ")\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    train_dir, batch_size=batch_size\n",
    ")\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    val_dir, batch_size=batch_size\n",
    ")\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    test_dir, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b73a8b7-4b3c-47e9-90c5-a74b705cc01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95997174-9c2a-4e32-9e4d-aa03e6579aba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Bigrams with TF-IDF encoding\n",
    "\n",
    "####  TF-IDF: Term Frequency / Inverse Document Frequency\n",
    "\n",
    "- the most frequent terms in documents tell us something about the **topic**;  \n",
    "  (\"flower\" appearing often in a doc about flowers)\n",
    "- however, **some terms** appear **often and everywhere**;  \n",
    "  (\"the\", \"a\", are frequent in the whole dataset);  \n",
    "- to counteract that, divide the frequency in one doc by the frequency in all docs:\n",
    "\n",
    "$$\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "\\text{TF-IDF} = \\frac{\\text{frequency in one document}}{\\text{frequency in all documents (dataset)}}\n",
    "}\n",
    "$$\n",
    "\n",
    "<small>[See Wikipedia the exact maths](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "926d762d-f631-4f70-8355-716a71d20402",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text_vectorization = tf.keras.layers.TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"count\" # configuring the `TextVectorization` layer to return token counts\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "209349b5-cfd3-4ebe-a4b3-0b7cd52f348f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([8. 2. 0. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "\n",
      "token    0 | frequency: 8.0 | decoded: [UNK]\n",
      "token    1 | frequency: 2.0 | decoded: the\n",
      "token  152 | frequency: 1.0 | decoded: over\n",
      "token  588 | frequency: 1.0 | decoded: over the\n",
      "token 1567 | frequency: 1.0 | decoded: dog\n",
      "token 3088 | frequency: 1.0 | decoded: fox\n",
      "token 3124 | frequency: 2.0 | decoded: quick\n",
      "token 4447 | frequency: 2.0 | decoded: brown\n",
      "token 6116 | frequency: 1.0 | decoded: jumps\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "tokens = text_vectorization(\"the quick brown fox jumps over the quick brown dog\")\n",
    "print(tokens) # instead of just 1 at the word index we get a count!\n",
    "print()\n",
    "words = tf.where(tokens > 0)\n",
    "for word in words:\n",
    "    word = word.numpy().item()\n",
    "    print(f\"token {word:>4} | frequency: {tokens[word]} | decoded: {inverse_vocab[word]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40602054-9453-4b04-bd9b-ddc5df5f470b",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text_vectorization = tf.keras.layers.TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"tf_idf\", # configuring `TextVectorization` to return TF-IDF-weighted outputs\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f3791bd-b181-424e-92e5-cf890b53cc18",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([42.640827   1.3944945  0.        ...  0.         0.         0.       ], shape=(20000,), dtype=float32)\n",
      "\n",
      "token    0 | tf-idf: 42.6408272 | decoded: [UNK]\n",
      "token    1 | tf-idf:  1.3944945 | decoded: the\n",
      "token  152 | tf-idf:  1.9256997 | decoded: over\n",
      "token  588 | tf-idf:  2.9419792 | decoded: over the\n",
      "token 1567 | tf-idf:  4.0243702 | decoded: dog\n",
      "token 3088 | tf-idf:  4.7474136 | decoded: fox\n",
      "token 3124 | tf-idf:  8.9111671 | decoded: quick\n",
      "token 4447 | tf-idf:  9.9094706 | decoded: brown\n",
      "token 6116 | tf-idf:  5.0974345 | decoded: jumps\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "tokens = text_vectorization(\"the quick brown fox jumps over the quick brown dog\")\n",
    "print(tokens) # instead of just 1 or a count at the word index we get the tf-idf quantity!\n",
    "print()\n",
    "words = tf.where(tokens > 0)\n",
    "for word in words:\n",
    "    word = word.numpy().item()\n",
    "    print(f\"token {word:>4} | tf-idf: {tokens[word]:10.7f} | decoded: {inverse_vocab[word]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6db221-a3ef-47ee-8a2d-224fe8bce583",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Training a simple model with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55e52d0b-c1ef-4f13-be21-4445af688d73",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# our datasets\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fd5365e-ab04-40e2-97c2-d84b4577b5c8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(max_tokens=20000, hidden_dim=16, clear=True):\n",
    "    \"\"\"\n",
    "    Our model-building utility\n",
    "    \"\"\"\n",
    "    if clear:\n",
    "        tf.keras.backend.clear_session()\n",
    "    inputs = tf.keras.Input(shape=(max_tokens,))\n",
    "    x = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"binary_crossentropy\", # our labels are only 0 or 1 (negative/positive)\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d9ce932-1bc9-40ba-89e9-57b619324368",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 38s 45ms/step - loss: 0.4911 - accuracy: 0.7893 - val_loss: 0.2936 - val_accuracy: 0.8748\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.3090 - accuracy: 0.8792 - val_loss: 0.2787 - val_accuracy: 0.8892\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.2739 - accuracy: 0.8952 - val_loss: 0.2950 - val_accuracy: 0.8868\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 25s 40ms/step - loss: 0.2549 - accuracy: 0.8992 - val_loss: 0.3079 - val_accuracy: 0.8792\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.2406 - accuracy: 0.9069 - val_loss: 0.3191 - val_accuracy: 0.8880\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.2259 - accuracy: 0.9119 - val_loss: 0.3218 - val_accuracy: 0.8862\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 24s 39ms/step - loss: 0.2173 - accuracy: 0.9129 - val_loss: 0.3277 - val_accuracy: 0.8836\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 23s 37ms/step - loss: 0.2115 - accuracy: 0.9153 - val_loss: 0.3407 - val_accuracy: 0.8678\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 26s 42ms/step - loss: 0.2050 - accuracy: 0.9172 - val_loss: 0.3441 - val_accuracy: 0.8756\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 25s 40ms/step - loss: 0.1924 - accuracy: 0.9204 - val_loss: 0.3535 - val_accuracy: 0.8760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f288a3ad0d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        str(base_dir / \"tfidf_2gram.h5\"),   # NOTE: in DLWP, he uses the current `.keras` format, but ModelCheckpoint\n",
    "        save_best_only=True                 # in TF 2.13 now breaks when using it with `save_best_only`...\n",
    "    )                                       # see: https://github.com/keras-team/tf-keras/issues/151\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    tfidf_2gram_train_ds.cache(),\n",
    "    validation_data=tfidf_2gram_val_ds.cache(),\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcf425ce-24db-474a-ac27-cb4cd1613d43",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.881\n"
     ]
    }
   ],
   "source": [
    "# and testing\n",
    "model = tf.keras.models.load_model(base_dir / \"tfidf_2gram.h5\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds, verbose=0)[1]:.3f}\") # this did not beat the bigram model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "colab": {
    "provenance": [],
    "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
