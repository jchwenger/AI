{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IseqBck83KLf"
   },
   "source": [
    "---\n",
    "\n",
    "## 11.3.3 Processing words as a sequence: The sequence model approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zvHILCRA3KLj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import random\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhxKc-py3KLl",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhxKc-py3KLl"
   },
   "source": [
    "A big revolution of this last decade: researchers discovered that we can get systems to **learn to project words** into a **vector space** that retain **semantic relationship**.\n",
    "\n",
    "The idea behind this discovery is that **similar words** (similar meanings) occur in **similar contexts**.\n",
    "\n",
    "*You shall know a word by the company it keeps.*  \n",
    "(J. R. Firth, \"A Synopsis of Linguistic Theory\", 1957, cf. also [the late Wittgenstein](https://plato.stanford.edu/entries/wittgenstein/#MeanUse))\n",
    "\n",
    "The algorithms will train on large text corpora, and:\n",
    "- count all the occurrences of words and contexts;  \n",
    "- try and predict either the word given the context;\n",
    "- or the context given the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R54DLm963KLl"
   },
   "source": [
    "Word embeddings in the context of a large dictionary have typically 256, 512 or 1024 dimensions  \n",
    "(the dimension of a vector is its length i.e. the number of components).\n",
    "\n",
    "One-hot encoded vectors can exceed 20,000 dimensions!  \n",
    "Also, technically all one-hot encoded are **orthogonal**: no similarity between them.\n",
    "\n",
    "Word embeddings compress the information into fewer dimensions.  \n",
    "Two word vectors can be compared to each other!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yEKI0u53KLl"
   },
   "source": [
    "<!-- <img style=\"height: 700px\" src=\"images/nlp/chollet.one-hot-embeddings.png\"> -->\n",
    "<img src=\"https://github.com/jchwenger/AI/blob/main/6-text-and-sequences/images/nlp/chollet.one-hot-embeddings.png?raw=true\">\n",
    "\n",
    "<small>DLWP, p.330</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67aFp1LR3KLm"
   },
   "source": [
    "|One-hot|Word embeddings|\n",
    "|:---|:---|\n",
    "|binary (integers: 0/1)|floating point vectors|\n",
    "|sparse (most elements are zeros)|*dense*|\n",
    "|very high-dimensional|low-to-medium-dimensional|\n",
    "|hard-coded|learnt from data|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUtbqm_r3KLm"
   },
   "source": [
    "The results is that each token will be represented as a **coordinate** (aka a **vector**) in a high-dimensional space.\n",
    "\n",
    "The most striking features of these spaces is that they seem to encode **semantic relationships**!\n",
    "\n",
    "<!-- <img src=\"images/nlp/linear-relationships.svg\"> -->\n",
    "<img src=\"https://github.com/jchwenger/AI/blob/main/6-text-and-sequences/images/nlp/linear-relationships.svg?raw=true\">\n",
    "\n",
    "\n",
    "<small>[Embeddings: Translating to a Lower-Dimensional Space, Google Foundational Courses, Machine Learning, Embeddings](https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w63zewYQ3KLm"
   },
   "source": [
    "#### Note: universal embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w63zewYQ3KLm"
   },
   "source": [
    "A universal embedding is unlikely, or very difficult to achieve (although [recent work](https://phillipi.github.io/prh/) proposes the opposite conjecture, thanks Peyton Hammersley for the references).\n",
    "\n",
    "Semantic relationships depend on task – the text corpus and what we are learning.\n",
    "\n",
    "Expect different geometries for different tasks (e.g. sentiment analysis is very different from classification of legal documents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w63zewYQ3KLm"
   },
   "source": [
    "#### Also: bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w63zewYQ3KLm"
   },
   "source": [
    "The biases of your dataset **will be encoded** in the space (for instance, gendered associations between professions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fvva2tqH3KLn",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fvva2tqH3KLn",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fvva2tqH3KLn"
   },
   "source": [
    "Google, 2013, aka the Skip-Gram model, or Continuous Bag of Words (CBOW)\n",
    "\n",
    "<small>[Mikolov et al., \"Distributed Representations of Words and Phrases and their Compositionality\", arxiv](https://arxiv.org/abs/1310.4546)</small>  \n",
    "\n",
    "Perhaps the most famous word embedding scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSXbytIV3KLn",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### GloVe: *Global Vectors for Word Representation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSXbytIV3KLn"
   },
   "source": [
    "Stanford University, 2014\n",
    "\n",
    "<small>[Pennington et al., \"GloVe: Global Vectors for Word Representation\", arxiv](https://nlp.stanford.edu/pubs/glove.pdf)</small>  \n",
    "\n",
    "\n",
    "Based on factorizing a matrix of word co-occurrence statistics.\n",
    "\n",
    "Millions of English tokens harvested from Wikipedia and other sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnrYdE8_3KLo"
   },
   "source": [
    "#### Learning word embeddings with the Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnrYdE8_3KLo"
   },
   "source": [
    "Later, researchers discovered that you can simply **learn** these vectors with your DL model using backprop like everything else!\n",
    "\n",
    "In current models, you just invoke a specific layer, and all the work is done for you.\n",
    "\n",
    "Note that you must specify in advance the **dimensionality** of the embedding space.\n",
    "\n",
    "As usual, more dimensions == more **resolution** (finer-grained), but more computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "id": "rxVYZVKS3KLo",
    "outputId": "f97babb9-3c1e-4d42-d30e-5800a3b9f002"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048,000</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8000\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8000\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │       \u001b[38;5;34m2,048,000\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,048,000</span> (7.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,048,000\u001b[0m (7.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,048,000</span> (7.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,048,000\u001b[0m (7.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_tokens = 8000\n",
    "inputs = tf.keras.Input(shape=(max_tokens,))\n",
    "embedding_layer = tf.keras.layers.Embedding( # ← EMBEDDING LAYER\n",
    "    input_dim=max_tokens,                    # the size of the vocabulary (8000)\n",
    "    output_dim=256                           # the dimensionality of the embedding space\n",
    ")\n",
    "x = embedding_layer(inputs)\n",
    "model = tf.keras.Model(inputs, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aV4BRIx23KLo"
   },
   "source": [
    "The two arguments of the embedding layer code are:\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Embedding(input_dim=max_tokens, output_dim=256)\n",
    "```\n",
    "\n",
    "- input_dim = 8000 (the size of our vocab)\n",
    "- output_dim = 256 (the dimension of the embedding space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "161aDoh33KLp"
   },
   "source": [
    "In Chollet's example (the IMDB sentiment task again), reviews have been reduced (or expanded) to a constant length of 600 words.\n",
    "\n",
    "E.g. `x_test[0] = [65, 16, 38, 1334, 88, 12, ..., 16, 5345, 19, 178, 32]`\n",
    "\n",
    "65, 16, 38... are dictionary entries – word 65, word 16, word 38... in the 8000-words dictionary.\n",
    "\n",
    "The `Embedding` layers creates a matrix of randomly initialised vectors, one for each word: $d_{vocab} \\times d_{embedding}$.\n",
    "\n",
    "Then, each elements of `x_test[0]` is used as an index to retrieve the appropriate embedding vector => 600 vectors of dimensions $256$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "161aDoh33KLp",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Note: one-hot vector matrix multiplication as index retrieval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "161aDoh33KLp"
   },
   "source": [
    "The actual operation implemented in `Keras` and other libraries does not, to my knowledge, use this, as it would require the allocation of a large sparse matrix, but it is worth noting that if your word index is encoded as a one-hot vector, a matrix multiplication will in effect retrieve the vector in the corresponding row:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & \\cdots & 1_{\\text{col r}} & \\cdots & 0\\\\\n",
    "\\end{bmatrix}\n",
    "\\overbrace{\n",
    "\\begin{bmatrix}\n",
    "\\text{e}_{11} & \\cdots & \\text{e}_{1d_e}\\\\\n",
    "\\vdots & & \\vdots \\\\\n",
    "\\color{red}{\\text{e}_{r1}} & \\cdots & \\color{red}{\\text{e}_{rd_e}} \\\\ \n",
    "\\vdots & & \\vdots \\\\\n",
    "\\text{e}_{d_v1} & \\cdots &\\text{e}_{d_vd_e}\\\\\n",
    "\\end{bmatrix}}^{\\text{all embedding vectors (rows)}} = \n",
    "\\begin{bmatrix}\n",
    "\\text{e}_{r1} & \\cdots & \\text{e}_{rd_e}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Or, column, if transposed:\n",
    "\n",
    "$$\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "\\text{e}_{11} & \\cdots & \\overbrace{\\color{red}{\\text{e}_{1r}}}^{\\text{selected embedding}} & \\cdots & \\text{e}_{1\\ d_{v}}\\\\\n",
    "\\vdots & & \\vdots & & \\vdots  \\\\\n",
    "\\text{e}_{d_e1} & \\cdots & \\color{red}{\\text{e}_{d_er}} & \\cdots & \\text{e}_{d_e\\ d_v}\\\\\n",
    "\\end{bmatrix}\n",
    "}_{\\text{all embedding vectors (columns)}}\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "\\vdots \\\\\n",
    "1_{\\text{row r}} \\\\\n",
    "\\vdots \\\\\n",
    "0\\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\text{e}_{1r}\\\\\n",
    "\\vdots \\\\\n",
    "\\text{e}_{d_er}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $e_{ij}$ represents a small number in the embedding vector\n",
    "- $d_v$ is the vocabulary dimension (`input_dim`)\n",
    "- $d_e$ is the embedding dimension (`output_dim`)\n",
    "\n",
    "In our case, a vanilla way of retrieving the vectors would bbe to have our sequence of integers encoded as matrix of one-hot vectors, dimensions: $600 \\times 8'000$, which would then be multiplied by an embedding matrix of $8'000 \\times 256$ of dimensions $256 \\times 8'000$ => a sequence of embeddings, 600 floating point vectors of length 256, or $600 \\times 256$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Embedding layer: \\# of parameters and dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcqe4W5r3KLp"
   },
   "source": [
    "How many learnable parameters does the embedding weight matrix have?\n",
    "\n",
    "$$\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "\\mathrm{input\\_dim} \\times \\mathrm{output\\_dim}\n",
    "}\n",
    "$$\n",
    "\n",
    "That is:\n",
    "\n",
    "$$\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "\\mathrm{vocab\\_size} \\times \\mathrm{embed\\_size}\n",
    "}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$ 10'000 \\times 8 = 80'000$ elements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lK6nk0K3KLp"
   },
   "source": [
    "The embedding layer takes as input tensors of shape `(batch_size, sequence_length)`.  \n",
    "The outputs tensors of shape `(bach_size, sequence_length, output_dim)`.\n",
    "\n",
    "As usual in Keras, the batch_size is represented as `None`:\n",
    "\n",
    "$$\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "In: (None, sequence\\_length) \\to Out: (None, sequence\\_length, output\\_dim)\n",
    "}\n",
    "$$\n",
    "\n",
    "`output_dim` could be called `embed_dim`, the number of dimensions of our embedding space!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDWE4G4W3KLp",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### A first practical example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDWE4G4W3KLp"
   },
   "source": [
    "Sentiment analysis on the IMDB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvSjstDz3KLp",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Downloading the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bZEh6hCc4HGO"
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = pathlib.Path(\"aclImdb\")\n",
    "\n",
    "if \"google.colab\" in sys.modules and not DATASET_DIR.exists():\n",
    "    !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    !tar -xf aclImdb_v1.tar.gz # this untars the archive to a folder called aclImdb\n",
    "    !rm -r aclImdb/train/unsup\n",
    "\n",
    "MODELS_DIR = pathlib.Path(\"models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_mIqUOaK3KLp"
   },
   "outputs": [],
   "source": [
    "# code to split the data into train/val folders\n",
    "DATASET_DIR = pathlib.Path(\"aclImdb\")\n",
    "TRAIN_DIR = DATASET_DIR / \"train\"\n",
    "VAL_DIR = DATASET_DIR / \"val\"\n",
    "TEST_DIR = DATASET_DIR / \"test\"\n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    if not os.path.isdir(VAL_DIR / category):    # do this only once\n",
    "        os.makedirs(VAL_DIR / category)          # make 'neg'/'pos' dir in validation\n",
    "        files = os.listdir(TRAIN_DIR / category) # list files in 'train'\n",
    "        random.Random(1337).shuffle(files)       # shuffle using a seed\n",
    "        num_val_samples = int(0.2 * len(files))  # 2% of our samples for validation\n",
    "        val_files = files[-num_val_samples:]\n",
    "        for fname in val_files:                  # move our files\n",
    "            shutil.move(TRAIN_DIR / category / fname,\n",
    "                        VAL_DIR / category / fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvSjstDz3KLp",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Processing using `text_dataset_from_directory`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8WyI2ov3KLp"
   },
   "source": [
    "The [`tf.keras.utils.text_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/text_dataset_from_directory) layer, for a directory structure like so:\n",
    "```\n",
    "main_directory/\n",
    "...class_a/\n",
    "......a_text_1.txt\n",
    "......a_text_2.txt\n",
    "...class_b/\n",
    "......b_text_1.txt\n",
    "......b_text_2.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ScT4WfaB3KLp",
    "outputId": "f201f14a-59e6-4e2f-f160-8d0f9430b2bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# each of these iterables returns tuples containing two tensors:\n",
    "# samples, shape: (batch_size, sample_shape) ← our texts\n",
    "# targets, shape: (batch_size,)              ← 0 or 1\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    TRAIN_DIR, batch_size=BATCH_SIZE\n",
    ")\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    VAL_DIR, batch_size=BATCH_SIZE\n",
    ")\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    TEST_DIR, batch_size=BATCH_SIZE\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CKlRfhk43KLp"
   },
   "outputs": [],
   "source": [
    "# Preparing integer sequence train/val/test datasets\n",
    "max_length = 600   # we cut our sequences to 600 words max! (For memory.) This will affect performance...\n",
    "max_tokens = 8000\n",
    "\n",
    "text_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvSjstDz3KLp",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Training with `one_hot` vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fjwCFnJ-3KLq"
   },
   "outputs": [],
   "source": [
    "# A sequence model built on one-hot encoded vector sequences\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "# ↓ our one-hot vectors --------------------------------------------\n",
    "embedded = tf.keras.ops.one_hot(inputs, max_tokens) # tf.one_hot incompatible with keras in new versions\n",
    "# ---------------------------------------------------- passed here ↓\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(embedded)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "4c-Bma3d3KLq",
    "outputId": "56470998-4a06-447d-8552-3f20a9739639"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ one_hot (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">OneHot</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,056,448</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ one_hot (\u001b[38;5;33mOneHot\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8000\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │       \u001b[38;5;34m2,056,448\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,056,513</span> (7.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,056,513\u001b[0m (7.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,056,513</span> (7.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,056,513\u001b[0m (7.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7tsZPct3KLq",
    "outputId": "8473d99a-60c5-4738-facf-faf91fea9a79",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 174ms/step - accuracy: 0.6192 - loss: 0.6332 - val_accuracy: 0.8436 - val_loss: 0.4016\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 186ms/step - accuracy: 0.8504 - loss: 0.3838 - val_accuracy: 0.7244 - val_loss: 0.5503\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 188ms/step - accuracy: 0.8750 - loss: 0.3252 - val_accuracy: 0.8628 - val_loss: 0.3209\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 174ms/step - accuracy: 0.8979 - loss: 0.2863 - val_accuracy: 0.8762 - val_loss: 0.3561\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 173ms/step - accuracy: 0.9151 - loss: 0.2568 - val_accuracy: 0.8658 - val_loss: 0.3175\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 189ms/step - accuracy: 0.9231 - loss: 0.2262 - val_accuracy: 0.8692 - val_loss: 0.3277\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 188ms/step - accuracy: 0.9289 - loss: 0.2180 - val_accuracy: 0.8756 - val_loss: 0.3324\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 173ms/step - accuracy: 0.9349 - loss: 0.1994 - val_accuracy: 0.8390 - val_loss: 0.5477\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 186ms/step - accuracy: 0.9446 - loss: 0.1794 - val_accuracy: 0.8760 - val_loss: 0.3581\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 172ms/step - accuracy: 0.9478 - loss: 0.1680 - val_accuracy: 0.8700 - val_loss: 0.4435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7c274494c0a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a first basic sequence model\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        str(MODELS_DIR / \"one_hot_bidir_lstm.keras\"),\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "model.fit(\n",
    "    int_train_ds,\n",
    "    validation_data=int_val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pa5j8-bS3KLq",
    "outputId": "26bd556f-83ce-4b76-e2c1-ef472e28996c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.855\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(MODELS_DIR / \"one_hot_bidir_lstm.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds, verbose=0)[1]:.3f}\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YJNrm7a3KLq",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Learning word embeddings with the `Embedding` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XZWL0rE43KLq"
   },
   "outputs": [],
   "source": [
    "# A model that uses an `Embedding` layer trained from scratch\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "# ↓ our embedding layer --------------------------------------------\n",
    "embedded = tf.keras.layers.Embedding(\n",
    "    input_dim=max_tokens, # our data comes in with a vocab size of `max_tokens`\n",
    "    output_dim=256        # and comes out as dense vectors of dim (\"vocab\") of 256\n",
    ")(inputs)\n",
    "# ---------------------------------------------------- passed here ↓\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(embedded)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjoBADrl3KLq"
   },
   "source": [
    "Note that given our vocabulary of $8'000$ tokens the embedding layer is **large**: $256 \\times 8'000 = 2'048'000$ elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "Z7PvYO-S3KLr",
    "outputId": "b9edf78f-7439-441c-8aa0-2d2702b4b103"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,984</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │       \u001b[38;5;34m2,048,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m73,984\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,122,049</span> (8.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,122,049\u001b[0m (8.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,122,049</span> (8.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,122,049\u001b[0m (8.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lnljWYBu3KLr",
    "outputId": "16a724ce-1590-4fee-da1c-f5b9d04937c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 60ms/step - accuracy: 0.6196 - loss: 0.6325 - val_accuracy: 0.8026 - val_loss: 0.4396\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 53ms/step - accuracy: 0.8230 - loss: 0.4248 - val_accuracy: 0.8142 - val_loss: 0.4022\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 58ms/step - accuracy: 0.8603 - loss: 0.3736 - val_accuracy: 0.8580 - val_loss: 0.3404\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 57ms/step - accuracy: 0.8800 - loss: 0.3204 - val_accuracy: 0.8484 - val_loss: 0.3408\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 73ms/step - accuracy: 0.9046 - loss: 0.2642 - val_accuracy: 0.8664 - val_loss: 0.3336\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 55ms/step - accuracy: 0.9216 - loss: 0.2273 - val_accuracy: 0.8730 - val_loss: 0.3516\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 58ms/step - accuracy: 0.9292 - loss: 0.2046 - val_accuracy: 0.8658 - val_loss: 0.3490\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 55ms/step - accuracy: 0.9408 - loss: 0.1761 - val_accuracy: 0.8648 - val_loss: 0.3656\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.9494 - loss: 0.1527 - val_accuracy: 0.8688 - val_loss: 0.3915\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 65ms/step - accuracy: 0.9590 - loss: 0.1288 - val_accuracy: 0.8572 - val_loss: 0.3907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7c27ac0f5c60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        str(MODELS_DIR / \"embeddings_bidir_lstm.keras\"),\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "model.fit(\n",
    "    int_train_ds,\n",
    "    validation_data=int_val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKnhXZBI3KLr",
    "outputId": "b78bf3ef-8e32-4391-a883-2b53b9e6028a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.857\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(MODELS_DIR / \"embeddings_bidir_lstm.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds, verbose=0)[1]:.3f}\") # in this case, roughly the same as one-hot\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCE2Nin53KLr",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### Using pretrained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCE2Nin53KLr"
   },
   "source": [
    "Pretrained word embeddings are useful **when training data is limited** – just as with pretrained convnets.\n",
    "\n",
    "Very structured embeddings hopefully capture **generic structure** appropriate to diverse domains.\n",
    "\n",
    "(The more data you can train on, the more likely your task-specific embeddings will perform better.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_BQTVat3KLr"
   },
   "source": [
    "Let's see how we can use GloVe embeddings in `tensorflow.keras`.\n",
    "\n",
    "(The same method applies to Word2Vec or any other embedding technique.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ngnlgw593KLr",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_n4vKQNb3KLr"
   },
   "outputs": [],
   "source": [
    "GLOVE_DIR = pathlib.Path(\"glove\") # I have my file in a folder called 'glove'\n",
    "\n",
    "if not DATASET_DIR.exists():\n",
    "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    !unzip -q glove.6B.zip -d glove # unzip to a directory called \"glove\"\n",
    "    !rm glove.6B.zip                # remove the zip file\n",
    "\n",
    "PATH_TO_GLOVE_FILE =  GLOVE_DIR / \"glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ngnlgw593KLr",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0xEWKiTL3KLx",
    "outputId": "6b93a03c-dbed-467d-cb8d-e8b018b27217",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n"
     ]
    }
   ],
   "source": [
    "# another Jupyter magic: use $python_variable in bash commands\n",
    "!head -n 1 $PATH_TO_GLOVE_FILE\n",
    "# ↓ the word \"the\" followed by its coordinates in a 100-dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ngnlgw593KLr",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Import into an `Embedding` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q0bbPEiI3KLx",
    "outputId": "39c0d30d-057c-40bb-c359-ef9f4c2ea760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400,000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# parsing the GloVe word-embeddings file\n",
    "embeddings_index = {}                                   # our dictionary: {'word': np.array([...coordinates..])}\n",
    "with open(PATH_TO_GLOVE_FILE) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)            # split: word | coordinates\n",
    "        coefs = np.fromstring(coefs, \"float\", sep=\" \")  # load string floats into numpy, space-separated\n",
    "        embeddings_index[word] = coefs                  # save into dictionary\n",
    "\n",
    "print(f\"Found {len(embeddings_index):,} word vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MlQs8SOV3KLx"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "# we reuse the same TextVectorization object as earlier, turning our sentences into integers\n",
    "# max_length: 600, max_tokens: 8000\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "# preparing the GloVe word-embeddings matrix\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim))     # create a matrix (max_tokens, embedding_dim)\n",
    "for word, i in word_index.items():                           # looping through our vocab\n",
    "    if i < max_tokens:                                       # don't try and retrieve beyond max_tokens\n",
    "        embedding_vector = embeddings_index.get(word)        # try and get the vector associated with the word\n",
    "    if embedding_vector is not None:                         # if the vector exists\n",
    "        embedding_matrix[i] = embedding_vector               # assign it to our matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "CnGyDuqn3KLx"
   },
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    max_tokens,\n",
    "    embedding_dim,        # using our embedding matrix through an initializer\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,      # WE DO NOT TRAIN IT!\n",
    "    mask_zero=True,\n",
    ")\n",
    "\n",
    "# Given that our network is initialized randomly, the massive changes it undergoes at the beginning\n",
    "# of training would certainly affect/damage the representations in our embedding matrix\n",
    "# (same scenario as with pretrained ConvNets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Define model & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XETTfelE3KLx"
   },
   "outputs": [],
   "source": [
    "# A model that uses a pretrained Embedding layer\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "# ↓ our embedding layer --------------------------------------------\n",
    "embedded = embedding_layer(inputs)\n",
    "# ---------------------------------------------------- passed here ↓\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(embedded)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "7tsXPqN_3KLx",
    "outputId": "1d7ca4ab-65ab-480a-e3b2-ba19504a8284"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">800,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ bidirectional             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">34,048</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)           │                        │                │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)      │        \u001b[38;5;34m800,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ bidirectional             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m34,048\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)           │                        │                │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m65\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">834,113</span> (3.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m834,113\u001b[0m (3.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,113</span> (133.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,113\u001b[0m (133.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">800,000</span> (3.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m800,000\u001b[0m (3.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJpAwD273KLx",
    "outputId": "aaaa8ffc-ab73-4b52-9b7d-3823ba6c4e49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 55ms/step - accuracy: 0.6133 - loss: 0.6418 - val_accuracy: 0.7982 - val_loss: 0.4468\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 58ms/step - accuracy: 0.7835 - loss: 0.4693 - val_accuracy: 0.8238 - val_loss: 0.3928\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 54ms/step - accuracy: 0.8112 - loss: 0.4135 - val_accuracy: 0.8370 - val_loss: 0.3720\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 80ms/step - accuracy: 0.8340 - loss: 0.3787 - val_accuracy: 0.8446 - val_loss: 0.3593\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 55ms/step - accuracy: 0.8500 - loss: 0.3477 - val_accuracy: 0.8596 - val_loss: 0.3285\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 62ms/step - accuracy: 0.8613 - loss: 0.3282 - val_accuracy: 0.8602 - val_loss: 0.3192\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 52ms/step - accuracy: 0.8681 - loss: 0.3095 - val_accuracy: 0.8330 - val_loss: 0.3783\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 62ms/step - accuracy: 0.8785 - loss: 0.2951 - val_accuracy: 0.8766 - val_loss: 0.3029\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 66ms/step - accuracy: 0.8903 - loss: 0.2741 - val_accuracy: 0.8764 - val_loss: 0.3014\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 59ms/step - accuracy: 0.8921 - loss: 0.2640 - val_accuracy: 0.8554 - val_loss: 0.3596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7c27347d36d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        str(MODELS_DIR / \"glove_embeddings_sequence_model.keras\"),\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "model.fit(\n",
    "    int_train_ds,\n",
    "    validation_data=int_val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "crXYCzYd3KLx",
    "outputId": "a7859de2-c392-4a2b-eb45-9d647b5de753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.858\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {model.evaluate(int_test_ds, verbose=0)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4HvuEo0IV1N",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Save models to Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "61lf2L3YWQl9"
   },
   "outputs": [],
   "source": [
    "EXPORT=False\n",
    "\n",
    "if EXPORT:\n",
    "    # zip models\n",
    "    !zip sequences.models.zip {MODELS_DIR}/*\n",
    "    # connect to drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # copy zip to drive (adjust folder as needed)\n",
    "    !cp sequences.models.zip drive/MyDrive/IS53024B-Artificial-Intelligence/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OOtyS8I3KLy",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OOtyS8I3KLy"
   },
   "source": [
    "### Word embbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OOtyS8I3KLy"
   },
   "source": [
    "- **Various kinds of word encodings**:\n",
    "  - **one-hot/multi-hot**: the presence of words is marked by a 1 (binary) → *sparse* & *hard-coded*\n",
    "  - **word embeddings**: project words/tokens into vector spaces where collocations between words (\"the company a word keeps\") are modeled as the distance between vectors. → *dense* & *learnt from data*\n",
    "- Two most important embedding models\n",
    "    - **Word2Vec** from Google\n",
    "    - **GloVe** from Stanford\n",
    "- **Embedding layers** can be trained end to end with your net!\n",
    "- **Pretrained embeddings** can also be used on top of your own models, like pretrained networks!\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
