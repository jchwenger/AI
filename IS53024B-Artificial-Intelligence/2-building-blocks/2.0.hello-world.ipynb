{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500990f-be29-40d5-b010-0f61f40c3d81",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images:\t\tshape: (60000, 28, 28)\n",
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "def info(s, x):\n",
    "    print(f\"{s}:\\t\\tshape: {x.shape}\\n{x}\")\n",
    "    \n",
    "info('train images', train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ba856b2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels:\t\tshape: (60000,)\n",
      "[5 0 4 ... 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "info('train labels', train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fb8013b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train label [0] = 5\n",
      "train image [0]:\t\tshape: (28, 28)\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253 253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201  78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# info('test images', test_images)\n",
    "# info('test labels', test_labels)\n",
    "\n",
    "info('train label [0] =', train_labels[0])\n",
    "info('train image [0]', train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf7ca5f-b9fa-4aa6-afb4-e0306106cb5a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Workflow\n",
    "\n",
    "1. **Prepare** the training data;\n",
    "\n",
    "2. **Train** the network to associate images and labels; \n",
    "\n",
    "3. **Predict** (also called \"infer\") on test images, that the network hasn't seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd735cf-16c3-4089-b3b7-2ddb21cc2cd0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Layers\n",
    "\n",
    "- The core building blocks are **layers** of artificial neurons/units;\n",
    "\n",
    "- Each layer is a **data-processing module (transformation)**;\n",
    "\n",
    "- Layers transform input data into more abstract **representations**;\n",
    "\n",
    "- Layers are chained together in a **dense** network;\n",
    "\n",
    "- A DL model is a succession of increasingly refined data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd631100-2fb5-46c4-a814-5b87eb366ab5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The network\n",
    "\n",
    "- In this case: the network is a sequence of two dense layers. Dense means **fully-connected**:  the output of a unit in layer $k$ is linked to all units in layer $k + 1$;\n",
    "\n",
    "- The input in the diagram emanates from a non-computational layer; <img src=\"images/tikz12.png\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aab194",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The network\n",
    "\n",
    "- This input layer is not a conventional layer because it does not perform any computation.\n",
    "\n",
    "- The units in the first layer have a **Rectified Linear (RELU)** activation function (aka positive part):\n",
    "\n",
    "$$ relu(x) = \\max(0, x)$$\n",
    "\n",
    "- The second (and last) layer will be a 10-way **softmax** layer which means it will return an array of 10 scores, each an **interval**  $ \\bf [0, 1]$, with all scores *summing to 1*;\n",
    "\n",
    "- The scores can be interpreted as a **probability distribution** – the probability that the current digit image belongs to one of the 10 digit classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fb139b3-63a6-4b2c-8dd9-24b57bc948db",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation = 'relu', input_shape = (28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c37a413-b26d-4089-b929-4e860cba798b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss, optimiser and performance metrics\n",
    "\n",
    "- The **loss function**, the *categorical cross-entropy*, measures how well the network classifies the training data.\n",
    "\n",
    "- The **optimiser**: the network update algorithm. The update depends on the bundle of input data (a *mini-batch*) and the loss function.\n",
    "\n",
    "- **Metrics** monitor training and testing. Here we will only monitor **accuracy** (the fraction of the images that are correctly classified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b539355-fb39-45b4-b3cc-1b9e1da7690e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "network.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e3439-7a56-469b-b4f7-978fd2810b29",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preparing the image data\n",
    "\n",
    "- Data is generally preprocessed (reshaped, normalised).\n",
    "\n",
    "- The MNIST training images are stored in an array of 60000 pixel maps of 28 x 28 pxl of type `uint8` and values range in {0, 1, ..., 255}.\n",
    "\n",
    "- The array is reshaped into an array of **shape** (60000, 28 * 28) and **dtype** `float32`, floating point values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12ded9ba-6102-4a3c-bf02-8c053ec6c26f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63125bac-0911-423e-b349-5b41f526476b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The labels are categorically encoded (Keras comes with a nifty function to do just that).\n",
    "\n",
    "Instead of integers, we have a 1 at the *index* of the class, and zeros elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "271d93c0-e2da-4003-8e5c-ea988ddbdec9",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(train_labels[0:4])\n",
    "train_labels = to_categorical(train_labels) \n",
    "print(train_labels[0:4])\n",
    "\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c1ccd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Before we train: what's our starting point?\n",
    "\n",
    "- Let's see what our model can do without any training.\n",
    "\n",
    "- We `evaluate` it on the test set, as we will do later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6991f09a-fcf5-4342-af69-74d05b230c16",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 2.3064 - accuracy: 0.0998\n",
      "\n",
      "test_acc: 0.0997999981045723\n",
      "1 would have been perfect...ground breaking! o(〒﹏〒)o\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "\n",
    "print()\n",
    "print('test_acc:', test_acc)\n",
    "print('1 would have been perfect...ground breaking! o(〒﹏〒)o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba4d01-0534-4b95-b4be-1f5f71ba94c2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training \n",
    "\n",
    "- All set to train!The model is *fitted* to its training data (= **training**). In Keras: `model.fit()`.\n",
    "\n",
    "- Two quantities will be displayed during training:\n",
    "    - the **loss** of the network over the training data;\n",
    "    - the **accuracy** of the network over the training data.\n",
    "\n",
    "- An **epoch** is a complete pass over the training set.\n",
    "\n",
    "- Each pass is split into **mini-batches** (number of sample processed in one go)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2503f34c-8f91-4d7e-bce3-a56ddae2cfb8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.2594 - accuracy: 0.9229\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1060 - accuracy: 0.9690\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0690 - accuracy: 0.9794\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0509 - accuracy: 0.9847\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0382 - accuracy: 0.9887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1fd0659f10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a51aa45-2f8b-48ca-bc1c-ac0fa3f22cbd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Testing\n",
    "\n",
    "- Training accuracy is about 98.9% after five training epochs...\n",
    "\n",
    "- How does the model performs on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e498a674-fe9a-426f-ae1b-7a91f9748567",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0671 - accuracy: 0.9797\n",
      "\n",
      "test_acc: 0.9797000288963318\n",
      "Yay! ٩(◕‿◕｡)۶\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "\n",
    "print()\n",
    "print('test_acc:', test_acc)\n",
    "print('Yay! ٩(◕‿◕｡)۶')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318883ea-3c0a-49e9-8e4b-3464991d53a8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "- Test set accuracy ~98% is lower than the training set accuracy.\n",
    "\n",
    "- This is to be expected! (You do better on a test you prepared than on the exam you haven't seen!)\n",
    "\n",
    "- This gap between *training accuracy* and *test accuracy* will become very important: it is called **overfitting**.\n",
    "\n",
    "- Machine learning models tend to perform worse on new data than on their training data. (But, honestly, who doesn't??)\n",
    "\n",
    "- Intuitively, what happens is that  the network *overadapts* to the training data, creating an excessively precise representation, which can perform poorly on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30bbafa-bebe-496a-9b81-3c62b9107ab1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What's next?\n",
    "\n",
    "- Clarify what is really going on behind the scenes;\n",
    "\n",
    "- Tensors & tensor operations recap;\n",
    "\n",
    "- Gradient descent – how the network actually trains."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
