{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics 1, 2, 3 and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "```\n",
    "np.array([[1, 2], [3, 4]])\n",
    "rank = ???\n",
    "shape = ???\n",
    "```\n",
    "\n",
    "**Rank = 2 and shape = (2, 2)**\n",
    "```\n",
    "x = [[1, 2]\n",
    "     [3, 4]]\n",
    "```\n",
    "a $2 \\times 2$ matrix. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "```\n",
    "np.array([[1, 2], [3, 4]])\n",
    "x[1] = ???\n",
    "```\n",
    "\n",
    "**[3, 4]**\n",
    "\n",
    "```\n",
    "a = [1, 2]\n",
    "b = [3, 4]\n",
    "x = [a, b]\n",
    "x[1] = b = [3, 4]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "```\n",
    "input_shape = (60000, 28, 28)\n",
    "num_samples = ???\n",
    "```\n",
    "\n",
    "**60000 samples**\n",
    "\n",
    "The samples axis is always the first axis. Each sample is a rank 2 tensor of shape ```(28, 28)```.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "```\n",
    "x = train_images[:128]\n",
    "num_samples = ???\n",
    "```\n",
    "\n",
    "\n",
    "**128 samples**\n",
    "\n",
    "```a:b``` is a slice from index ```a``` inclusive to ```b``` exclusive.\n",
    "\n",
    "```[:b]``` is the same as ```[0:b]```\n",
    "\n",
    "```train[images[0]``` to ```train_images[127]```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "```\n",
    "second_mini_batch_of_128 = train_images([p:q])\n",
    "p = ???\n",
    "q = ???\n",
    "```\n",
    "\n",
    "\n",
    "**p = 128, q = 256**\n",
    "\n",
    "First batch is [0:128], second is [128:256] etc. Batch ```n``` is the slice ```[n * 128:(n + 1) * 128]```. First batch is batch 0, second batch is batch 1 and so on. Remember 0 = first, 1 = second etc.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "```\n",
    "sigmoid([-1, 0, 1]) = ???\n",
    "```\n",
    "\n",
    "\n",
    "**```sigmoid([-1, 0, 1])= [0.5 - a, 0.5, 0.5 + a]``` where $0 < a < 0.5$**\n",
    "\n",
    "In fact $a = 0.23$.\n",
    "\n",
    "```sigmoid(0) = 0.5, sigmoid(inf) = 1, sigmoid(-inf) = 0```, rotational symmetry about $(0, 0.5)$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7\n",
    "```\n",
    "w = [[1, 0],\n",
    "     [1, 1]],\n",
    "b = [-2, 2],\n",
    "x = [-1, 1]\n",
    "relu(dot(w, x) + b) = ???\n",
    "```\n",
    "\n",
    "**```[0, 2]```**\n",
    "\n",
    "$w \\cdot x = [[1, 0] \\cdot [-1, 1]^T, [1, 1] \\cdot [-1, 1]^T] =  [-1, 0]^T$\n",
    "\n",
    "$[-1, 0]^T + [-2, 2]^T = [-3, 2]^T$\n",
    "\n",
    "$\\text{relu}([-3, 2]^T) = [0, 2]^T$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8\n",
    "Suppose the third item in a word dictionary is 'deep' and the seventh item is 'learning'. What is the one-hot encoding of 'deep learning'?\n",
    "\n",
    "**(0, 0, 0, 1, 0, 0, 0, 1, 0, 0...)**\n",
    "\n",
    "The index 0 is not used. The first dictionary word has index 1, the second word has index 2 and so on. So, place 1's at indices 3 and 7.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation = 'relu', input_shape = (10000,))) # layer 1\n",
    "model.add(layers.Dense(32, activation = 'relu')) # layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the shape of:\n",
    "1. Layer 1 weight tensor\n",
    "2. Layer 1 bias tensor\n",
    "3. Layer 2 weight tensor\n",
    "4. Layer 2 bias tensor?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "1. (10000, 64)\n",
      "2. (64,)\n",
      "3. (64, 32)\n",
      "4. (32,)\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m') # print bold\n",
    "print('1.', model.layers[0].get_weights()[0].shape) # first layer weights\n",
    "print('2.', model.layers[0].get_weights()[1].shape) # first layer bias\n",
    "print('3.', model.layers[1].get_weights()[0].shape) # second layer weights\n",
    "print('4.', model.layers[1].get_weights()[1].shape) # second layer bias\n",
    "print('\\033[0m') # print back to normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10\n",
    "```\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation = 'relu', input_shape = (28*28,)))\n",
    "model.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "```\n",
    "\n",
    "What problem type has the above network been designed to solve?\n",
    "\n",
    "\n",
    "**Single label, multi-class**\n",
    "\n",
    "The final layer has 10 units - there are 10 outputs, so this must be a multi-class problem *and* multi-claaa, sngle layer requires a softmax last layer activation (see DLWP Table 4.1).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11\n",
    "```\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation = 'relu', input_shape = (28*28,)))\n",
    "model.add(layers.Dense(10, activation = 'softmax'))\n",
    "number_input_elements = ???\n",
    "```\n",
    "\n",
    "How many input elements are implied in the above network?\n",
    "\n",
    "**784 ($\\bf 28 \\times 28$)**\n",
    "\n",
    "Inputs to a dense FF network are vectors. The shape of a vector is ```(num_elements, )```.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12\n",
    "```\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation = 'relu', input_shape = (28 * 28,))) # layer 1\n",
    "model.add(layers.Dense(10, activation = 'softmax')) # layer 2\n",
    "\n",
    "```\n",
    "1. ```number_of_layer_1_trainable_params = ```\n",
    "2. ```number_params_layer_2_weight_tensor = ```\n",
    "\n",
    "\n",
    "1. **```number_of_layer_1_trainable_params = 784 * 512 + 512 = 401920```**\n",
    "\n",
    "2. **```number_params_layer_2_weight_tensor = 512 * 10 + 10 = 5130```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation = 'relu', input_shape = (28 * 28,))) # layer 1\n",
    "model.add(layers.Dense(10, activation = 'softmax')) # layer 2\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q13\n",
    "```\n",
    "softmax([1, 10, 1]) = [a, b, c]\n",
    "```\n",
    "\n",
    "What can you say about the relative values of $a$, $b$ and $c$?\n",
    "\n",
    "**$\\bf a = c$ and $\\bf 0 < a < b$**\n",
    "\n",
    "Softmax is applied element-wise. All softmax'd elements are greater than zero and the order of sizes is preserved (in fact, amplified).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q14\n",
    "```\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation = 'relu', input_shape = (28*28,)))\n",
    "model.add(layers.Dense(x, activation = 'relu'))\n",
    "model.add(layers.Dense(10, activation = 'softmax'))\n",
    "```\n",
    "\n",
    "What is the range of possible value for ```x```, the number of units in the third layer?\n",
    "\n",
    "$\\bf x \\ge 10$\n",
    "\n",
    "Bottlenecks may occur when the number of units of a preceding layer is smaller.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q15\n",
    "```\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation = 'relu', input_shape = (28*28,)))\n",
    "model.add(layers.Dense(10, activation = 'softmax'))\n",
    "model.compile(optimizer = 'rmsprop', loss = unknown_loss, metrics = [unknown_metric])\n",
    "```\n",
    "\n",
    "1. ```unknown_loss = ```\n",
    "2. ```unknown_metric = ```\n",
    "\n",
    "1. **unknown loss = ```categorical_crossentropy```**\n",
    "\n",
    "2. **unknown metric = accuracy or ```'acc'```**\n",
    "\n",
    "This is a single label multi-class problem - refer to DLWP Table 4.1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q16\n",
    "\n",
    "Consider ```x``` below.\n",
    "\n",
    "1. What is the rank of `x`?\n",
    "2. What is the shape of `x`?\n",
    "3. What is `x[0]`?\n",
    "4. What is `x[0][1]`?\n",
    "5. What is `x[0][1][0]`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[1 0]\n",
      "  [0 1]]\n",
      "\n",
      " [[2 2]\n",
      "  [2 2]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[[1, 2], [3, 4]], \n",
    "              [[1, 0], [0, 1]], \n",
    "              [[2, 2], [2, 2]]])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "1. 3\n",
      "2. (3, 2, 2)\n",
      "3.\n",
      " [[1 2]\n",
      " [3 4]]\n",
      "4. [3 4]\n",
      "5. 3\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m') # print bold\n",
    "print('1.', x.ndim)\n",
    "print('2.', x.shape)\n",
    "print('3.\\n', x[0])\n",
    "print('4.', x[0][1])\n",
    "print('5.', x[0][1][0])\n",
    "print('\\033[0m') # print back to normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q17\n",
    "\n",
    "Consider ```x``` below.\n",
    "\n",
    "1. What is the rank of `x`?\n",
    "2. What is the shape of `x`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print('x = ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "1. 1\n",
      "2. (3,)\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m') # print bold\n",
    "print('1.', x.ndim)\n",
    "print('2.', x.shape)\n",
    "print('\\033[0m') # print back to normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q18\n",
    "Consider the mathematical vector $x = (1, 2, 3)$\n",
    "1. How many elements does $x$ have?\n",
    "2. What is the dimensionality of $x$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **3**\n",
    "2. **3**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q19\n",
    "\n",
    "For ```w, b, x``` below,\n",
    "\n",
    "1. What is $w \\cdot x$?\n",
    "2. What is $w \\cdot x + b$?\n",
    "3. What is ```relu```$(w \\cdot x + b)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = \n",
      " [[ 1  0  2]\n",
      " [-2  1 -1]]\n",
      "b =  [1 2]\n",
      "x =  [2 0 1]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[1, 0, 2], [-2, 1,-1]])\n",
    "b = np.array([1, 2])\n",
    "x = np.array([2, 0, 1])\n",
    "\n",
    "print('w = \\n', w)\n",
    "print('b = ', b)\n",
    "print('x = ', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "1. [ 4 -5]\n",
      "2. [ 5 -3]\n",
      "3. [5 0]\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m') # print bold\n",
    "print('1.', np.dot(w, x))\n",
    "print('2.', np.dot(w, x) + b)\n",
    "ans = np.array([5, 0])\n",
    "print('3.', ans)\n",
    "print('\\033[0m') # print back to normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q20\n",
    "\n",
    "With regard to the model below:\n",
    "\n",
    "Fill in the blanks\n",
    "\n",
    "| Layer (type) | Output Shape | Param # |\n",
    "|--------------|--------------|---------|\n",
    "dense_1 (Dense)| (None, ___ ) |  ___    |\n",
    "dense_2 (Dense)| (None, ___ ) |  ___    |\n",
    "\n",
    "Total params:____\n",
    "\n",
    "Trainable params:____\n",
    "\n",
    "\n",
    "Non-trainable params: ____\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(784, ))) # dense_1\n",
    "model.add(layers.Dense(10, activation = 'softmax'))                  # dense_2 \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q21\n",
    "\n",
    "$x = (1, 2, 1, 1)$. \n",
    "\n",
    "1. What is softmax$(x)$?\n",
    "2. What is the sum of the elements of softmax$(x)$?\n",
    "3. Is softmax$(x)$ a probability distribution? Why/why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "1. [0.1748777  0.47536689 0.1748777  0.1748777 ]\n",
      "2. 1.0000000000000002\n",
      "3. For all intent and purposes, the output of `softmax` is a probability distribution:\n",
      "   it preserves the relative weight of the elements of its input, always outputs positive\n",
      "   numbers, and scales them so that the total always sums to 1.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def softmax(x):\n",
    "    y = np.exp(x)\n",
    "    return y / np.sum(y)\n",
    "\n",
    "x = np.array([1, 2, 1, 1])\n",
    "print('\\033[1m') # print bold\n",
    "print('1.', softmax(x))\n",
    "print('2.', sum(softmax(x)))\n",
    "print('3. For all intent and purposes, the output of `softmax` is a probability distribution:')\n",
    "print('   it preserves the relative weight of the elements of its input, always outputs positive')\n",
    "print('   numbers, and scales them so that the total always sums to 1.')\n",
    "print('\\033[0m') # print back to normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q22\n",
    "\n",
    "An image in the MNIST training is labelled '7'. \n",
    "\n",
    "What is the categorical encoding of this label?\n",
    "\n",
    "$\\bf (0, 0, 0, 0, 0, 0, 0, 1, 0, 0)$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q23\n",
    "\n",
    "1. What is an affine transformation?\n",
    "2. What is an activation function?\n",
    "3. What are the essential properties of an activation function?\n",
    "4. What is the action of an activated layer on an input vector?\n",
    "5. Sketch the form of the sigmoid function.\n",
    "\n",
    "\n",
    "**1. A linear transform followed by a translation i.e. $\\bf w \\cdot x + b$ where $w$ is the weight matrix $\\bf b$ is the bias vector and $\\bf x$ is the layer input.**  \n",
    "**2. An element-wise application of a function to $\\bf w.x + b$**  \n",
    "**3. Non-linearity and differentiability**  \n",
    "**4. An affine transformation followed by an activation**  \n",
    "**5. (Fig 3.5)**  \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q24\n",
    "\n",
    "1. What is a loss function?\n",
    "2. What is the optimiser?\n",
    "3. What is gradient descent optimisation?\n",
    "4. What is stochastic gradient descent?\n",
    "5. What kind of loss functions are suitable for gradient optimisation?\n",
    "6. What is backpropagation?\n",
    "\n",
    "\n",
    "**1. The loss function quantifies the departure of the network prediction from the desired target.**  \n",
    "**2. The optimiser adjusts weights (and biases) in a direction that will lower the loss for the current sample or mini-batch.**  \n",
    "**3. Gradient optimisation: weights and biases are moved by a small step in the direction of the negative gradient.**   \n",
    "**4. In SGD, the loss for a mini-batch, chosen at random, is calculated.**  \n",
    "**5. Any smooth function such that the loss is 0 when prediction matches target, and otherwise positive.**  \n",
    "**6. An efficient implementation of the SGD using the chain rule of differentiation.**  \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "\n",
    "```\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "output_shape = (none, x, x, 32)\n",
    "x = ???\n",
    "```\n",
    "\n",
    "**```x``` = 26**\n",
    "\n",
    "The stride is 1. The $3 \\times 3$ filter is moved by one pixel, starting at the first pixel and ending at the 26th (so the filter occupies pixels 26, 27, 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320\n",
      "Trainable params: 320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "\n",
    "Consider this layer:\n",
    "\n",
    "```\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "```\n",
    "\n",
    "1. What is the number of trainable of parameters?  \n",
    "2. And how many filters?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\bf (3 * 3 + 1) * 32 = 320$  \n",
    "2. $\\bf 32$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320\n",
      "Trainable params: 320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "\n",
    "```\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2))\n",
    "output_shape = (none, x, x, 32)\n",
    "x = ???\n",
    "```\n",
    "\n",
    "**x = 13**\n",
    "\n",
    "The output shape from the first layer is ```(26, 26)```. The max pooling layer covers the $26 \\times 26$ feature map with 2 x 2 patches and each patch produces a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320\n",
      "Trainable params: 320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "```\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=3, input_length=50))\n",
    "```\n",
    "\n",
    "The output shape of the above ```Embedding``` layer is ```(None, x, y)```.\n",
    "\n",
    "1. What is x?\n",
    "2. What is y?\n",
    "3. How many trainable parameters are there in the ```Embedding``` layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **x = 50**\n",
    "2. **y = 3**\n",
    "3. $\\bf input\\_dim * output\\_dim = 10000 * 3 = 30000$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 3)             30000     \n",
      "=================================================================\n",
      "Total params: 30,000\n",
      "Trainable params: 30,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "model = models.Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=3, input_length=50))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 \n",
    "```\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=32)))\n",
    "model.add(SimpleRNN(32))\n",
    "```\n",
    "\n",
    "What is the number if trainable RNN_parameters? \n",
    "\n",
    "**```32 * 32 + 32 * 32 + 32 = 2080``` or ```(32 + 32 + 1) * 32```**\n",
    "\n",
    "The Simple RNN layer has two weight tensors, W and U. The layer input vector has 32 elements and this is a 32 unit layer. Therefore, w is $32 \\times 32$ and b has 32 elements. The 32 element output feeds back to the layer input so U has $32 \\times 32$ elements.\n",
    "\n",
    "Or: Suppose the layer has M units and the input has N elements. Each unit has $N$ weights for the $N$ input features, $M$ weights for the previous output, and a bias: that's $(N + M + 1)$ coefficients. But there are $M$ units, giving $(N + M + 1) \\times M$ coefficients (or, $N \\times M + M \\times M + M$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 32)          320000    \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 32)                2080      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 322,080\n",
      "Trainable params: 322,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "```\n",
    "tanh([-1, 0, 1]) = [a, b, c]\n",
    "```\n",
    "\n",
    "What can you say about ```a, b``` and ```c```?\n",
    "\n",
    "**```tanh([-1, 0, 1])``` is of the form ```[-x, 0, x]``` where $\\bf 0 < x < 1$. (In fact $\\bf x = 0.76$)**\n",
    "\n",
    "From the $\\tanh$ graph, $\\tanh(0) = 0$ and $\\tanh(x) = -\\tanh(-x)$. Also, $\\tanh$ is contained between in the interval $[-1, 1]$.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "```\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=16))\n",
    "model.add(LSTM(8))\n",
    "```\n",
    "\n",
    "```number_trainable_LSTM_parameters = ?```\n",
    "\n",
    "**800**\n",
    "\n",
    "\n",
    "An LSTM layer has four simple RNN's. \n",
    "\n",
    "`output = gated_tanh(memory)` and `memory = gated_prev_memory + gated_SimpleRNN`. There are three gates and a simple RNN â€“ four RNNs in total.\n",
    "\n",
    "Here N = 16 and M = 8. So Simple RNN parameters sum to `(16 + 8 + 1) * 8 = 200`. Then 4 * 200 = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, None, 16)          160000    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 8)                 800       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,800\n",
      "Trainable params: 160,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 16))\n",
    "model.add(LSTM(8))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
