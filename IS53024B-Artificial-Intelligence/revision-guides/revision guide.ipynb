{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revision guide\n",
    "\n",
    "---\n",
    "\n",
    "The exam is in form of four multiple choice quizzes:\n",
    "\n",
    "| |# Questions |Topics |Marks | Duration\n",
    "|:---|:---|:---|:---|:---|\n",
    "1-2-3-4-part-A | 10  | DLWP Chapters 1-6 | 40 | 2 hours\n",
    "1-2-3-4-part-B | 5   |DLWP Chapters 1-6 | 20 | 1 hour\n",
    "5 DL for computer vision | 5 |  DLWP Chapters 8-9 | 20 | 1 hour\n",
    "6 DL for text and sequences | 5 |  DLWP Chapter 10-11 | 20 | 1 hour\n",
    "\n",
    "The questions will test concepts and skills. The following lists are not necessarily exhaustive but they should give a good indication of the examinable parts of the syllabus. \n",
    "\n",
    "This is an open book exam but it doesn't mean that you shouldn't revise anything! There won't be time for an extensive book search in pursuit of an answer. \n",
    "\n",
    "Warning: use only the resources from the module (my lecture notebooks and DLWP).\n",
    "\n",
    "Each student will receive a randomised quiz composed of questions drawn from a large set.\n",
    "\n",
    "The quizzes will be open for a period of 36 hours.\n",
    "\n",
    "There are four marks per question and each question carries four responses. You should choose the best response. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Concepts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics 1-4\n",
    "\n",
    "1:     `1.Artificial-Intelligence.introduction.ipynb`  \n",
    "2.1:  `2.1.building-blocks.ipynb`  \n",
    "2.2:  `2.2.data-representations.ipynb`  \n",
    "2.3:  `2.3.optimisation.ipynb`  \n",
    "1+3: `01-03.recap.ipynb`  \n",
    "4.1:  `04.1.classifying-movie-reviews-imdb.ipynb`  \n",
    "4.2:  `04.2.classifying-newswires-reuters.ipynb`  \n",
    "4.3:  `04.3.predicting-prices-boston.ipynb`  \n",
    "5+6: `05-06.fundamentals.universal-workflow.ipynb`\n",
    "\n",
    "| Concept | Lecture notebook |\n",
    "|:---|:---|\n",
    "learning | 1\n",
    "statistical model | 1\n",
    "layer | 1, 3\n",
    "depth | 1\n",
    "deep | 1\n",
    "neural network | 1\n",
    "vectorisation | 1\n",
    "weights, biases | 1, 2.1, 2.3\n",
    "samples | 1, 2.3\n",
    "loss function | 1, 2.3\n",
    "optimiser | 2.3\n",
    "backpropagation | 2.3\n",
    "one-hot | 4.1, 4.2\n",
    "softmax | 2.1, 4.2, 1+3\n",
    "probability distribution |  2.1, 4.2, 1+3\n",
    "categorical crossentropy | 4.2, 1+3\n",
    "accuracy | 2\n",
    "epoch | 2.1\n",
    "dense layer | 2.1\n",
    "fully connected | 2.1\n",
    "tensor | 2.2\n",
    "dimensionality | 2.2\n",
    "length (of a vector, 1D tensor) | 2.2\n",
    "axis | 2.2\n",
    "rank | 2.2\n",
    "shape | 2.2\n",
    "slicing | 2.2\n",
    "mini-batch | 2.2, 2.3\n",
    "activation | 2, 1+3\n",
    "element-wise | 2.2\n",
    "broadcasting | 2.2\n",
    "tensor dot product | 2.2\n",
    "reshaping | 2.2\n",
    "transposition | 2.2\n",
    "linearity  | 2.3\n",
    "forward pass | 2.3\n",
    "gradient descent | 2.3\n",
    "derivative | 2.3\n",
    "stochastic gradient descent | 2.3\n",
    "learning rate | 2.3\n",
    "momentum | 2.3\n",
    "backpropagation | 2.3\n",
    "backwards pass | 2.3\n",
    "affine transformation | 2.2, 2.3, 1+3\n",
    "targets | 2.3\n",
    "sigmoid, relu, tanh activations | 1+3\n",
    "layer transformation | 1, 2.2, 2.3\n",
    "hypothesis space | 4.1, 5+6\n",
    "unit | 4.1\n",
    "binary cross-entropy | 4.1\n",
    "training, validation and test sets | 4.1, 4.3, 5+6\n",
    "underfitting/overfitting | 4.1, 4.2, 4.3 5+6\n",
    "information bottleneck | 4.2\n",
    "regression | 4.3\n",
    "normalisation | 4.3\n",
    "hyperparameter | 4.1, 4.2, 4.3, 5+6\n",
    "generalisation | 5+6\n",
    "information leak | 4.1, 5+6\n",
    "simple hold-out validation | 5+6\n",
    "K-fold validation | 4.3\n",
    "iterated K-fold validation | 5+6\n",
    "feature engineering | 5+6\n",
    "regularisation | 5+6\n",
    "capacity | 5+6\n",
    "regular (parameters) | 5+6\n",
    "weight regularisation | 5+6\n",
    "dropout | 5+6\n",
    "last-layer activation and loss for problem type | 5+6\n",
    "statistical power | 5+6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 DL for computer vision and 6 DL for texts and sequences\n",
    "\n",
    "- 8:      `8.introduction-to-convnets.ipynb`  \n",
    "- 9:      `9.4.interpreting-what-convnets-learn.ipynb`\n",
    "- 10:    `chapter10_dl-for-timeseries.ipynb`  \n",
    "- 11.1: `chapter11_part01_introduction.ipynb`  \n",
    "- 11.2: `chapter11_part02_sequence-models.ipynb`  \n",
    "- 11.3: `chapter11_part03_transformer.ipynb`\n",
    "\n",
    "| Concept | Lecture notebook |\n",
    "|:---|:---|\n",
    "convolutional layer | 8\n",
    "filters | 8\n",
    "local receptive field | 8\n",
    "pooling | 8\n",
    "data augmentation | 8\n",
    "(pre-trained) convolutional base | 8\n",
    "freezing | 8\n",
    "fine-tuning | 8\n",
    "response of lower and higher layer filters | 9\n",
    "heatmap | 9\n",
    "recurrent layer transformation function | 10 \n",
    "SimpleRNN layer | 10 \n",
    "RNN pseudocode | 10 \n",
    "LSTM | 10 \n",
    "gates | 10 \n",
    "GRU | 10 \n",
    "bidirectional RNN layer | 10 \n",
    "1D convnets | 10 \n",
    "embedding layer | 11.1 \n",
    "word embeddings | 11.2 \n",
    "transformer | 11.3 \n",
    "attention (query/keys/values) | 11.3 \n",
    "self-attention | 11.3 \n",
    "cross-attention | 11.3 \n",
    "(causal) masking | 11.3 \n",
    "positional encoding | 11.3 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Skills "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics 1-4\n",
    "\n",
    "1. Identify rank and shape from Numpy code\n",
    "2. Code to extract parts of a tensor \n",
    "3. Identify number of samples in an input tensor of given shape\n",
    "4. Python slicing\n",
    "5. Extracting the n'th mini-batch\n",
    "6. Activation of a vector from the graphical form of ```relu, sigmoid``` and ```tanh```\n",
    "7. Calculation of $\\text{relu}(w\\cdot x + b)$ from $w, x, b$.\n",
    "8. One-hot encode a very short text using a short dictionary\n",
    "9. Recognise weight and bias shapes in a specified layer\n",
    "10. Infer the intended problem from the model\n",
    "11. Infer the number of elements of the input tensor from model code\n",
    "12. Calculate number of trainable parameters in a weight/bias tensor for a given layer and total number of trainable layer parameters\n",
    "13. Guessing a possible ```softmax``` output\n",
    "14. Use bottleneck avoidance to guess numebr of units in a layer\n",
    "15. Missing arguments in compilation statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 5: DL for computer vision\n",
    "1. Output shape of a convolutonal layer\n",
    "2. Number of trainable parameters/filters in a convolutional layer\n",
    "3. Output tensor shape of pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic 6: DL for text and sequences\n",
    "1. Embedding layer output shape and number of trainable parameters\n",
    "2. The number of trainable parameters in a simple RNN layer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
