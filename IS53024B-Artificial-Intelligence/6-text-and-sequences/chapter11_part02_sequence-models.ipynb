{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 11.3.3 Processing words as a sequence: The sequence model approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys, pathlib, shutil, random\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# reminder: Colab code to mount your drive\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive') # 'My Drive' is the default name of Google Drives, change the rest accordingly!\n",
    "    os.chdir('drive/My Drive/IS53024B-Artificial-Intelligence/6-text-and-sequences') \n",
    "    os.listdir() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Word embeddings\n",
    "\n",
    "A big revolution of this last decade: researchers discovered that we can get systems to **learn to project words** into a **vector space** that retain **semantic relationship**.\n",
    "\n",
    "The idea behind this discovery is that **similar words** (similar meanings) occur in **similar contexts**.\n",
    "\n",
    "*You shall know a word by the company it keeps.*  \n",
    "(J. R. Firth, \"A Synopsis of Linguistic Theory\", 1957, cf. also the late Wittgenstein)\n",
    "\n",
    "The algorithms will train on large text corpora, and:\n",
    "- count all the occurrences of words and contexts;  \n",
    "- try and predict either the word given the context;\n",
    "- or the context given the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Word embeddings in the context of a large dictionary have typically 256, 512 or 1024 dimensions  \n",
    "(the dimension of a vector is its length i.e. the number of components).\n",
    "\n",
    "One-hot encoded vectors can exceed 20,000 dimensions!  \n",
    "Also, technically all one-hot encoded are **orthogonal**: no similarity between them.\n",
    "\n",
    "Word embeddings compress the information into fewer dimensions.  \n",
    "Two word vectors can be compared to each other!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- <img style=\"height: 700px\" src=\"images/nlp/chollet.one-hot-embeddings.png\"> -->\n",
    "<img src=\"https://drive.google.com/uc?id=1wFVlsvlZMqoTcM_dDRnX3QKvL9nAmOJy\">\n",
    "\n",
    "<small>DLWP, p.330</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|One-hot|Word embeddings|\n",
    "|:---|:---|\n",
    "|binary (integers: 0/1)|floating point vectors|\n",
    "|sparse (most elements are zeros)|*dense*|\n",
    "|very high-dimensional|low-to-medium-dimensional|\n",
    "|hard-coded|learnt from data|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The results is that each token will be represented as a **coordinate** (aka a **vector**) in a high-dimensional space.\n",
    "\n",
    "The most striking features of these spaces is that they seem to encode **semantic relationships**!\n",
    "\n",
    "<!-- <img src=\"images/nlp/linear-relationships.svg\"> -->\n",
    "<img src=\"https://drive.google.com/uc?id=1qsmSHdJxMYmnapU0br6ESLc69deKedcj\">\n",
    "\n",
    "\n",
    "<small>[Embeddings: Translating to a Lower-Dimensional Space, Google Foundational Courses, Machine Learning, Embeddings](https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Note \n",
    "\n",
    "A universal embedding is unlikely, or very difficult to achieve.\n",
    "\n",
    "Semantic relationships depend on task – the text corpus and what we are learning.\n",
    "\n",
    "Expect different geometries for different tasks (e.g. sentiment analysis is very different from classification of legal documents).\n",
    "\n",
    "#### Also\n",
    "\n",
    "The biases of your dataset **will be encoded** in the space (for instance, gendered associations between professions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### References\n",
    "\n",
    "#### Word2Vec\n",
    "\n",
    "Google, 2013, aka the Skip-Gram model, or Continuous Bag of Words (CBOW)\n",
    "\n",
    "<small>[Mikolov et al., \"Distributed Representations of Words and Phrases and their Compositionality\", arxiv](https://arxiv.org/abs/1310.4546)</small>  \n",
    "\n",
    "Perhaps the most famous word embedding scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### GloVe: *Global Vectors for Word Representation*\n",
    "\n",
    "Stanford University, 2014\n",
    "\n",
    "<small>[Pennington et al., \"GloVe: Global Vectors for Word Representation\", arxiv](https://nlp.stanford.edu/pubs/glove.pdf)</small>  \n",
    "\n",
    "\n",
    "Based on factorizing a matrix of word co-occurrence statistics.\n",
    "\n",
    "Millions of English tokens harvested from Wikipedia and other sources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Tutorial\n",
    "\n",
    "[Word embeddings](https://www.tensorflow.org/text/guide/word_embeddings) (3D embeddings with TensorBoard!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"853\"\n",
       "            height=\"480\"\n",
       "            src=\"https://www.youtube.com/embed/rmVRLeJRkl4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7f4d603b2880>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('rmVRLeJRkl4', width=853, height=480) #  Stanford CS224N: NLP with Deep Learning | Winter 2021 |\n",
    "                                                   #  Lecture 1 - Intro & Word Vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Learning word embeddings with the Embedding layer\n",
    "\n",
    "Later, researchers discovered that you can simply **learn** these vectors with your DL model using backprop like everything else!\n",
    "\n",
    "In current models, you just invoke a specific layer, and all the work is done for you.\n",
    "\n",
    "Note that you must specify in advance the **dimensionality** of the embedding space.\n",
    "\n",
    "As usual, more dimensions == more **resolution** (finer-grained), but more computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 8000)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 8000, 256)         2048000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2048000 (7.81 MB)\n",
      "Trainable params: 2048000 (7.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 8000\n",
    "inputs = tf.keras.Input(shape=(max_tokens,))\n",
    "embedding_layer = tf.keras.layers.Embedding( # ← EMBEDDING LAYER\n",
    "    input_dim=max_tokens,                    # the size of the vocabulary (8000)\n",
    "    output_dim=256                           # the dimensionality of the embedding space\n",
    ")\n",
    "x = embedding_layer(inputs)\n",
    "model = tf.keras.Model(inputs, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The two arguments of the embedding layer code are:\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Embedding(input_dim=max_tokens, output_dim=256)\n",
    "```\n",
    "\n",
    "- input_dim = 8000 (the size of our vocab)\n",
    "- output_dim = 256 (the dimension of the embedding space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Chollet's example (the IMDB sentiment task again), reviews have been reduced (or expanded) to a constant length of 600 words.\n",
    "\n",
    "E.g. `x_test[0] = [65, 16, 38, 1334, 88, 12, ..., 16, 5345, 19, 178, 32]`\n",
    "\n",
    "65, 16, 38... are dictionary entries – word 65, word 16, word 38... in the 8000-words dictionary.\n",
    "\n",
    "Then, each elements of `x_test[0]` is one-hot encoded => 600 one-hot vectors of length 8000.\n",
    "\n",
    "The embedding layer multiplies each one-hot vector by a matrix of dimensions $256 \\times 8'000$ => 600 floating point vectors of length 256."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How many learnable parameters does the embedding weight matrix have?\n",
    "\n",
    "$$\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "\\mathrm{input\\_dim} \\times \\mathrm{output\\_dim}\n",
    "}\n",
    "$$\n",
    "\n",
    "Example: \n",
    "\n",
    "$ 8 \\times 10'000 = 80'000$ elements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The embedding layer takes as input tensors of shape `(batch_size, sequence_length)`.  \n",
    "The outputs tensors of shape `(bach_size, sequence_length, output_dim)`. \n",
    "\n",
    "As usual in Keras, the batch_size is represented as `None`: \n",
    "\n",
    "$$\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "In: (None, sequence\\_length) \\to Out: (None, sequence\\_length, output\\_dim)\n",
    "}\n",
    "$$\n",
    "\n",
    "`output_dim` could be called `embed_dim`, the number of dimensions of our embedding space!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A first practical example\n",
    "\n",
    "Sentimenta analysis on the IMDB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Downloading the data\n",
    "\n",
    "Commands to download and uncompress the imdb dataset:\n",
    "\n",
    "```bash\n",
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz\n",
    "!rm -r aclImdb/train/unsup\n",
    "```\n",
    "\n",
    "#### Reminders\n",
    "\n",
    "- in Jupyter, use a bang (!) to run a bash command. You can also run them in a terminal (without the initial !);\n",
    "- in Colab, don't forget to mount your Google drive!\n",
    "\n",
    "#### Other reminders\n",
    "\n",
    "- You can use symlinks in Linux to avoid downloading things twice!\n",
    "\n",
    "    ```bash\n",
    "    # will create a folder called 'dataset' in the current dir, linked to the other one\n",
    "    !ln -s /path/to/already/downloaded/dataset \n",
    "    ```\n",
    "     \n",
    "- In Google drive, the same can be achieved by selecting the directory you want to link to, and pressing Shift+Z, then selecting the target directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# code to split the data into train/val folders\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "train_dir = base_dir / \"train\"\n",
    "val_dir = base_dir / \"val\"\n",
    "test_dir = base_dir / \"test\"\n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    if not os.path.isdir(val_dir / category):    # do this only once\n",
    "        os.makedirs(val_dir / category)          # make 'neg'/'pos' dir in validation\n",
    "        files = os.listdir(train_dir / category) # list files in 'train'\n",
    "        random.Random(1337).shuffle(files)       # shuffle using a seed\n",
    "        num_val_samples = int(0.2 * len(files))  # 2% of our samples for validation\n",
    "        val_files = files[-num_val_samples:]\n",
    "        for fname in val_files:                  # move our files\n",
    "            shutil.move(train_dir / category / fname,\n",
    "                        val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The [`tf.keras.utils.text_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/text_dataset_from_directory) layer, for a directory structure like so:\n",
    "```\n",
    "main_directory/\n",
    "...class_a/\n",
    "......a_text_1.txt\n",
    "......a_text_2.txt\n",
    "...class_b/\n",
    "......b_text_1.txt\n",
    "......b_text_2.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# each of these iterables returns tuples containing two tensors:\n",
    "# samples, shape: (batch_size, sample_shape) ← our texts\n",
    "# targets, shape: (batch_size,)              ← 0 or 1\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    train_dir, batch_size=batch_size\n",
    ")\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    val_dir, batch_size=batch_size\n",
    ")\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    test_dir, batch_size=batch_size\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Preparing integer sequence train/val/test datasets\n",
    "max_length = 600   # we cut our sequences to 600 words max! (For memory.) This will affect performance...\n",
    "max_tokens = 8000\n",
    "\n",
    "text_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# A sequence model built on one-hot encoded vector sequences\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "# ↓ our one-hot vectors --------------------------------------------\n",
    "embedded = tf.one_hot(\n",
    "    inputs,\n",
    "    depth=max_tokens  # we have a vocabulary size of `max_tokens`\n",
    ")\n",
    "# ---------------------------------------------------- passed here ↓ \n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(embedded)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " tf.one_hot (TFOpLambda)     (None, None, 8000)        0         \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 64)                2056448   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2056513 (7.84 MB)\n",
      "Trainable params: 2056513 (7.84 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 115s 166ms/step - loss: 0.5714 - accuracy: 0.6927 - val_loss: 0.4670 - val_accuracy: 0.7884\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcw/anaconda3/envs/gold/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 105s 168ms/step - loss: 0.3716 - accuracy: 0.8576 - val_loss: 0.3055 - val_accuracy: 0.8766\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 106s 169ms/step - loss: 0.3104 - accuracy: 0.8875 - val_loss: 0.3912 - val_accuracy: 0.8174\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 106s 170ms/step - loss: 0.2791 - accuracy: 0.9006 - val_loss: 0.4427 - val_accuracy: 0.8140\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 108s 173ms/step - loss: 0.2548 - accuracy: 0.9101 - val_loss: 0.3583 - val_accuracy: 0.8840\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 109s 174ms/step - loss: 0.2383 - accuracy: 0.9183 - val_loss: 0.2793 - val_accuracy: 0.8914\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 110s 175ms/step - loss: 0.2216 - accuracy: 0.9247 - val_loss: 0.2895 - val_accuracy: 0.8868\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 111s 177ms/step - loss: 0.2010 - accuracy: 0.9314 - val_loss: 0.2918 - val_accuracy: 0.8844\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 110s 175ms/step - loss: 0.1891 - accuracy: 0.9373 - val_loss: 0.3393 - val_accuracy: 0.8862\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 110s 175ms/step - loss: 0.1717 - accuracy: 0.9436 - val_loss: 0.3814 - val_accuracy: 0.8772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f4c85b76df0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a first basic sequence model\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        str(base_dir / \"one_hot_bidir_lstm.h5\"),  # NOTE: in DLWP, he uses the current `.keras` format, but ModelCheckpoint\n",
    "        save_best_only=True                       # in TF 2.13 now breaks when using it with `save_best_only`...\n",
    "    )                                             # see: https://github.com/keras-team/tf-keras/issues/151\n",
    "]\n",
    "model.fit(\n",
    "    int_train_ds,\n",
    "    validation_data=int_val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.880\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(base_dir / \"one_hot_bidir_lstm.h5\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds, verbose=0)[1]:.3f}\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Learning word embeddings with the Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# A model that uses an `Embedding` layer trained from scratch\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "# ↓ our embedding layer --------------------------------------------\n",
    "embedded = tf.keras.layers.Embedding(\n",
    "    input_dim=max_tokens, # our data comes in with a vocab size of `max_tokens`\n",
    "    output_dim=256        # and comes out as dense vectors of dim (\"vocab\") of 256\n",
    ")(inputs)\n",
    "# ---------------------------------------------------- passed here ↓ \n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(embedded)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that given our vocabulary of $8'000$ tokens the embedding layer is **large**: $256 \\times 8'000 = 20'480'000$ elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 256)         2048000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 64)                73984     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2122049 (8.09 MB)\n",
      "Trainable params: 2122049 (8.09 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 209s 322ms/step - loss: 0.5316 - accuracy: 0.7318 - val_loss: 0.3738 - val_accuracy: 0.8436\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 106s 169ms/step - loss: 0.3750 - accuracy: 0.8528 - val_loss: 0.4598 - val_accuracy: 0.8414\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 79s 126ms/step - loss: 0.3094 - accuracy: 0.8859 - val_loss: 0.3245 - val_accuracy: 0.8750\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 72s 116ms/step - loss: 0.2748 - accuracy: 0.9007 - val_loss: 0.3408 - val_accuracy: 0.8594\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 66s 106ms/step - loss: 0.2424 - accuracy: 0.9144 - val_loss: 0.3234 - val_accuracy: 0.8668\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 70s 112ms/step - loss: 0.2228 - accuracy: 0.9222 - val_loss: 0.3746 - val_accuracy: 0.8740\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 67s 107ms/step - loss: 0.2047 - accuracy: 0.9297 - val_loss: 0.3650 - val_accuracy: 0.8774\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 68s 109ms/step - loss: 0.1873 - accuracy: 0.9369 - val_loss: 0.3560 - val_accuracy: 0.8810\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 64s 103ms/step - loss: 0.1719 - accuracy: 0.9416 - val_loss: 0.5043 - val_accuracy: 0.8696\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 62s 100ms/step - loss: 0.1580 - accuracy: 0.9478 - val_loss: 0.3852 - val_accuracy: 0.8798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f4bf4232100>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        str(base_dir / \"embeddings_bidir_lstm.h5\"),\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "model.fit(\n",
    "    int_train_ds,\n",
    "    validation_data=int_val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.856\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(base_dir / \"embeddings_bidir_lstm.h5\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds, verbose=0)[1]:.3f}\") # in this case, roughly the same as one-hot\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Using pretrained word embeddings\n",
    "\n",
    "Pretrained word embeddings are useful **when training data is limited** – just as with pretrained convnets.\n",
    "\n",
    "Very structured embeddings hopefully capture **generic structure** appropriate to diverse domains.\n",
    "\n",
    "(The more data you can train on, the more likely your task-specific embeddings will perform better.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see how we can use GloVe embeddings in `tensorflow.keras`.\n",
    "\n",
    "(The same method applies to Word2Vec or any other embedding technique.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Commands to download and uncompress the pretrained GloVe word vectors.\n",
    "\n",
    "```bash\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip -d glove # unzip to a directory called \"glove\"\n",
    "!rm glove.6B.zip                # remove the zip file\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = pathlib.Path(\"glove\") # I have my file in a folder called 'glove'\n",
    "path_to_glove_file =  base_dir / \"glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n"
     ]
    }
   ],
   "source": [
    "# another Jupyter magic: use $python_variable in bash commands\n",
    "!head -n 1 $path_to_glove_file\n",
    "# ↓ the word \"the\" followed by its coordinates in a 100-dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400,000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# parsing the GloVe word-embeddings file\n",
    "embeddings_index = {}                                   # our dictionary: {'word': np.array([...coordinates..])}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)            # split: word | coordinates\n",
    "        coefs = np.fromstring(coefs, \"float\", sep=\" \")  # load string floats into numpy, space-separated\n",
    "        embeddings_index[word] = coefs                  # save into dictionary\n",
    "\n",
    "print(f\"Found {len(embeddings_index):,} word vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "# we reuse the same TextVectorization object as earlier, turning our sentences into integers\n",
    "# max_length: 600, max_tokens: 8000\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "# preparing the GloVe word-embeddings matrix\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim))     # create a matrix (max_tokens, embedding_dim)\n",
    "for word, i in word_index.items():                           # looping through our vocab\n",
    "    if i < max_tokens:                                       # don't try and retrieve beyond max_tokens\n",
    "        embedding_vector = embeddings_index.get(word)        # try and get the vector associated with the word\n",
    "    if embedding_vector is not None:                         # if the vector exists\n",
    "        embedding_matrix[i] = embedding_vector               # assign it to our matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    max_tokens,\n",
    "    embedding_dim,        # using our embedding matrix through an initializer\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,      # WE DO NOT TRAIN IT! \n",
    "    mask_zero=True,\n",
    ") \n",
    "\n",
    "# Given that our network is initialized randomly, the massive changes it undergoes at the beginning\n",
    "# of training would certainly affect/damage the representations in our embedding matrix \n",
    "# (same scenario as with pretrained ConvNets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# A model that uses a pretrained Embedding layer\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "# ↓ our embedding layer --------------------------------------------\n",
    "embedded = embedding_layer(inputs)\n",
    "# ---------------------------------------------------- passed here ↓ \n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(embedded)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, None, 100)         800000    \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 64)                34048     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 834113 (3.18 MB)\n",
      "Trainable params: 34113 (133.25 KB)\n",
      "Non-trainable params: 800000 (3.05 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 86s 117ms/step - loss: 0.2822 - accuracy: 0.8831 - val_loss: 0.2914 - val_accuracy: 0.8744\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 64s 102ms/step - loss: 0.2677 - accuracy: 0.8893 - val_loss: 0.2969 - val_accuracy: 0.8760\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 69s 110ms/step - loss: 0.2589 - accuracy: 0.8949 - val_loss: 0.2858 - val_accuracy: 0.8820\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 63s 101ms/step - loss: 0.2453 - accuracy: 0.8999 - val_loss: 0.3004 - val_accuracy: 0.8720\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 63s 101ms/step - loss: 0.2338 - accuracy: 0.9061 - val_loss: 0.3166 - val_accuracy: 0.8700\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 64s 103ms/step - loss: 0.2212 - accuracy: 0.9118 - val_loss: 0.3040 - val_accuracy: 0.8742\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 63s 101ms/step - loss: 0.2110 - accuracy: 0.9179 - val_loss: 0.3061 - val_accuracy: 0.8776\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 64s 102ms/step - loss: 0.2001 - accuracy: 0.9215 - val_loss: 0.3033 - val_accuracy: 0.8766\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 64s 102ms/step - loss: 0.1896 - accuracy: 0.9234 - val_loss: 0.3275 - val_accuracy: 0.8710\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 64s 103ms/step - loss: 0.1797 - accuracy: 0.9306 - val_loss: 0.3673 - val_accuracy: 0.8686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f49e80b61c0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        str(base_dir / \"glove_embeddings_sequence_model.h5\"),\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "model.fit(\n",
    "    int_train_ds,\n",
    "    validation_data=int_val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.867\n"
     ]
    }
   ],
   "source": [
    "# annoyance: in TF 2.13, this model fails to load in .h5 format, whilst it also fails to train when using\n",
    "# the ModelCheckpoint with `save_best_only`...\n",
    "# model = tf.keras.models.load_model(base_dir / \"glove_embeddings_sequence_model.h5\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds, verbose=0)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "### NLP libraries & tools in Python\n",
    "\n",
    "#### NLTK: the Natural Language Toolkit\n",
    "\n",
    "NLTK is a leading platform for building Python programs to work with human language data ([website](https://www.nltk.org/)).\n",
    "\n",
    "#### Gensim\n",
    "\n",
    "Perhaps the best dedicated library for word vectors & similar text processing tools ([website](https://radimrehurek.com/gensim/index.html)).\n",
    "\n",
    "#### spaCy\n",
    "\n",
    "A a free open-source library for Natural Language Processing in Python ([website](https://spacy.io/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### Word embbeddings\n",
    "\n",
    "- **Various kinds of word encodings**:\n",
    "  - **one-hot/multi-hot**: the presence of words is marked by a 1 (binary) → *sparse* & *hard-coded*\n",
    "  - **word embeddings**: project words/tokens into vector spaces where collocations between words (\"the company a word keeps\") are modeled as the distance between vectors. → *dense* & *learnt from data*\n",
    "- Two most important embedding models\n",
    "    - **Word2Vec** from Google\n",
    "    - **GloVe** from Stanford\n",
    "- **Embedding layers** can be trained end to end with your net!\n",
    "- **Pretrained embeddings** can also be used on top of your own models, like pretrained networks!\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "chapter11_part02_sequence-models.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
