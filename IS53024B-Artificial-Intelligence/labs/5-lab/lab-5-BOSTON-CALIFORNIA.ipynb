{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6gb6X1pzNp1"
   },
   "source": [
    "# The Two Housing Datasets\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Practice\n",
    "- Run k-fold validation on the Boston dataset;\n",
    "- Notice that the mini-batch size is set to 1. Experiment with different mini-batch sizes. What do you observe? Can you account for your observation?\n",
    "- Run a series of experiments to find the best model. (Hint: look back at the previous labs.)\n",
    "- Retrain the best model on all the training data and evaluate on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[California Housing](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html), original website.\n",
    "\n",
    "[California Housing, Kaggle](https://www.kaggle.com/datasets/camnugent/california-housing-prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"california/cal_housing.data\") as i:\n",
    "    data = [l.strip().split(\",\") for l in i.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude',\n",
       " 'latitude',\n",
       " 'housingMedianAge',\n",
       " 'totalRooms',\n",
       " 'totalBedrooms',\n",
       " 'population',\n",
       " 'households',\n",
       " 'medianIncome',\n",
       " 'medianHouseValue']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"california/cal_housing.domain\") as i:\n",
    "    categories = [l.split(\": \")[0] for l in i.readlines()]\n",
    "\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Harder dataset\n",
    "\n",
    "[Ames Housing](http://jse.amstat.org/v19n3/decock/AmesHousing.txt), original dataset (what [website](http://jse.amstat.org/jse_data_archive.htm)!).\n",
    "\n",
    "\n",
    "[Ames Housing, Kaggle](https://www.kaggle.com/code/tracyporter/ames-house-prices-keras/notebook)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQvMWYSdzNp8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "\n",
    "mean = train_data.mean(axis = 0)\n",
    "train_data -= mean # shift\n",
    "std = train_data.std(axis = 0)\n",
    "train_data /= std # rescale\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            64, activation = 'relu', input_shape = (train_data.shape[1], )\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.Dense(64, activation = 'relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(\n",
    "        optimizer='rmsprop',\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bOCOzAjOzNqC"
   },
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, epochs):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.epochs = epochs\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        c = ['|', '/', '-', '\\\\'] \n",
    "        print(f\"\\r{c[epoch % 4]} epoch: {epoch+1}/{self.epochs}\", end=\"\")\n",
    "    def on_train_end(self, logs=None):\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cubqyRFzNqG"
   },
   "outputs": [],
   "source": [
    "K = 4\n",
    "num_val_samples = len(train_data) // K\n",
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "for i in range(K):\n",
    "    print('processing fold', i)\n",
    "    \n",
    "    # Prepare the validation data: data from partition i\n",
    "    a, b = i * num_val_samples, (i + 1) * num_val_samples\n",
    "    val_data = train_data[a : b]\n",
    "    val_targets = train_targets[a : b]\n",
    "    \n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate([train_data[:a], train_data[b:]], axis=0)\n",
    "    partial_train_targets = np.concatenate([train_targets[:a], train_targets[b:]], axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model = build_model()\n",
    "    \n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    history = model.fit(\n",
    "        partial_train_data,\n",
    "        partial_train_targets,\n",
    "        validation_data=(val_data, val_targets),\n",
    "        epochs=num_epochs, batch_size=1, verbose=0, \n",
    "        callbacks=[CustomCallback(num_epochs)]\n",
    "    )\n",
    "\n",
    "    mae_history = history.history['val_mae']\n",
    "    all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dNFOVfdzNqJ"
   },
   "outputs": [],
   "source": [
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories])for i in range(num_epochs)]\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8I58FoiazNqM"
   },
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor = 0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points: # an empty list is 'False'\n",
    "            previous = smoothed_points[-1] # the last appended point\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IaEOcb8zNqO"
   },
   "outputs": [],
   "source": [
    "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
