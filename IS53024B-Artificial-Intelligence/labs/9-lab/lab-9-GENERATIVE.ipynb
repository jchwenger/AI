{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Deep Learning\n",
    "\n",
    "### Pick **one** of these topics.\n",
    "\n",
    "The lecture notebooks are adapted from the [official repository](https://github.com/fchollet/deep-learning-with-python-notebooks) and should be used **as starter code**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Text generation\n",
    "\n",
    "Try building different language models. There are tens of thousands of texts in [Project Gutenberg](https://www.gutenberg.org). There is small but wide range of texts (and other data) [here](https://introcs.cs.princeton.edu/java/data/). Are you able to train models that generate realistic texts?\n",
    "\n",
    "[Karpathy](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) noted in his blog post that language models also work with code. What happens if you try and train a language model on the [TensorFlow codebase](https://github.com/tensorflow/tensorflow)?\n",
    "\n",
    "Other common sampling strategies on top of `temperature` are `top_k`, sampling only from the most probable `k` logits, and `top_p`, nucleus sampling, sampling only from the most probable logits which taken together amount for `p` probability (see [here](https://huggingface.co/blog/how-to-generate) for a more complete discussion). Can you implement these generation functionalities, for instance importing them from [here](https://github.com/akanyaani/gpt-2-tensorflow2.0/blob/master/sample.py#L11) and [here](https://github.com/akanyaani/gpt-2-tensorflow2.0/blob/master/sample.py#L27)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Deep Dream\n",
    "\n",
    "Produce dream-like images with DeepDream. Experiment with different input images – faces, landscapes etc. Experiment with lower and upper layers – the lower layers should produce more geometric patterns, the upper layers, more abstract (noses, feathers, eyes etc.). The layers that are maximised for activation are specified in the `layer_contributions` dictionary. Get the layer names from `model.summary()`. Finally, experiment with the number of octaves, loss limit and step size, and, if you feel curious, different pretrained models shipping with TF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. VAEs\n",
    "\n",
    "Experiment with VAE image generation. Instead of MNIST, it is possible to train on FashionMNIST, or the 200,000 celebrity portraits in the free [celebA dataset](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) (also on [Kaggle](https://www.kaggle.com/jessicali9530/celeba-dataset))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. GANs\n",
    "\n",
    "Experiment with GANs image generation (beware: training can be quite long). The main difficulty with GANs is the instability of training (the Discriminator loss going to zero, and the Generator one blowing up, which has to do with the fact that the generating task is much harder than the discriminating one!). Chollet has two recommendations for this: lowering the Discriminator learning rate, increasing its dropout. Experiment with various learning rates and dropout rate, as well as batch sizes, so as to find a way to train your GAN in a stable manner.\n",
    "\n",
    "Another common strategy is to train the Generator more often than the Discriminator. The `train_step` function can be modified to include a loop that trains "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
