{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4dd38f7-759f-47fe-951b-90c77b35b6d5",
   "metadata": {},
   "source": [
    "# The Attention Mechanism\n",
    "\n",
    "<!-- <img style=\"float:right;height:600px\" src=\"images/transformer/beyer.self-attention.png\"> -->\n",
    "<img style=\"float:right;height:600px\"  src=\"https://drive.google.com/uc?id=1QUOC1-gC7fF8Xn9eCgKfeSTF84BhjaK-\">\n",
    "\n",
    "<small style=\"position:absolute;bottom:0;right:0\">[Lucas Beyer, \"Transformers\"](https://docs.google.com/presentation/d/1ZXFIhYczos679r70Yu8vV9uO6B1J0ztzeDxbnBxD1S0/edit#slide=id.g13dd67c5ab8_0_79)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59079ed5-b4ee-4e2e-a113-d02654ce2dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698dd70d-a5d9-43b1-97b7-35ce083917c8",
   "metadata": {},
   "source": [
    "The principle of **self-attention** is to compute the similarity of each time step of queries sequence with all others. In this case, the $Queries$ and the $Keys$ are both stemming from the same sequence, whereas in **cross-attention**, the values are generated using another sequence, rather than the one used for the $Queries$ and $Keys$.\n",
    "\n",
    "The first step is to transform our input sequence $x$ by multiplying it with three different matrices $Q$, $K$, $V$ (three different linear transformations):\n",
    "\n",
    "\\begin{align}\n",
    "Queries &= Qx\\\\\n",
    "Keys &= Kx\\\\\n",
    "Values &= Vx\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Dimensions:  \n",
    "$x$: ($steps$, $embed\\_dim$)  \n",
    "$Q, K, V$: ($steps$, $embed\\_dim$, $embed\\_dim$)  \n",
    "$Queries, Keys, Values$: ($steps$, $embed\\_dim$)  \n",
    "$embed\\_dim$: each token in our sequence is an embedding vector of $embed\\_dim$ dimensions.  \n",
    "\n",
    "The result are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49d75b3-2701-420b-b0aa-00705a918643",
   "metadata": {},
   "source": [
    "In this simplest example, let's imagine our *embedding* is just **one** number. Our sequence is therefore of shape ($steps$, 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7eaba1d3-7f50-484b-bf76-6b8f629104d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = tf.random.uniform((3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9005757f-7a01-4905-b692-15e9de3177a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       "array([[0.2830789 ],\n",
       "       [0.43633425],\n",
       "       [0.04906607]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = seq\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06326da-6e21-4a3f-aeb7-ca5e5f7f6b8e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The similarity scores: $(QK^T)$\n",
    "\n",
    "First, we compute the similarity of each step with all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44fc88a7-1aa2-4f83-a7f2-6100abbbde66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.2830789 , 0.43633425, 0.04906607]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = tf.transpose(queries)\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12b4db56-897d-4ec1-ac16-ea595f04cf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0.08013367, 0.12351702, 0.01388957],\n",
       "       [0.12351702, 0.19038758, 0.02140921],\n",
       "       [0.01388957, 0.02140921, 0.00240748]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKt = queries@keys\n",
    "QKt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d9f717-11e7-44e9-9fe9-ee326de9d96b",
   "metadata": {},
   "source": [
    "If we wanted to extract that information for just one token in our source sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a2bb7db0-7c72-4165-8561-fa76d933697f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.2830789]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1 = queries[None, 0]\n",
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "df4e4173-bad3-454a-bdef-756ffe6c667d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.08013367, 0.12351702, 0.01388957]], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1Kt = q1@keys\n",
    "Q1Kt # This is the same as the first line of `QKt`..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80fbfb-a17b-40ec-a196-a7b0cdeab9a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Note\n",
    "\n",
    "That matrix is symmetric (not in general, but in our case, since we look at the relationship between every step and every step in the same sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "790e6614-f9a6-4071-b9c9-098d7c53b388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=bool, numpy=\n",
       "array([[ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True]])>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKt == tf.transpose(QKt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327b2806-87fc-47c7-89fb-621b4c2ec856",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Turned into a mask using softmax and scaling\n",
    "\n",
    "Applying the scaling by $\\sqrt{(embedding\\_dim)}$ and $softmax$ in the temporal dimension.\n",
    "\n",
    "What we want is to turn our similarity metric (the dot product) into something like a **mask**, where we ultimately want the new representation for our token to be a **weighted average** of all other tokens in our sequence (we mix information from all steps in our input sequence, without increasing the total amount of information, that has nice mathematical properties for our gradient as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "051c3889-3d82-4e42-b776-117566485e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0.33554336, 0.35042074, 0.31403583],\n",
       "       [0.33646366, 0.3597325 , 0.30380386],\n",
       "       [0.33376372, 0.33628297, 0.3299533 ]], dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKt_soft_scaled = tf.nn.softmax(QKt / tf.math.sqrt(tf.cast(queries.shape[-1], dtype=tf.float32)), axis=-1) \n",
    "QKt_soft_scaled                                   # shenanigans: queries.shape[-1] is an int, we need a float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc87ed48-4f38-4cc4-b95f-d857d8cfd4e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Note\n",
    "\n",
    "(Given the symmetric nature of our matrix, the effect with `axis=0` would yield a symmetric result.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a890a9b-b80a-4720-bf0c-f65be09141d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0.33554336, 0.33646366, 0.33376372],\n",
       "       [0.35042074, 0.3597325 , 0.33628297],\n",
       "       [0.31403583, 0.30380386, 0.3299533 ]], dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(QKt / tf.math.sqrt(tf.cast(queries.shape[-1], dtype=tf.float32)), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "66a20faa-55ef-4c82-b077-89d512c102d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=bool, numpy=\n",
       "array([[ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True]])>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(QKt_soft_scaled) == tf.nn.softmax(QKt / tf.math.sqrt(tf.cast(queries.shape[-1], dtype=tf.float32)), axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df91aa5-e8c2-463d-949f-b35bdf03692b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "What happens if we continue with our single $query$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1dff92a2-9fc1-4f01-bb55-ced289809f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.33554336, 0.35042074, 0.31403583]], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1Kt_soft_scaled = tf.nn.softmax(Q1Kt / tf.math.sqrt(tf.cast(queries.shape[-1], dtype=tf.float32))) \n",
    "Q1Kt_soft_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "36cf1656-c6fa-4bd5-8ad7-f1799e768d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=bool, numpy=array([[ True,  True,  True]])>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKt_soft_scaled[0] == Q1Kt_soft_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac28cead-b54c-4547-bc8a-b20bd39495a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Applying the mask to our values $(QK^T)V$\n",
    "\n",
    "Now that we have our **mask**, we multiply the scaled score for each token to the token itself, and sum the results. This is equivalent to allowing some information from each token to pass into our new representation (the softmax makes sure that the total of what we allow sums to one: we are redistributing information without adding or sub\n",
    "\n",
    "$\\overbrace{softmax(\\frac{QK^T}{\\sqrt{embed\\_dim}})}^{mask}V$:\n",
    "\n",
    "### Beware\n",
    "\n",
    "The mask is sometimes called **weights**, which, it should be said **are not weights in the neural network sense**, just in the **weighted average** sence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8d2bf30-cc92-49e9-8077-02af723885ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1f69dad-23f8-493c-9be8-3e7502aa6daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       "array([[0.26329434],\n",
       "       [0.26711583],\n",
       "       [0.25740278]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QKtV = QKt_soft_scaled @ values\n",
    "QKtV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfeab28-e269-4b4d-9bbd-888b9a1ccaa4",
   "metadata": {},
   "source": [
    "This should be read as a **new sequence** with the same number of steps, each token still being represented as one number, but with each token containing contextual information gathered from the rest of the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Cross-attention\n",
    "\n",
    "This works in the exact same way, only with another sequence providing the $queries$.\n",
    "\n",
    "<!-- <img style=\"float:right;height:600px\" src=\"images/transformer/beyer.cross-attention.png\"> -->\n",
    "<img style=\"float:right;height:600px\"  src=\"https://drive.google.com/uc?id=1fq19uEJd52_qDf1qmcV9BZWdCcstx1M7\">\n",
    "\n",
    "<small style=\"position:absolute;bottom:0;right:0\">[Lucas Beyer, \"Transformers\"](https://docs.google.com/presentation/d/1ZXFIhYczos679r70Yu8vV9uO6B1J0ztzeDxbnBxD1S0/edit#slide=id.g13dd67c5ab8_0_79)</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67fa070c-b797-4adc-8f94-7eea55d50554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       "array([[0.0127939 ],\n",
       "       [0.7549559 ],\n",
       "       [0.58881783]], dtype=float32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_queries = tf.random.uniform((3,1))\n",
    "other_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f2aafb11-af9e-4b0a-8cea-019c8f28f56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       "array([[0.2564841 ],\n",
       "       [0.2749517 ],\n",
       "       [0.27088535]], dtype=float32)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cross_QKtV = tf.nn.softmax((other_queries @ keys)/tf.math.sqrt(tf.cast(queries.shape[-1], dtype=tf.float32))) @ values\n",
    "Cross_QKtV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
