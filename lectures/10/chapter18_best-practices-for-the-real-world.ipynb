{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orCWPlazrhZf"
   },
   "source": [
    "# Attendance\n",
    "\n",
    "[The link to SEAts](https://gold.seats.cloud/angular/#/lectures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeImlHZ9rhZx"
   },
   "source": [
    "# 13 Best practices for the real world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeImlHZ9rhZx"
   },
   "source": [
    "## Getting the most out of your models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71Kd9YIurhZx"
   },
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71Kd9YIurhZx"
   },
   "source": [
    "Deep Learning engineers have many seemingly arbitrary decisions:\n",
    "\n",
    "- How many layers?\n",
    "- How many unites per layer?\n",
    "- What activation function(s)?\n",
    "- Batch/Layer normalise?\n",
    "- How much dropout/regularisation?\n",
    "- What optimizer or learning rate?\n",
    "- etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-LMzxkerhZx"
   },
   "source": [
    "### Hyperparameter optimisation workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-LMzxkerhZx"
   },
   "source": [
    "Throughout: **Document everything**.\n",
    "\n",
    "1. Think of a *baseline* (without a neural net & with an untrained net)\n",
    "2. Choose a set of hyperparameters;\n",
    "3. Build the model;\n",
    "4. Fit to training data and measure performance on validation data;\n",
    "5. If good enough:\n",
    "    - Stop & go to 7.;\n",
    "6. Go to 2;\n",
    "7. Retrain with the same hyperparameters as the best run on *your entire training set* (partial train + validation, no longer any validation split!), up until the epoch where your best model started overfitting, evaluate on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1cIe89WrhZy"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1cIe89WrhZy"
   },
   "source": [
    "### Keras Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1cIe89WrhZy"
   },
   "source": [
    "Keras now comes with its own module for automated hyperparameter optimisation.\n",
    "\n",
    "If you go for the Data Science option in Coursework 2, please **do not use it**!!  \n",
    "We would like you to practice building this **yourself**.\n",
    "\n",
    "This could be used in projects with another focus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1cIe89WrhZy"
   },
   "source": [
    "#### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1cIe89WrhZy"
   },
   "source": [
    "[Documentation](https://keras.io/keras_tuner/)  \n",
    "[TensorFlow tutorial](https://www.tensorflow.org/tutorials/keras/keras_tuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHRW76F5rhZy"
   },
   "source": [
    "#### The future of hyperparameter tuning: automated machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHRW76F5rhZy"
   },
   "source": [
    "Automating Hyperparameter search is an active area of research, known as AutoML. Here is the [the Keras package trying to develop that](https://autokeras.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ld3ykRcvrhZy"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ld3ykRcvrhZy"
   },
   "source": [
    "### Model ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ld3ykRcvrhZy"
   },
   "source": [
    "Ensembling means pooling predictions from a number of **independent models**.\n",
    "\n",
    "The idea is based on the observation that any model can only grasp **part of the truth**.\n",
    "\n",
    "We hope that a *more diverse* set of models might access **more aspects of the truth**.\n",
    "\n",
    "Ensembling generally produces the most competitive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OW15pmcxrhZy"
   },
   "source": [
    "Pooling can proceed by **averaging** model predictions.\n",
    "\n",
    "But there is a disadvantage – a poor model can worsen the average, even dragging it below the performace of the best model.\n",
    "\n",
    "The predictions can be weighted, with higher weights to better models.\n",
    "\n",
    "The weights can even be automatically optimised.  \n",
    "(Idea: give the best-performing model on the validation set the highest weight...)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoSON0IrrhZy"
   },
   "source": [
    "#### Equal weight for all predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoSON0IrrhZy"
   },
   "source": [
    "```python\n",
    "preds_a = model_a.predict(x_val) # Use four different\n",
    "preds_b = model_b.predict(x_val) # models to compute\n",
    "preds_c = model_c.predict(x_val) # initial predictions.\n",
    "preds_d = model_d.predict(x_val)\n",
    "             # ↓ This new prediction array should be accurate than any of the initial ones.\n",
    "final_preds = 0.25 * (preds_a + preds_b + preds_c + preds_d)\n",
    "```\n",
    "\n",
    "DLWP, p.420"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRSRJJMXrhZy"
   },
   "source": [
    "#### Different weights for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRSRJJMXrhZy"
   },
   "source": [
    "```python\n",
    "preds_a = model_a.predict(x_val)\n",
    "preds_b = model_b.predict(x_val)\n",
    "preds_c = model_c.predict(x_val)\n",
    "preds_d = model_d.predict(x_val)\n",
    "            # ↓ These weights (0.5, 0.25, 0.1, 0.15) are assumed to be learned empirically.\n",
    "final_preds = 0.5 * preds_a + 0.25 * preds_b + 0.1 * preds_c + 0.15 * preds_d\n",
    "```\n",
    "DLWP, p.420-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuUlclbErhZy"
   },
   "source": [
    "#### Note: Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuUlclbErhZy"
   },
   "source": [
    "The models in the ensemble need to be **as diverse** as possible.\n",
    "\n",
    "A model in the ensemble might be comparatively poor and have low weight, but can nevertheless make a quantitative difference to the overall prediction because it was distant from the other models and may perform really well in specific situations.\n",
    "\n",
    "The concern is for a **diversity of models** rather than how well the best model performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuUlclbErhZy"
   },
   "source": [
    "#### Note: different initialisations vs architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuUlclbErhZy"
   },
   "source": [
    "Avoid training the same model from different initialisations/order of exposure to data: this is not different enough!\n",
    "\n",
    "Better to have different architectures or approaches (neural and not neural, for instance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJiM9ofArhZy"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJiM9ofArhZy"
   },
   "source": [
    "## Scaling-up model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcInTrxsrhZy"
   },
   "source": [
    "### Speeding up training on GPU with mixed precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcInTrxsrhZy"
   },
   "source": [
    "#### Understanding floating-point precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcInTrxsrhZy"
   },
   "source": [
    "There are three of levels of precision you’d typically use:\n",
    "- Half precision, or `float16`, where numbers are stored on 16 bits;\n",
    "- Single precision, or `float32`, where numbers are stored on 32 bits;\n",
    "- Double precision, or `float64`, where numbers are stored on 64 bits.\n",
    "\n",
    "(DLWP, p.422)\n",
    "\n",
    "Tradeoff: for some operations, you want more **precision** (requires more compute), for others you want more **speed** (the precision matters less)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGTycStVrhZy"
   },
   "source": [
    "#### Manual tensor conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIqa4zAArhZy",
    "outputId": "a5e9181c-8c95-481b-beca-53beffa9b33e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = np.zeros((2, 2))\n",
    "tf_tensor = tf.convert_to_tensor(np_array)\n",
    "tf_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LAetHdycrhZy",
    "outputId": "c854c6e0-5308-44ff-c6ea-7440d3a29d09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = np.zeros((2, 2))\n",
    "tf_tensor = tf.convert_to_tensor(np_array, dtype=\"float32\")\n",
    "tf_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emDZZ_0KrhZy"
   },
   "source": [
    "#### Mixed-precision training in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emDZZ_0KrhZy"
   },
   "source": [
    "```python\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQU9gpjlrhZy"
   },
   "source": [
    "### Multi-GPU training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQU9gpjlrhZy"
   },
   "source": [
    "<!-- ![Chollet mirrored strategy](images/chollet/figure18.2.png) -->\n",
    "\n",
    "![Chollet  mirrored strategy](https://raw.githubusercontent.com/jchwenger/AI/main/lectures/10/images/chollet/figure18.2.png)\n",
    "\n",
    "[DLWP](https://deeplearningwithpython.io/chapters/chapter18_best-practices-for-the-real-world/#model-parallelism-splitting-your-model-across-multiple-gpus), Figure 18.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBuOBYadrhZy"
   },
   "source": [
    "```python\n",
    "# Create a “distribution strategy” object. (go-to solution: `MirroredStrategy`)\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "with strategy.scope():           # Open scope: everything inside is distributed\n",
    "    model = get_compiled_model() # All variables must be under the scope!\n",
    "                                 # (Model construction and `compile()`)\n",
    "```\n",
    "```python\n",
    "model.fit(                       # The training will automatically be\n",
    "    train_dataset,               # distributed across devices!\n",
    "    epochs=100,\n",
    "    validation_data=val_dataset\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyObWiECrhZz"
   },
   "source": [
    "#### Global batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyObWiECrhZz"
   },
   "source": [
    "When using `datasets`, the batch size is then a **global** batch size, and gets split up on each device.\n",
    "\n",
    "Usually then you calculate it from the capacity of each GPU:\n",
    "\n",
    "```python\n",
    "batch_size = batch_size_per_device * n_devices\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6rOTmZhrhZz"
   },
   "source": [
    "### TPU training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6rOTmZhrhZz"
   },
   "source": [
    "#### Using a TPU via Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6rOTmZhrhZz"
   },
   "source": [
    "They still sometimes have a few available for free, try it with the starter code in [`lab-8-TPU`](https://github.com/jchwenger/AI/blob/main/labs/8-lab/lab-8-TPU.ipynb)! (Change the Runtime to TPU.)\n",
    "\n",
    "This notebook also contains a link to another Colab notebook, which itself refers to **more** Colab notebooks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwAv0ZyVrhZz"
   },
   "source": [
    "#### TPU checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwAv0ZyVrhZz"
   },
   "source": [
    "- TPUs have a lot of capacity: you can easily have larger batch sizes!\n",
    "- Loading data can be a big bottleneck in these contexts. TensorFlow recommends using their [binary format TFRecords](https://www.tensorflow.org/tutorials/load_data/tfrecord) (and the [Keras page](https://keras.io/examples/keras_recipes/creating_tfrecords/));\n",
    "- TPUs require that everything is compiled before it runs. You can even gain in speed by *compiling several steps of training in one* (called **step fusing**) to improve performance: specify `steps_per_execution=n` in the `compile()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhAuCqegrhZz"
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhAuCqegrhZz"
   },
   "source": [
    "- the `Sequential` syntax;\n",
    "- the `Functional` syntax;\n",
    "- `Model` or `Layer` subclassing\n",
    "- Losses must match outputs!\n",
    "- Various losses are **summed**, weighted average using `loss_weights`;\n",
    "- Retrieve inner layers from models → build new ones;\n",
    "- Mix & match: functional & subclassing can be combined;\n",
    "- No `summary()` method with subclassing!\n",
    "\n",
    "Personal advice: *when you reach subclassing, you may leave Keras behind and just use TF... :}*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlTE-AkWrhZz"
   },
   "source": [
    "### Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlTE-AkWrhZz"
   },
   "source": [
    "- DL engineering requires exploration in hyperparameter space:\n",
    "    - currently guided by intuition/experience;\n",
    "    - automation is coming;\n",
    "    - human or automatic search should be as **systematic** as possible;\n",
    "    - three ideas:\n",
    "        - grid search;\n",
    "        - random search;\n",
    "        - babysitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgWRvp76rhZz"
   },
   "source": [
    "### Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgWRvp76rhZz"
   },
   "source": [
    "- Ensembling with the appropriate weighted average of contributions is very powerful;\n",
    "- Component models should be as dissimilar as possible."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
