{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6f605f-39cf-4a2b-ab6a-6058d6437cb3",
   "metadata": {},
   "source": [
    "# References | 6. Text and Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4572c75-5421-40e5-9d9f-9adf2db36236",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20688acf-c84e-4a8e-a29d-fe5f15df3cc9",
   "metadata": {},
   "source": [
    "## RNNs, LSTMs, GRUs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "818db0a6-3b42-4b03-a8e6-59fefc6b2d9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "e_FDWPvcxM_G",
    "outputId": "9cb9261b-653b-4115-f271-7f703bb6c6cc"
   },
   "source": [
    "[MIT 6.S191 (2021): Recurrent Neural Networks](https://www.youtube.com/watch?v=qjrad0V0uJE)  \n",
    "More details in [`2nd-ed.chapter10_dl-for-timeseries_rnn_lstm.3.ipynb`](https://github.com/jchwenger/AI/blob/main/lectures/06.more/2nd-ed.chapter10_dl-for-timeseries_rnn_lstm.3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e415f9dd-2200-4dbe-818e-5af45c89c2cf",
   "metadata": {
    "id": "dH6eU-l6xM_G"
   },
   "source": [
    "### Blog posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310314b4-e95d-48e9-bdb6-c2eb7c5c26f6",
   "metadata": {
    "id": "dH6eU-l6xM_G"
   },
   "source": [
    "[Olah, \"Understanding LSTM Networks\"](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)  \n",
    "[Madsen, \"Visualizing memorization in RNNs\", Distill](https://distill.pub/2019/memorization-in-rnns/)  \n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552f123f-4d46-473f-a978-b71c58c905a4",
   "metadata": {
    "id": "dH6eU-l6xM_G"
   },
   "source": [
    "### Tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf8dd2-a0ff-4870-b926-049ebb357480",
   "metadata": {
    "id": "dH6eU-l6xM_G"
   },
   "source": [
    "[Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation)  \n",
    "[TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55177cbc-25d1-42e4-8229-005d7bed4f4e",
   "metadata": {
    "id": "dH6eU-l6xM_G"
   },
   "source": [
    "### Papers / Courses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd894805-ecbb-4c18-88b6-1bab2f4ec1a8",
   "metadata": {
    "id": "dH6eU-l6xM_G"
   },
   "source": [
    "An [in-depth survey from CS230, Stanford University](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1794ccf-124f-4e3f-8a0b-d733260a21a1",
   "metadata": {
    "id": "dH6eU-l6xM_G"
   },
   "source": [
    "#### RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84771c8-d5ff-40e7-9603-cc157c46c0e3",
   "metadata": {
    "id": "dH6eU-l6xM_G"
   },
   "source": [
    "[Rumelhart, David E; Hinton, Geoffrey E, and Williams, Ronald J (Sept. 1985), \"Learning internal representations by error propagation\"](https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf)  \n",
    "[Jordan, Michael I. (May 1986), \"Serial order: a parallel distributed processing approach\"](https://www.osti.gov/biblio/6910294)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269511e-5d6c-40c2-87bf-645040949ceb",
   "metadata": {
    "id": "dH6eU-l6xM_G"
   },
   "source": [
    "#### LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c093dc-baea-496a-83c5-04883674c954",
   "metadata": {
    "id": "dH6eU-l6xM_G"
   },
   "source": [
    "[Sepp Hochreiter, JÃ¼rgen Schmidhuber, \"Long Short-Term Memory\"](https://arxiv.org/abs/2105.06756)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c7c306-f85c-4cbf-ba7f-ce95a2cbbbc8",
   "metadata": {
    "id": "dH6eU-l6xM_G"
   },
   "source": [
    "#### GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af3f2b1-220a-4ff9-8c9b-c002045861b9",
   "metadata": {
    "id": "dH6eU-l6xM_G"
   },
   "source": [
    "[Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, Yoshua Bengio, \"On the Properties of Neural Machine Translation: Encoder-Decoder Approaches\"](https://arxiv.org/abs/1409.1259)  \n",
    "[Ralf C. Staudemeyer, Eric Rothstein Morris, \"Understanding LSTM -- a tutorial into Long Short-Term Memory Recurrent Neural Networks\", arXiv](https://arxiv.org/abs/1909.09586)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d00f0d-5589-44c3-90f6-68d863a76a6a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ccbfe8-10c3-4f4b-9770-20bce6cc2d7b",
   "metadata": {},
   "source": [
    "## Text encoding, Unicode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "380d143c-8abd-4f6b-aba9-d6fa1384a821",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "qhnzYAq6IuWk",
    "outputId": "e7b43be5-37d9-41a8-d798-773b778731ff",
    "tags": []
   },
   "source": [
    "A good reference: [John Sturtz, \"Strings and Character Data in Python\", Real Python](https://realpython.com/python-strings/)  \n",
    "[Characters, Symbols and the Unicode Miracle - Computerphile](https://www.youtube.com/watch?v=MijmeoH9LT4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f08df-c08c-4122-9e3c-8cf4dadf41f9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd78f68-b237-4f6d-b3c6-f57cd304acb6",
   "metadata": {},
   "source": [
    "## NLP,  Word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "201dcd3d-fc25-4d5f-aa0d-068ca4dcc88a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "LaxliTf73KLn",
    "outputId": "23a558ac-613d-40da-8b81-fb333c35588b"
   },
   "source": [
    "See [Huggingface's NLP course](https://huggingface.co/learn/nlp-course/chapter1/1)  \n",
    "Tutorial on [Word embeddings](https://www.tensorflow.org/text/guide/word_embeddings) (3D embeddings with TensorBoard!)  \n",
    "[Stanford CS224N: NLP with Deep Learning | Winter 2021 | Lecture 1 - Intro & Word Vectors](https://www.youtube.com/watch?v=rmVRLeJRkl4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8885f4d9-2fb1-418e-a9d9-c2f78ba9e37d",
   "metadata": {
    "id": "c1eHeEqO3KLy"
   },
   "source": [
    "### NLP libraries & tools in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263469e-ba9c-412f-9d88-fb7c41d7c0a3",
   "metadata": {
    "id": "c1eHeEqO3KLy"
   },
   "source": [
    "#### NLTK: the Natural Language Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d88fe-85d3-4ae4-a543-78da0eaec95b",
   "metadata": {
    "id": "c1eHeEqO3KLy"
   },
   "source": [
    "NLTK is a leading platform for building Python programs to work with human language data ([website](https://www.nltk.org/))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79604b5-2265-438a-ad8f-b05703a61909",
   "metadata": {
    "id": "c1eHeEqO3KLy"
   },
   "source": [
    "#### Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60314e7c-6c1b-4a97-8f97-1381e53f3364",
   "metadata": {
    "id": "c1eHeEqO3KLy"
   },
   "source": [
    "Perhaps the best dedicated library for word vectors & similar text processing tools ([website](https://radimrehurek.com/gensim/index.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc9e8ae-2ed1-4d56-bf2a-70cd6ab8bb30",
   "metadata": {
    "id": "c1eHeEqO3KLy"
   },
   "source": [
    "#### spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fbf28-65a9-4bf0-91b3-1e0bafa8781c",
   "metadata": {
    "id": "c1eHeEqO3KLy"
   },
   "source": [
    "A a free open-source library for Natural Language Processing in Python ([website](https://spacy.io/))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c56113-4539-4c7a-bb76-3b5c5fddb872",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Transformers & Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86155b06-7243-495c-941b-43662e46a6d4",
   "metadata": {},
   "source": [
    "[3Blue1Brown, Large Language Models explained briefly  | DL5](https://www.youtube.com/watch?v=LPZh9BOjkQs&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=5) (and the rest of the playlist)   \n",
    "[MIT 6.S191, Recurrent Neural Networks, Transformers, and Attention (2025)](https://www.youtube.com/watch?v=GvezxUdLrEk)  \n",
    "[MIT 6.S191, Recurrent Neural Networks, Transformers, and Attention (2024)](https://www.youtube.com/watch?v=dqoEU9Ac3ek)  \n",
    "[MIT 6.S191, Recurrent Neural Networks, Transformers, and Attention (2023)](https://www.youtube.com/watch?v=ySEx_Bqxvvo)  \n",
    "[MIT 6.S191, Recurrent Neural Networks, Transformers, and Attention (2022)](https://www.youtube.com/watch?v=QvkQ1B3FBqA) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc7d25-80ff-47f3-8fd0-be2433f41cc4",
   "metadata": {
    "id": "bokyYLI9A6R3"
   },
   "source": [
    "### Tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f9b120-3e78-4201-a5b0-08263d7001e5",
   "metadata": {
    "id": "bokyYLI9A6R3",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "[Classify text with BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) (what is done below, without building the net)  \n",
    "[Neural machine translation with a Transformer and Keras](https://www.tensorflow.org/text/tutorials/transformer) (building the net from scratch, on another task)  \n",
    "More in the notebook [`2nd-ed.chapter11_part04_sequence_to_sequence_learning.ipynb`](https://github.com/jchwenger/AI/blob/main/lectures/06.more/2nd-ed.chapter11_part04_sequence_to_sequence_learning.ipynb)  \n",
    "[Huggingface's NLP course](https://huggingface.co/learn/nlp-course/chapter1/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f39c274-ad2d-4bbc-a5c5-d160708909d7",
   "metadata": {
    "id": "bokyYLI9A6R3"
   },
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782eb228-aad3-4ecb-a4b5-a3eacd491855",
   "metadata": {
    "id": "bokyYLI9A6R3",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "[Lucas Beyer, \"Transformers\"](https://docs.google.com/presentation/d/1ZXFIhYczos679r70Yu8vV9uO6B1J0ztzeDxbnBxD1S0/edit)  \n",
    "[Jay Alammar, \"The Illustrated Transformer\"](https://jalammar.github.io/illustrated-transformer/)  \n",
    "[Vaswani et al, \"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762)  \n",
    "[Tensor2Tensor Colab](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)  \n",
    "[Peter Bloem, \"Transformers From Scratch\"](https://peterbloem.nl/blog/transformers) (in PyTorch!)  \n",
    "[The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/) (same)  \n",
    "[BertViz, Visualize Attention in NLP Models](https://github.com/jessevig/bertviz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f9b38-0347-4816-bb21-8355b0d01a00",
   "metadata": {},
   "source": [
    "[Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 7 - Translation, Seq2Seq, Attention](https://www.youtube.com/watch?v=wzfWHP6SXxY)  \n",
    "[Stanford CS224n NLP with Deep Learning | 2023 | Lecture 8 - Self-Attention and Transformers](https://www.youtube.com/watch?v=LWMzyfvuehA)  \n",
    "[Vision Transformers (ViT) Explained + Fine-tuning in Python](https://www.youtube.com/watch?v=qU7wO02urYU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c99a7-457f-4137-958a-fd43d61586cd",
   "metadata": {
    "id": "V49irtbeA6R9"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c030bdc7-3f1c-497b-bb9b-71a172a8f859",
   "metadata": {
    "id": "V49irtbeA6R9"
   },
   "source": [
    "### A bit of history: Sequence to sequence modelling as a precursor to Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816b856e-2dfc-40b3-acc7-510c123c08b4",
   "metadata": {
    "id": "V49irtbeA6R9"
   },
   "source": [
    "A generic, powerful learning framework for many NLP problems (including machine translation): encode a source sequence first, then use that information to generate a target sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a6265-461a-474f-a71d-99ee7a1d5bc8",
   "metadata": {
    "id": "hwnaEZ6LA6R9"
   },
   "source": [
    "<!-- <img style=\"height:700px\" src=\"images/nlp/stanford.seq2seq.png\"> -->\n",
    "<img style=\"height:700px\" src=\"https://github.com/jchwenger/AI/blob/main/lectures/06/images/nlp/stanford.seq2seq.png?raw=true\">\n",
    "\n",
    "<small>[Chris Manning, CS224n, Stanford](https://web.stanford.edu/class/cs224n/index.html), [lecture 7](https://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture07-nmt.pdf)  \n",
    "The original paper: [\"Sequence to Sequence Learning with Neural Networks\"](https://arxiv.org/abs/1409.3215)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109474a5-ba9b-4123-ae0e-845b036371bc",
   "metadata": {
    "id": "SHo1sVx2A6R9"
   },
   "source": [
    "#### Sequence to sequence with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670aa32a-4921-49f0-a0f5-03efb9b497b8",
   "metadata": {
    "id": "SHo1sVx2A6R9"
   },
   "source": [
    "However, the previous setup needed to encode **everything** in that last stage. A real translator would look up the source target while translating...\n",
    "\n",
    "<!-- <img src=\"images/nlp/seq2seq-nmt-model-fast.gif\"> -->\n",
    "<img src=\"https://github.com/jchwenger/AI/blob/main/lectures/06/images/nlp/seq2seq-nmt-model-fast.gif?raw=true\">\n",
    "\n",
    "<small>[Google seq2seq documentation](https://google.github.io/seq2seq/)  \n",
    "And the paper: [\"Neural Machine Translation by Jointly Learning to Align and Translate\"](https://arxiv.org/abs/1409.0473)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980d032f-157d-4f11-8cde-596b51f61eee",
   "metadata": {
    "id": "9FOgrzV7A6R9"
   },
   "source": [
    "#### The Transformer: who *needs* RNNs anyway??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e7dce-4b33-490d-abd5-5ec1fd925cbf",
   "metadata": {
    "id": "9FOgrzV7A6R9"
   },
   "source": [
    "<!-- <img src=\"images/transformer/apply_the_transformer_to_machine_translation.gif\"> -->\n",
    "<img src=\"https://github.com/jchwenger/AI/blob/main/lectures/06/images/transformer/apply_the_transformer_to_machine_translation.gif?raw=true\">\n",
    "\n",
    "<small>[\"Neural machine translation with a Transformer and Keras\", TensorFlow](https://www.tensorflow.org/text/tutorials/transformer)  \n",
    "The paper: [Vaswani et al, \"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762)</small>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
