{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NNZhQ_jIuWh"
   },
   "source": [
    "# 14 Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Colab!\")\n",
    "    !pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTrS5pCPIuWh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import random\n",
    "import pathlib\n",
    "import unidecode\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import keras\n",
    "\n",
    "@register_cell_magic\n",
    "def backend(line, cell):\n",
    "    current, required = os.environ.get(\"KERAS_BACKEND\", \"\"), line.split()[-1]\n",
    "    if current == required:\n",
    "        get_ipython().run_cell(cell)\n",
    "    else:\n",
    "        print(\n",
    "            f\"This cell requires the {required} backend. To run it, change KERAS_BACKEND to \"\n",
    "            f\"\\\"{required}\\\" at the top of the notebook, restart the runtime, and rerun the notebook.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SMZwTxMIuWi"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SMZwTxMIuWi"
   },
   "source": [
    "## Natural-language processing (NLP): The bird's eye view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SMZwTxMIuWi"
   },
   "source": [
    "The **big** story of recent years has been a move away from **hand-crafted features**.\n",
    "\n",
    "Well until the 1990s the industry employed many linguists to implement specialist knowledge into rules for NLP systems.\n",
    "\n",
    "Then, statistical approaches took over, especially with the rise of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xj5VZ6xtIuWi"
   },
   "source": [
    "We leave aside the question of exactly **what** current systems understand from language.\n",
    "\n",
    "What we know is that we can now train them to take in pieces of text, execute task on them.\n",
    "\n",
    "- \"What’s the topic of this text?\" (**text classification**)  \n",
    "- \"Does this text contain abuse?\" (**content filtering**)  \n",
    "- \"Does this text sound positive or negative?\" (**sentiment analysis**)  \n",
    "- \"What should be the next word in this incomplete sentence?\" (**language modeling**)  \n",
    "- \"How would you say this in German?\" (**translation**)  \n",
    "- \"How would you summarize this article in one paragraph?\" (**summarization**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knljOOB5IuWk"
   },
   "source": [
    "## Preparing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knljOOB5IuWk"
   },
   "source": [
    "### Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knljOOB5IuWk"
   },
   "source": [
    "- **standardisation**: make your text easier to process\n",
    "- **tokenisation**: split into chunks\n",
    "- **indexing**: turn chunks into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YP6gAcCiIuWk"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YP6gAcCiIuWk"
   },
   "source": [
    "### Text encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YP6gAcCiIuWk"
   },
   "source": [
    "The question of **how language is represented** in computers is an important topic: if you're interested in working with language, do go and learn this!\n",
    "\n",
    "The [**The Unicode Standard**](https://en.wikipedia.org/wiki/Unicode) is a world-wide consensus to associate characters of all languages with a unique identifier (a number), called a **code point**.\n",
    "\n",
    "The Python documentation on [Unicode](https://docs.python.org/3/howto/unicode.html).\n",
    "\n",
    "**Code points** can then be **encoded** in bits ([**UTF-8**](https://en.wikipedia.org/wiki/UTF-8), 8 bits, is one of the ways you can do that, and is backward-compatible with [**ASCII**](https://en.wikipedia.org/wiki/ASCII), 7 bits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlMC3_SRIuWk"
   },
   "source": [
    "---\n",
    "\n",
    "### Text standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlMC3_SRIuWk"
   },
   "source": [
    "- Convert to **lower case**;  \n",
    "- Remove **punctuation**;\n",
    "- Convert **special characters** to a standard form (é → e);\n",
    "- *Stemming*: turn **inflected forms** into base ones (\"stares\"/\"stared\"/\"staring\" → \"[stare]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZ-0zRhtIuWk"
   },
   "outputs": [],
   "source": [
    "s = \"I didn't wake up this morning. How did that happen?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Kpr7_OiZIuWk",
    "outputId": "9c00a61a-4615-4c19-b8e2-4d03be759006"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"i didn't wake up this morning. how did that happen?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "m-bwsaLnIuWl",
    "outputId": "bd4d149e-ea4f-4627-a926-f80526db8c65"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'i didnt wake up this morning how did that happen'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(char for char in s.lower() if char not in string.punctuation) # removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XDsBolqIuWl",
    "outputId": "b226b0e4-51f7-463f-ac99-99a4a4c6e681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jérémie | b'J\\xc3\\xa9r\\xc3\\xa9mie' | [74, 195, 169, 114, 195, 169, 109, 105, 101]\n",
      "jérémie | b'j\\xc3\\xa9r\\xc3\\xa9mie' | [106, 195, 169, 114, 195, 169, 109, 105, 101]\n",
      "jeremie | b'jeremie' | [106, 101, 114, 101, 109, 105, 101]\n"
     ]
    }
   ],
   "source": [
    "def str_to_ints(s):\n",
    "    s_enc = s.encode()\n",
    "    print(f\"{s} | {s_enc} | {list(s_enc)}\") # encode into bytes, then turn into a list\n",
    "s = \"Jérémie\"\n",
    "str_to_ints(s)\n",
    "str_to_ints(s.lower())\n",
    "str_to_ints(unidecode.unidecode(s.lower())) # removing anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRAlSt-oIuWl"
   },
   "source": [
    "---\n",
    "\n",
    "### Text splitting (tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRAlSt-oIuWl"
   },
   "source": [
    "- **word-level tokenization**: split on spaces;\n",
    "- **n-gram tokenization**: same as above, but making groups of length n;\n",
    "- **character-level tokenization**: split on characters;\n",
    "- **byte-level tokenization**: split on bytes.\n",
    "\n",
    "These (except perhaps the last one) are forms of feature engineering, especially if combined with standardisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrEULzG3IuWl"
   },
   "source": [
    "**Note**: algorithms such as [*byte-pair encoding*](https://huggingface.co/course/chapter6/5?fw=pt) (BPE) or [WordPiece](https://huggingface.co/course/chapter6/6?fw=pt) combine the best of both worlds by processing a corpus at the byte level, and gradually merging frequent co-occurrences, parts of words, all the way up to whole words or even phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WhdMwwokIuWl",
    "outputId": "c1aedec2-28ce-4b50-dbd8-5a6c69376eb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox jumps over the lazy dog\n",
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "s = \"the quick brown fox jumps over the lazy dog\"\n",
    "s_split = s.split(\" \")\n",
    "print(s)\n",
    "print(s_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qq0aY5gWIuWl"
   },
   "outputs": [],
   "source": [
    "def n_grams(l, n=2):\n",
    "    n_grams = []\n",
    "    for i in range(0, len(s_split) - n + 1):\n",
    "        n_grams.append(s_split[i:i+n])\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VjWSWM2IuWl",
    "outputId": "5564d54c-a0a0-4197-db02-fa84565b3d8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'quick'],\n",
       " ['quick', 'brown'],\n",
       " ['brown', 'fox'],\n",
       " ['fox', 'jumps'],\n",
       " ['jumps', 'over'],\n",
       " ['over', 'the'],\n",
       " ['the', 'lazy'],\n",
       " ['lazy', 'dog']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_grams(s_split, n=2) # each token is two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzK9Yl9xIuWl",
    "outputId": "d79dbd1e-196e-43ac-c5d5-3a8a0a166f43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'quick', 'brown'],\n",
       " ['quick', 'brown', 'fox'],\n",
       " ['brown', 'fox', 'jumps'],\n",
       " ['fox', 'jumps', 'over'],\n",
       " ['jumps', 'over', 'the'],\n",
       " ['over', 'the', 'lazy'],\n",
       " ['the', 'lazy', 'dog']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_grams(s_split, n=3) # each token is three words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkUEOJrkIuWl"
   },
   "source": [
    "---\n",
    "\n",
    "### Vocabulary indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkUEOJrkIuWl"
   },
   "source": [
    "From a dataset, we create a big table **associating each word with a number**.\n",
    "\n",
    "We can also add special information like an index for:\n",
    "- **unknown words** (\"\\[UNK\\]\" for *unknown*, aka OOV: out of vocabulary);\n",
    "- **beginning of sentence** (\"bos\");\n",
    "- **end of sentence** (\"eos\");\n",
    "- **padding** (\"\\[PAD\\]\", to make sequences the same length);\n",
    "- **masked token**, and other customised \"special tokens\" depending on needs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhwO9Pk7IuWl",
    "outputId": "8c7034d1-96e9-4892-c981-105e764730aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brown': 0,\n",
       " 'dog': 1,\n",
       " 'fox': 2,\n",
       " 'jumps': 3,\n",
       " 'lazy': 4,\n",
       " 'over': 5,\n",
       " 'quick': 6,\n",
       " 'the': 7}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {token:i for i,token in enumerate(sorted(list(set(s_split))))}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3l0MYeJIuWl",
    "outputId": "dfe54042-1e93-4bbc-b9f3-67d41ee91f35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 6, 0, 2, 3, 5, 7, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "print([vocab[word] for word in s_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LtRwaqtcIuWl",
    "outputId": "eb5840cb-5499-43b3-f01a-ead1917aaae3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 6, 2]\n"
     ]
    }
   ],
   "source": [
    "print([vocab[word] for word in \"the quick fox\".split(\" \")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZcb-ojTIuWl"
   },
   "source": [
    "<!-- <img style=\"height: 400px\" src=\"images/chollet/figure14.1.png\"> -->\n",
    "<img style=\"height: 400px\" src=\"https://raw.githubusercontent.com/jchwenger/AI/main/lectures/07/images/chollet/figure14.1.png\">\n",
    "\n",
    "[DLWP](https://deeplearningwithpython.io/chapters/chapter14_text-classification/#preparing-text-data), Figure 14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWpr0xn0IuWl"
   },
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    \"\"\"\n",
    "    An implementation of the transformations in the above schema.\n",
    "    \"\"\"\n",
    "    def standardize(self, text):\n",
    "        \"\"\"Make lowercase and remove punctuation\"\"\"\n",
    "        text = text.lower()                                # Python built-in list of punctuation characters\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Standardize then split on space\"\"\"\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        \"\"\"Generate token vocabulary from text\"\"\"\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            tokens = self.tokenize(text) # this will standardize\n",
    "            for token in tokens:                                  # first element will be 2\n",
    "                if token not in self.vocabulary:                  # then len(vocab) is 3 →\n",
    "                    self.vocabulary[token] = len(self.vocabulary) # next token is 3, etc.\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Encode string to tokens\"\"\"\n",
    "        tokens = self.tokenize(text) # this will standardize\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        \"\"\"Decode tokens to string\"\"\"\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "    def one_hot_encode(self, encoded_sequence):\n",
    "        vector = np.zeros((len(encoded_sequence), len(self.vocabulary)))\n",
    "        for i, token in enumerate(encoded_sequence):\n",
    "            vector[i, token] = 1\n",
    "        return vector\n",
    "\n",
    "    def one_hot_decode(self, one_hot_sequence):\n",
    "        return list(np.where(one_hot_sequence == 1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpG6cgt-IuWm"
   },
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MbdP48mKIuWm",
    "outputId": "5cc8dd10-9afb-4470-ea24-d07131b96ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOR6DjYuIuWm",
    "outputId": "77506db7-d437-4f39-f9bf-461b807d438c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "one_hot_sentence = vectorizer.one_hot_encode(encoded_sentence)\n",
    "print(one_hot_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_4L30GkIuWm",
    "outputId": "1278b15c-3103-46fb-b2b3-679ed95f65c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "encoded_sentence = vectorizer.one_hot_decode(one_hot_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v_XBMqSjIuWm",
    "outputId": "2afb01fc-2f01-4915-bb39-e94f54ee0dec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tw4TzKF_IuWm"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tw4TzKF_IuWm"
   },
   "source": [
    "### Using the TextVectorization layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tw4TzKF_IuWm"
   },
   "source": [
    "Documentation for [`keras.layers.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eO7WqcCxIuWm"
   },
   "outputs": [],
   "source": [
    "text_vectorization = keras.layers.TextVectorization(\n",
    "    output_mode=\"int\" # returns a sequence of integers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifD2NRlcIuWm"
   },
   "source": [
    "A lot of options available in the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UeKP7RANIuWm"
   },
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]                # ↓ the `adapt()` method is the equivalent to the `fit()` one for models\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CLIYSJ01IuWm",
    "outputId": "26409653-d912-4daf-f2c4-9768f41a664c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "print(vocabulary) # displaying the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OhqA5_IRIuWm",
    "outputId": "0296c3d8-75b8-48c3-c704-d6a90d9ed605",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HuWhPD4TIuWm",
    "outputId": "90ae5f57-291a-451a-fb57-041c7ad1216e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence) # 'still' was not present in the original corpus (our three sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfhrHIweIuWm"
   },
   "source": [
    "**Note**\n",
    "\n",
    "How do you customize such a layer? You can pass user-defined functions to it.\n",
    "\n",
    "(Documentation for the [`tf.strings`](https://www.tensorflow.org/api_docs/python/tf/strings) module)  \n",
    "(Regular expressions are *very* useful for any text manipulation: tutorials [here](https://realpython.com/regex-python/), [here](https://realpython.com/regex-python-part-2/) and [here](https://docs.python.org/3/howto/regex.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJtQXlb_IuWm"
   },
   "outputs": [],
   "source": [
    "def custom_standardization_fn(string_tensor):\n",
    "    \"\"\"Standardize string tensor using regex (REGular EXpressions)\"\"\"\n",
    "    lowercase_string = tf.strings.lower(string_tensor)               # to lower case\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\"   # remove all punctuation\n",
    "    )\n",
    "\n",
    "def custom_split_fn(string_tensor):\n",
    "    \"\"\"Split string tensor\"\"\"\n",
    "    return tf.strings.split(string_tensor)                           # split on spaces\n",
    "\n",
    "text_vectorization = keras.layers.TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_standardization_fn, # apply special functions to\n",
    "    split=custom_split_fn,                 # standardize and split\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ltpAmcSIuWn"
   },
   "source": [
    "---\n",
    "\n",
    "## Two approaches for representing groups of words: Sets and sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ltpAmcSIuWn"
   },
   "source": [
    "- Sets: \"bag of words\", the order is lost.  \n",
    "- Sequences: order/time matters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLHI2A5CIuWn"
   },
   "source": [
    "### Preparing the IMDB movie reviews data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmNQaHSpIuWn"
   },
   "source": [
    "#### Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WWSsZ68JLbF"
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = pathlib.Path(\"aclImdb\")\n",
    "\n",
    "if not DATASET_DIR.exists():\n",
    "    !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    !tar -xf aclImdb_v1.tar.gz # this untars the archive to a folder called aclImdb\n",
    "    !rm -r aclImdb/train/unsup\n",
    "\n",
    "MODELS_DIR = pathlib.Path(\"models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KPlX0kr9IuWn",
    "outputId": "b9447ba1-e733-40e3-a041-2b97a67ae546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
     ]
    }
   ],
   "source": [
    "!cat aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmNQaHSpIuWn"
   },
   "source": [
    "#### Process using `text_dataset_from_directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLU7TOmAIuWn"
   },
   "outputs": [],
   "source": [
    "# code to split the data into train/val folders\n",
    "\n",
    "TRAIN_DIR = DATASET_DIR / \"train\"\n",
    "VAL_DIR = DATASET_DIR / \"val\"\n",
    "TEST_DIR = DATASET_DIR / \"test\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    if not os.path.isdir(VAL_DIR / category):    # do this only once\n",
    "        os.makedirs(VAL_DIR / category)          # make 'neg'/'pos' dir in validation\n",
    "        files = os.listdir(TRAIN_DIR / category) # list files in 'train'\n",
    "        random.Random(1337).shuffle(files)       # shuffle using a seed\n",
    "        num_val_samples = int(0.2 * len(files))  # 2% of our samples for validation\n",
    "        val_files = files[-num_val_samples:]\n",
    "        for fname in val_files:                  # move our files\n",
    "            shutil.move(TRAIN_DIR / category / fname,\n",
    "                        VAL_DIR / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xee7wTfuIuWn",
    "outputId": "6ee864e3-7468-4e0a-d7e7-cd7693f98578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Keras helper functions creating Tensorflow Dataset\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "# The documentation:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/utils/text_dataset_from_directory\n",
    "\n",
    "# each of these iterables returns tuples containing two tensors:\n",
    "# samples, shape: (batch_size, sample_shape) ← our texts\n",
    "# targets, shape: (batch_size,)              ← 0 or 1\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    TRAIN_DIR, batch_size=BATCH_SIZE\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    VAL_DIR, batch_size=BATCH_SIZE\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    TEST_DIR, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgnkhEulIuWn",
    "outputId": "9bf0c5aa-335b-45b6-ae62-108d97b115a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "\n",
      "targets[0]: 0\n",
      "\n",
      "inputs[0]: b\"This film is awful. Not offensive but extremely predictable. The movie follows the life of a small town family in the mid-60's. The father, the principal at the school, is going through a mid-life crisis. Enter a pretty teacher from the big city who starts challenging her students' minds with some thought-provoking stuff, like think for yourself. The principal doesn't agree with her teaching but she is pretty. You can connect the dots. His teenage daughter (Winona Ryder wannabe Tara Frederick) is fed up with the small town lifestyle and wants to live. She gets some bad advice, hangs out with some bad boys and apparently family planning wasn't being taught at her school. Shocking! Seeing that director Paul Shapiro has mainly worked in TV, this movie plays like a more adult version of an after-school special or a very special episode of one of the more mundane sitcoms.\"\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:            # displaying the shapes and dtypes of the first batch\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print()\n",
    "    print(\"targets[0]:\", targets[0].numpy())\n",
    "    print()\n",
    "    print(\"inputs[0]:\", inputs[0].numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bR1iwloqIuWn"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bR1iwloqIuWn"
   },
   "source": [
    "## Processing words as a set: The bag-of-words approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bR1iwloqIuWn"
   },
   "source": [
    "### Single words (unigrams) with binary encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bR1iwloqIuWn"
   },
   "source": [
    "- unigram: first you split, then take each split on its own\n",
    "- binary encoding: multi-hot\n",
    "- the order is lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZedLG_6NIuWn",
    "outputId": "068830a0-f256-460c-c7c6-0c418998bf11",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat', 'mat', 'on', 'sat', 'the'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(\"the cat sat on the mat\".split()) # unigram == bag of words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-GiEcRQIuWn"
   },
   "outputs": [],
   "source": [
    "text_vectorization = keras.layers.TextVectorization( # preprocessing our datasets with a `TextVectorization` layer\n",
    "    max_tokens=20000,                                   # cap the vocab size to 20k\n",
    "    output_mode=\"multi_hot\",                            # a 1 at the index of each word present, 0 otherwise\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qepd-rWMIuWn"
   },
   "outputs": [],
   "source": [
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oj7wuLuKIuWn"
   },
   "outputs": [],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "inverse_vocab = dict(enumerate(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtSZ2fNdIuWn",
    "outputId": "116a74f0-5519-4707-90a6-103e801bf39c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ... 0 0 0] (20000,)\n",
      "\n",
      "[[   1]\n",
      " [1161]]\n",
      "\n",
      "the cat\n"
     ]
    }
   ],
   "source": [
    "cat = text_vectorization(\"the cat\")\n",
    "print(cat.numpy(), cat.shape)                                    # \"the cat\" is now a vector of 20000 zeros\n",
    "                                                                 #  with 1s at the index of \"the\" and \"cat\"\n",
    "print()\n",
    "indz = tf.where(cat > 0)                                         # retrieving the indices of the words\n",
    "print(indz.numpy())\n",
    "print()\n",
    "print(\" \".join([inverse_vocab[i.numpy().item()] for i in indz])) # retranslating back into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a1hECQ-IuWn",
    "outputId": "bef6f380-0519-4bb7-da48-019d69ddcf26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ... 0 0 0] (20000,)\n",
      "\n",
      "[[   1]\n",
      " [1020]\n",
      " [1161]]\n",
      "\n",
      "the cute cat\n"
     ]
    }
   ],
   "source": [
    "cat = text_vectorization(\"the cute cat\")\n",
    "print(cat.numpy(), cat.shape)\n",
    "print()\n",
    "indz = tf.where(cat > 0)\n",
    "print(indz.numpy())\n",
    "print()\n",
    "print(\" \".join([inverse_vocab[i.numpy().item()] for i in indz]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IDJKkucIuWn"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IDJKkucIuWn"
   },
   "source": [
    "### Training a simple model with unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkiWlB5xIuWn"
   },
   "outputs": [],
   "source": [
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4  # adding a casting to float as the new TF version returns int64\n",
    ")\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXdwzA7TIuWn",
    "outputId": "d1c27f32-74ce-4e13-eb4d-5731af9e4aa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'int64'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "\n",
      "targets[0]: 0\n",
      "\n",
      "inputs[0]: [1 1 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds: # inspecting the output of our binary unigram dataset\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print()\n",
    "    print(\"targets[0]:\", targets[0].numpy())\n",
    "    print()\n",
    "    print(\"inputs[0]:\", inputs[0].numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a12Y7azMIuWo"
   },
   "outputs": [],
   "source": [
    "def get_model(max_tokens=20000, hidden_dim=16, clear=True):\n",
    "    \"\"\"\n",
    "    Our model-building utility\n",
    "    \"\"\"\n",
    "    if clear:\n",
    "        keras.backend.clear_session()\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = keras.layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\", # our labels are only 0 or 1 (negative/positive)\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 660
    },
    "id": "Ls3mN9HgIuWo",
    "outputId": "77038ed1-3866-4628-a551-54e98389f74f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">320,016</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20000\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │         \u001b[38;5;34m320,016\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m17\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 898ms/step - accuracy: 0.7727 - loss: 0.4855 - val_accuracy: 0.8712 - val_loss: 0.3115\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 7ms/step - accuracy: 0.8972 - loss: 0.2767 - val_accuracy: 0.8812 - val_loss: 0.2989\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9144 - loss: 0.2383 - val_accuracy: 0.8784 - val_loss: 0.3159\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9231 - loss: 0.2250 - val_accuracy: 0.8806 - val_loss: 0.3268\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9286 - loss: 0.2249 - val_accuracy: 0.8798 - val_loss: 0.3427\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9296 - loss: 0.2112 - val_accuracy: 0.8766 - val_loss: 0.3549\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9350 - loss: 0.2054 - val_accuracy: 0.8734 - val_loss: 0.3695\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9389 - loss: 0.1957 - val_accuracy: 0.8776 - val_loss: 0.3878\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9391 - loss: 0.2005 - val_accuracy: 0.8720 - val_loss: 0.3934\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9399 - loss: 0.2062 - val_accuracy: 0.8706 - val_loss: 0.3952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7955f0763a60>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the binary unigram model\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = [ # using a callback to save our model\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        str(MODELS_DIR / \"binary_1gram.keras\"),\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    binary_1gram_train_ds.cache(),\n",
    "    validation_data=binary_1gram_val_ds.cache(),\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p4TON28UIuWo",
    "outputId": "ff073b51-4b6a-405a-9153-4f7022834e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m508s\u001b[0m 649ms/step - accuracy: 0.8823 - loss: 0.2938\n",
      "Test acc: 0.882\n"
     ]
    }
   ],
   "source": [
    "# And testing it\n",
    "model = keras.models.load_model(MODELS_DIR / \"binary_1gram.keras\")\n",
    "_, acc = model.evaluate(binary_1gram_test_ds)\n",
    "print(f\"Test acc: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IU5Jv_iLUoNE"
   },
   "outputs": [],
   "source": [
    "del (\n",
    "    model, text_vectorization,\n",
    "    binary_1gram_train_ds, binary_1gram_val_ds, binary_1gram_test_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xY38VxTrIuWo"
   },
   "source": [
    "----\n",
    "\n",
    "### Bigrams with binary encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xY38VxTrIuWo"
   },
   "source": [
    "- bigram: first you split, then take tokens two by two   \n",
    "- binary encoding: multi-hot (each bigram is one token!)\n",
    "- *almost all* order is lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9XfAr8sIuWo"
   },
   "outputs": [],
   "source": [
    "text_vectorization = keras.layers.TextVectorization(\n",
    "    ngrams=2, # ← returns bigrams\n",
    "    max_tokens=20000, output_mode=\"multi_hot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhriwK4vIuWo",
    "outputId": "a166dffb-bcdb-4dd5-ca8e-ac2790ede7e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 tokens: ['[UNK]', 'erase', 'write erase', 'write', 'then', 'rewrite', 'poppy blooms', 'poppy', 'i write', 'i', 'erase rewrite', 'erase again', 'blooms', 'and then', 'and', 'again and', 'again', 'a poppy', 'a']\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt([\"I write, erase, rewrite\",\"Erase again, and then\",\"A poppy blooms.\",])\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "print(f\"{len(vocabulary)} tokens: {vocabulary}\") # displaying the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4R2ZDsiYIuWo",
    "outputId": "aca8e362-21ce-4350-c675-81c0abb0a347",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0], shape=(19,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "encoded_sentence = text_vectorization(\"I write, rewrite, and still rewrite again\")\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwGgW7X4IuWo"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwGgW7X4IuWo"
   },
   "source": [
    "### Training a simple model with bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VjQJlbaIuWo"
   },
   "outputs": [],
   "source": [
    "text_vectorization = keras.layers.TextVectorization(\n",
    "    ngrams=2, # ← returns bigrams\n",
    "    max_tokens=20000, output_mode=\"multi_hot\",\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# prepare our datasets\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 660
    },
    "id": "COXxTLcSIuWo",
    "outputId": "f86757e0-e001-4689-eae3-3c976cf87f15"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">320,016</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20000\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │         \u001b[38;5;34m320,016\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m17\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">320,033</span> (1.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m320,033\u001b[0m (1.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1016s\u001b[0m 2s/step - accuracy: 0.7962 - loss: 0.4498 - val_accuracy: 0.8910 - val_loss: 0.2777\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9164 - loss: 0.2353 - val_accuracy: 0.8916 - val_loss: 0.2858\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9377 - loss: 0.1893 - val_accuracy: 0.8924 - val_loss: 0.3068\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9451 - loss: 0.1714 - val_accuracy: 0.8896 - val_loss: 0.3230\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9492 - loss: 0.1581 - val_accuracy: 0.8890 - val_loss: 0.3471\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9564 - loss: 0.1560 - val_accuracy: 0.8924 - val_loss: 0.3585\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9570 - loss: 0.1473 - val_accuracy: 0.8838 - val_loss: 0.3800\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9585 - loss: 0.1451 - val_accuracy: 0.8864 - val_loss: 0.3885\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9593 - loss: 0.1546 - val_accuracy: 0.8844 - val_loss: 0.4055\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9620 - loss: 0.1395 - val_accuracy: 0.8824 - val_loss: 0.4088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x79558e747df0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the binary bigram model\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        str(MODELS_DIR / \"binary_2gram.keras\"),\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    binary_2gram_train_ds.cache(),\n",
    "    validation_data=binary_2gram_val_ds.cache(),\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5DphQx_IuWo",
    "outputId": "0f4446d8-e321-4b62-e01e-51c73db74b6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m977s\u001b[0m 1s/step - accuracy: 0.8987 - loss: 0.2649\n",
      "Test acc: 0.899\n"
     ]
    }
   ],
   "source": [
    "#  and testing\n",
    "model = keras.models.load_model(MODELS_DIR / \"binary_2gram.keras\")\n",
    "_, acc = model.evaluate(binary_2gram_test_ds)\n",
    "print(f\"Test acc: {acc:.3f}\") # better than the unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Lv6lHSqUxi6"
   },
   "outputs": [],
   "source": [
    "del (\n",
    "    model, text_vectorization,\n",
    "    binary_2gram_train_ds, binary_2gram_val_ds, binary_2gram_test_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18bGckS_IuWo"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8B3G9T1FZ6Ua"
   },
   "source": [
    "### Save models to Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYIXMpgSV2ks"
   },
   "outputs": [],
   "source": [
    "EXPORT=False\n",
    "\n",
    "if EXPORT:\n",
    "    # zip models\n",
    "    !zip aclImdb.models.zip {MODELS_DIR}/*\n",
    "    # connect to drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # copy zip to drive (adjust folder as needed)\n",
    "    !cp aclImdb.models.zip drive/MyDrive/gold/IS53024B-Artificial-Intelligence/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_Hw1pPDIuWo"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_Hw1pPDIuWo"
   },
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_Hw1pPDIuWo"
   },
   "source": [
    "- **standardisation** / **tokenisation** / **indexing**\n",
    "- tokenisation: various possible **levels** (words, characters, n-grams, etc.)\n",
    "\n",
    "- **Two kinds of NLP models**:\n",
    "  - **bag-of-words** sets of words or n-grams, the order is lost.\n",
    "  - **sequence models** that process word order."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
