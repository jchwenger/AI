{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77737f0-7a70-4b18-aa48-62ff4e9367f9",
   "metadata": {},
   "source": [
    "# References | Text & Image Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c56419-d3e2-482c-b176-15bfb48d13ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3436a0-1141-479d-a0e1-bdb2b49b08f2",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e717461-6084-4217-a8e9-dd3e6435e1e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "gZO34IitCfw2",
    "outputId": "a06e68c7-bcfa-44d3-d21f-c33760260365"
   },
   "source": [
    "[Sunspring | A Sci-Fi Short Film Starring Thomas Middleditch](https://www.youtube.com/watch?v=LY7x2Ihqjmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8628187-f1e5-49ed-97b9-db8b56ecfca8",
   "metadata": {
    "id": "oTQ6A8m4CfxV"
   },
   "source": [
    "#### Tutorials:\n",
    "\n",
    "[\"Text generation with a miniature GPT\"](https://keras.io/examples/generative/text_generation_with_miniature_gpt/): pretty much the same as here, with some interesting variations (the Transformer architecture is closer to what's used for ChatGPT).  \n",
    "[\"Text generation with an RNN\"](https://www.tensorflow.org/text/tutorials/text_generation) using an RNN to train an auto-regressive char-level language model (some nice tricks using `tf.data.Dataset`).\n",
    "\n",
    "#### Reference\n",
    "\n",
    "One of the most famous blog posts in deep learning, the inspiration for the above tutorial:  \n",
    "[Andrej Karpathy, \"The Unreasonable Effectiveness of Recurrent Neural Networks\"](https://karpathy.github.io/2015/05/21/rnn-effectiveness/).   \n",
    "[Holtzman et al, \"The Curious Case of Neural Text Degeneration\"](https://arxiv.org/abs/1904.09751)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c36b5-a8a5-46ad-87ac-723f53e667be",
   "metadata": {
    "id": "o9m8M4mbv39w"
   },
   "source": [
    "\n",
    "### The rise of large language models (LLMs)\n",
    "\n",
    "Truly remarkable results emerge with very large models. Several companies have all built such models to try and make a business out of it. They have APIs with a free tier that allow you to test these capabilities:\n",
    "\n",
    "- [OpenAI's ChatGPT](https://openai.com/blog/chatgpt/)  \n",
    "- [OpenAI's GPT-4](https://openai.com/api/)\n",
    "- [Cohere](https://cohere.ai/)\n",
    "- [Anthropic](https://www.anthropic.com)\n",
    "- [GooseAI](https://goose.ai/) (open-source)\n",
    "- [Huggingface Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) (open-source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d3a77-e35d-41ba-888e-53f992ec7502",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "g0YtreH9Cfwv",
    "outputId": "ed59545a-92bc-403c-afe7-e08543c46f84"
   },
   "source": [
    "[MIT 6.S191: Deep Generative Modeling (2025)](https://www.youtube.com/watch?v=SdTZAMDKrNY)  \n",
    "[MIT 6.S191: Deep Generative Modeling (2024)](https://www.youtube.com/watch?v=Dmm4UG-6jxA)  \n",
    "[MIT 6.S191: Deep Generative Modeling (2023)](https://www.youtube.com/watch?v=3G5hWM6jqPk)  \n",
    "[MIT 6.S191: Deep Generative Modeling (2022)](https://www.youtube.com/watch?v=QcLlc9lj2hk)  \n",
    "\n",
    "[Stanford CS 231N, Lecture 13 | Generative Models](https://www.youtube.com/watch?v=5WoItGTWV54)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06654523-0417-42b8-b917-e7ef103c43f4",
   "metadata": {},
   "source": [
    "### Even more sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc4189-0a27-411a-b38a-75b66cd62588",
   "metadata": {
    "id": "ipbRGxxNCfxD"
   },
   "source": [
    "- [min p sampling](https://arxiv.org/abs/2407.01082) ([video](https://www.youtube.com/watch?v=LTf_SJOQH4s)): take the top probability, multiply it by a value (e.g. `0.2`, 20% of that), and use the result as a threshold (any token with less probability than that is discarded)\n",
    "- [top a sampling](https://github.com/BlinkDL/RWKV-LM/tree/4cb363e5aa31978d801a47bc89d28e927ab6912e?tab=readme-ov-file#the-top-a-sampling-method): same idea as *min p*, except the threshold is computed using $\\alpha * \\text{top-prob}^\\beta$, with $\\text{top-prob}$ being the top probability among our tokens, $\\alpha\\ (= 0.2)$ and $\\beta\\ (=2)$ as hyperparameters\n",
    "- [locally typical sampling](https://arxiv.org/abs/2202.00666) ([video](https://www.youtube.com/watch?v=_EDr3ryrT_Y&pp=ygUYdHlwaWNhbCBzYW1wbGluZyBraWxjaGVy) & [interview](https://www.youtube.com/watch?v=AvHLJqtmQkE)): sample only from tokens with an expected information content close to the conditional entropy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166618f-fd1b-467d-8ecd-0fdee1937594",
   "metadata": {},
   "source": [
    "### Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426464e-c9db-49ae-bb78-858c18072e92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "OgGTDPyTpta9",
    "outputId": "c9dda89e-0c6e-47fc-8d65-36d12c6da7c2"
   },
   "source": [
    "[Positional embeddings in transformers EXPLAINED | Demystifying positional encodings.](https://www.youtube.com/watch?v=1biZfFLPRSY)  \n",
    "[How positional encoding works in transformers?](https://www.youtube.com/watch?v=T3OT8kqoqjc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c57d4-c8d7-4eb6-8d04-abc85e9ad29e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa1bbe7-4fd0-4fae-9b63-1e2c838f6061",
   "metadata": {},
   "source": [
    "## Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf73aed-0faa-44e9-9eee-abbe1f970e5b",
   "metadata": {},
   "source": [
    "### Latent space, KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81211888-357c-4cb8-ace6-450d3c6eeed2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "4xoTl-kGmPRf",
    "outputId": "0ebe6877-b4f0-42fd-f5aa-404289690c8e"
   },
   "source": [
    "[Variational Autoencoder (VAE) Latent Space Visualization](https://www.youtube.com/watch?v=sV2FOdGqlX0)  \n",
    "[A Short Introduction to Entropy, Cross-Entropy and KL-Divergence](https://www.youtube.com/watch?v=ErfnhcEV1O8)  \n",
    "[Intuitively Understanding the KL Divergence](https://www.youtube.com/watch?v=SxGYPqCgJWM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6ebeef-a6c1-4e06-a6a7-b27e9db801a9",
   "metadata": {},
   "source": [
    "### Talks & courses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a4c799-c605-4d78-828d-11b9aad26331",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "QWCDXwypmPRh",
    "outputId": "478689ff-062e-4058-9783-91d01723efd8",
    "scrolled": true
   },
   "source": [
    "[ICLR14: D Kingma: Auto-Encoding Variational Bayes](https://www.youtube.com/watch?v=rjZL7aguLAs)  \n",
    "[Stanford CS236: Deep Generative Models I 2023 I Lecture 5 - VAEs](https://www.youtube.com/watch?v=MAGBUh77bNg)  \n",
    "[Stanford CS236: Deep Generative Models I 2023 I Lecture 6 - VAEs](https://www.youtube.com/watch?v=8cO61e_8oPY)  \n",
    "[L4 Latent Variable Models and Variational AutoEncoders -- CS294-158 SP24 Deep Unsupervised Learning](https://www.youtube.com/watch?v=NlIqjtbjjRE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df59f3ca-05a8-4c6b-bae3-210719349eba",
   "metadata": {
    "id": "SyDJlERVmPRi"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f50ec37-887d-43d5-be6d-cd06d83cdb2c",
   "metadata": {
    "id": "SyDJlERVmPRi"
   },
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f945ddd7-fe2a-49b0-8b22-35897002ca61",
   "metadata": {
    "id": "SyDJlERVmPRi"
   },
   "source": [
    "[TensorFlow tutorial](https://www.tensorflow.org/tutorials/generative/cvae)\n",
    "\n",
    "[Kingma and Welling, \"Auto-Encoding Variational Bayes\"](https://arxiv.org/abs/1312.6114)  \n",
    "[Kingma and Welling, \"An Introduction to Variational Autoencoders\"](https://arxiv.org/abs/1906.02691)\n",
    "\n",
    "[Arxiv insight, Variational Autoencoders](https://www.youtube.com/watch?v=9zKuYvjFFS8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7424ea9e-8f9b-49ef-9f8f-d5efb08855a0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad07135-22da-4bb7-b44c-8212df10cc54",
   "metadata": {},
   "source": [
    "## Diffusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14ba91a1-bac0-4d63-85da-26350985abe7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "oa3F7e6Rpta9",
    "outputId": "a726f1f8-45ca-4386-dedc-69b4548ae49b"
   },
   "source": [
    "Original tutorial: [Denoising Diffusion Probabilistic Model](https://keras.io/examples/generative/ddpm/)  \n",
    "\n",
    "See also this, with an introduction to the FID score (to measure the quality of images):  [Denoising Diffusion Implicit Models](https://keras.io/examples/generative/ddim/)  \n",
    "\n",
    "Two more in Keras:\n",
    "\n",
    "[High-performance image generation using Stable Diffusion in KerasCV](https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/)  \n",
    "[A walk through latent space with Stable Diffusion](https://keras.io/examples/generative/random_walks_with_stable_diffusion/)\n",
    "\n",
    "The original paper: [Ho et al, \"Denoising Diffusion Probabilistic Models\"](https://arxiv.org/abs/2006.11239) (and for an extra-thick cream top-up, the [author's implementation](https://github.com/hojonathanho/diffusion)).\n",
    "\n",
    "[How AI Image Generators Work (Stable Diffusion / Dall-E) - Computerphile](https://www.youtube.com/watch?v=1CIpzeNxIhU)  \n",
    "[Stable Diffusion in Code (AI Image Generation) - Computerphile](https://www.youtube.com/watch?v=-lz30by8-sU)  \n",
    "[What are Diffusion Models?](https://www.youtube.com/watch?v=fbLgFrlTnGU)  \n",
    "[DDPM - Diffusion Models Beat GANs on Image Synthesis (Machine Learning Research Paper Explained)](https://www.youtube.com/watch?v=W-O7AZNzbzQ)  \n",
    "[Tutorial on Denoising Diffusion-based Generative Modeling: Foundations and Applications](https://www.youtube.com/watch?v=cS6JQpEY9cs)  \n",
    "[Miika Aittala: Elucidating the Design Space of Diffusion-Based Generative Models](https://www.youtube.com/watch?v=T0Qxzf0eaio)\n",
    "\n",
    "Good resources exist in PyTorch, such as the series of videos by [Fast.ai](https://www.fast.ai/) (in PyTorch):\n",
    "- [Lesson 9: Deep Learning Foundations to Stable Diffusion, 2022](https://www.youtube.com/watch?v=_7rMfsA24Ls)\n",
    "- [Lesson 9A 2022 - Stable Diffusion deep dive](https://www.youtube.com/watch?v=0_BBRNYInx8)\n",
    "- [Lesson 9B - the math of diffusion](https://www.youtube.com/watch?v=mYpjmM7O-30)\n",
    "- [Lesson 10: Deep Learning Foundations to Stable Diffusion, 2022](https://www.youtube.com/watch?v=6StU6UtZEbU)\n",
    "\n",
    "As well as John O. Whittaker's [intro on Diffusion](https://www.youtube.com/watch?v=XTs7M6TSK9I) from his own course, [AIAIART](https://github.com/johnowhitaker/aiaiart).\n",
    "\n",
    "And the in-depth code guide: the [Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
