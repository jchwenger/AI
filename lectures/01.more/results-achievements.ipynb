{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cbe520d-41e6-4f56-bdf4-dbdf8a10c6c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Results & Achievements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112c72a-27a4-4226-9160-19b59dda50f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.1.6-1.1.8 Achievements "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e2c463-703d-40b5-be12-e622c5b70d45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14373b5d-db24-4130-b31d-b870bb7b2ae8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Movenet Multipose Estimation](https://www.youtube.com/watch?v=XxJNebC_oqc)  \n",
    "See also: [Realtime Multi-Person 2D Human Pose Estimation](https://www.youtube.com/watch?v=pW6nZXeWlGM)  \n",
    "[Powered by TensorFlow: helping paleographers transcribe medieval text using machine learning](https://www.youtube.com/watch?v=v-FgOACRgfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e3c3e-219e-4997-b8b8-64e77c4b7cd3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now also for animals! \n",
    "\n",
    "**Social LEAP Estimates Animal Poses (SLEAP)**   \n",
    "[sleap.ai](https://sleap.ai/)\n",
    "\n",
    "<!-- ![Social LEAP Estimates Animal Poses (SLEAP)](images/achievements/sleap_movie.gif) -->\n",
    "\n",
    "![Social LEAP Estimates Animal Poses (SLEAP)](https://raw.githubusercontent.com/jchwenger/AI/main/lectures/01.more/images/achievements/sleap_movie.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5111e704-5b68-417f-9bb6-fe9f621a875e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dries Depoorter, [*The Follower*](https://driesdepoorter.be/thefollower/)\n",
    "\n",
    "<!-- ![The Follower](images/achievements/The-Follower-Dries-Depoorter-03.gif) -->\n",
    "![The Follower](https://raw.githubusercontent.com/jchwenger/AI/main/lectures/01.more/images/achievements/The-Follower-Dries-Depoorter-03.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08841c13-45c2-44c0-9560-ce5723a3c1e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919de18-9959-42d0-97d3-0032c2142400",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[The Coded Gaze: Unmasking Algorithmic Bias](https://www.youtube.com/watch?v=162VzSzzoPs)  \n",
    "[Stanford HAI 2019 Fall Conference - Keynote: The Coded Gaze with Joy Buolamwini](https://www.youtube.com/watch?v=Mk5gLInf7So)  \n",
    "[Timnit Gebru (Google) -- Towards transparency in AI, Methods and Challenges](https://www.youtube.com/watch?v=0GrknKMfdTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1436d03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Recommender systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4947d1b-0d0d-4440-b2ee-d2381ccb41e1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[How TikTok's Algorithm Figures You Out | WSJ](https://www.youtube.com/watch?v=nfczi2cI6Cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf546e-fea2-4587-b401-a5550bbc9984",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Autonomous Driving & flying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7383325-48e9-4596-8218-ca5856040d36",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Full Self-Driving](https://www.youtube.com/watch?v=tlThdr3O5Qo)  \n",
    "[Quadcopter Navigation in the Forest using Deep Neural Networks](https://www.youtube.com/watch?v=umRdt3zGgpU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b15541a-0dc0-4b9b-b1c7-0e91582baefc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0fa50e-9498-4036-a1c4-32b401ec5781",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Hito Steyerl: The Language of Broken Glass](https://www.youtube.com/watch?v=iyyM4vDg0xw&start=62)  \n",
    "[What if You Could Sing in Your Favorite Musician's Voice? | Holly Herndon | TED](https://www.youtube.com/watch?v=5cbCYwgQkTE)  \n",
    "[Udio: Make your music](https://www.youtube.com/watch?v=-pcUUENVxW8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382b71d-fe58-4688-ba2e-438e858ceb22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## GANs & VAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1279aafe-4158-44ef-8730-0d854a692fac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[CycleGAN horse zebra](https://www.youtube.com/watch?v=9reHvktowLY)  \n",
    "[Mario Klingemann: The Hypnotic Allure of the AI Art Generator](https://www.youtube.com/watch?v=Jjv3m5oWICA)  \n",
    "[Mario Klingemann: StyleGAN2 - mapping music to facial expressions in real time](https://www.youtube.com/watch?v=A6bo_mIOto0)  \n",
    "[Synthesizing High-Resolution Images with StyleGAN2](https://www.youtube.com/watch?v=9QuDh3W3lOY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefbda6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## DeepFakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732df9dc-7715-4e27-89b5-02d623a808c7",
   "metadata": {},
   "source": [
    "[DeepFakes](https://www.youtube.com/watch?v=l82PxsKHxYc&t=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf800bef-8244-4ab3-b83c-b88142cfb204",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- ![DeepFake timeline](images/deepfake/deepfake-timeline.png) -->\n",
    "![DeepFake timeline](https://raw.githubusercontent.com/jchwenger/AI/main/lectures/01.more/images/deepfake/deepfake-timeline.png)\n",
    "\n",
    "[Momina Masood, Marriam Nawaz, Khalid Mahmood Malik, Ali Javed, Aun Irtaza, *Deepfakes Generation and Detection: State-of-the-art, open challenges, countermeasures, and way forward*](https://arxiv.org/abs/2103.00484)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c0daa1-4914-40f5-855a-c87158e9c6bb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- ![DeepFake table](images/deepfake/deepfake-table.png) -->\n",
    "![DeepFake table](https://raw.githubusercontent.com/jchwenger/AI/main/lectures/01.more/images/deepfake/deepfake-table.png)\n",
    "\n",
    "[Momina Masood, Marriam Nawaz, Khalid Mahmood Malik, Ali Javed, Aun Irtaza, *Deepfakes Generation and Detection: State-of-the-art, open challenges, countermeasures, and way forward*](https://arxiv.org/abs/2103.00484)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec617c-d1ce-4e0d-a351-d51fa25a421f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Adversarial attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7833a5ad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- ![Adversarial imperceptible noise](images/adversarial/adversarial-imperceptible.png) -->\n",
    "![Adversarial imperceptible noise](https://raw.githubusercontent.com/jchwenger/AI/main/lectures/01.more/images/adversarial/adversarial-imperceptible.png)\n",
    "\n",
    "[Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy, *Explaining and Harnessing Adversarial Examples*](https://arxiv.org/abs/1412.6572)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3220750-9e2c-4d60-9609-8a79641d1446",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Lecture 16 | Adversarial Examples and Adversarial Training](https://www.youtube.com/watch?v=CIfsB_EYsVI)  \n",
    "[Generating adversarial patches against YOLOv2](https://www.youtube.com/watch?v=MIbFvK2S9g8)  \n",
    "[Tricking AI Image Recognition - Computerphile](https://www.youtube.com/watch?v=gGIiechWEFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547380dc",
   "metadata": {},
   "source": [
    "<!-- ![Adversarial concentric circles](images/adversarial/concentric-circles.png) -->\n",
    "![Adversarial concentric circles](https://raw.githubusercontent.com/jchwenger/AI/main/lectures/01.more/images/adversarial/concentric-circles.png)\n",
    "\n",
    "[Ian Goodfellow, Lecture 16 | Adversarial Examples and Adversarial Training](https://youtu.be/CIfsB_EYsVI?t=3260)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8bb6b9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- ![Adversarial glasses 1](images/adversarial/adversarial-glasses.1.png) -->\n",
    "![Adversarial glasses 1](https://raw.githubusercontent.com/jchwenger/AI/main/lectures/01.more/images/adversarial/adversarial-glasses.1.png)\n",
    "\n",
    "[Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, Michael K. Reiter, *A General Framework for Adversarial Examples with Objectives*](https://arxiv.org/abs/1801.00349)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d233235",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- ![Adversarial glasses 2](images/adversarial/adversarial-glasses.2.png) -->\n",
    "![Adversarial glasses 2](https://raw.githubusercontent.com/jchwenger/AI/main/lectures/01.more/images/adversarial/adversarial-glasses.2.png)\n",
    "\n",
    "[Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, Michael K. Reiter, *A General Framework for Adversarial Examples with Objectives*](https://arxiv.org/abs/1801.00349)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0c729",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- ![Adversarial typographic attack 1](images/adversarial/adversarial-typography.1.png) -->\n",
    "![Adversarial typographic attack 1](https://raw.githubusercontent.com/jchwenger/AI/main/lectures/01.more/images/adversarial/adversarial-typography.1.png)\n",
    "\n",
    "<!-- ![Adversarial typographic attack 2](images/adversarial/adversarial-typography.2.png) -->\n",
    "![Adversarial typographic attack 2](https://raw.githubusercontent.com/jchwenger/AI/main/lectures/01.more/images/adversarial/adversarial-typography.2.png)\n",
    "\n",
    "[Gabriel Goh, Chelsea Voss, Daniela Amodei, Shan Carter, Michael Petrov, Justin Jay Wang, Nick Cammarata & Chris Olah, *Multimodal Neurons in Artificial Neural Networks*, OpenAI blog](https://openai.com/blog/multimodal-neurons/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95325514",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## NLP model hacking\n",
    "\n",
    "([The video of the course does not allow embedding on external sites](https://www.youtube.com/watch?v=Cgago6NNf7Y&list=PLWnsVgP6CzadI4-FT2Po4wsEK7MHCIQ-d&index=21&t=2463s)).\n",
    "\n",
    "It is possible to *extract* knowledge from a model by querying its public api with random sentences, getting the results, and training a similar net on these pairs (random query → API response)!\n",
    "\n",
    "It even works for translation: send **random** sentences in language X to Google Translate, get the (random?) translations into language Y, then train on these pairs, and lo and behold!, if you query your pirate model with **real** sentences from language X, the model should work reasonably well to give you translations into language Y.\n",
    "\n",
    "Even if the inputs are random to us, the relationship between these inputs and the API outputs somehow contain the knowledge the network has learnt when trained on the original datasets.\n",
    "\n",
    "There does not seem to be any real countermeasure against yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e067be55-bd1f-444e-b388-aee684c6bed2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8111d8-8583-4901-873a-c272cf2ace7a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Ilya Sutskever - GPT-2](https://www.youtube.com/watch?v=T0I88NhR_9M&start=1431)  \n",
    "[DALL·E 2 Explained 2'47](https://www.youtube.com/watch?v=qTgPSKKjfVg)  \n",
    "[AlphaFold (BERT?)](https://www.youtube.com/watch?v=gg7WjuFs8F4)  \n",
    "[DeepMind protein folding](https://www.youtube.com/watch?v=KpedmJdrTpY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4586742d-4dd7-43e6-8c08-a4a3facabb98",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd25682-030b-45fa-95d0-2d9be96d30fd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Stable Diffusion Img2Img x EbSynth x Koe Recast TEST](https://www.youtube.com/watch?v=sP3u09_YFxk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be7502-297f-4e9f-a8a8-bcdc0fddb776",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ad4c7-697e-4d53-85ec-35455f1ba8bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[AlphaGo Official Trailer](https://www.youtube.com/watch?v=8tq1C8spV_g)  \n",
    "[AlphaGo The Movie](https://www.youtube.com/watch?v=WXuK6gekU1Y)  \n",
    "[AlphaStar Official Trailer](https://www.youtube.com/watch?v=WXuK6gekU1Y)  \n",
    "[AlphaStar Demonstration](https://www.youtube.com/watch?v=cUTMhmVh1qs)  \n",
    "[Solving Rubik's Cube with a Robot Hand: Uncut 4'09](https://www.youtube.com/watch?v=kVmp0uGtShk)  \n",
    "[Emergence of Locomotion Behaviours in Rich Environments](https://www.youtube.com/watch?v=hx_bgoTF7bs)  \n",
    "[Flexible Muscle-Based Locomotion for Bipedal Creatures](https://www.youtube.com/watch?v=pgaEE27nsQw)  \n",
    "[Learning to Walk via Deep Reinforcement Learning](https://www.youtube.com/watch?v=n2gE7n11h1Y)  \n",
    "[DQN Breakout 1'13](https://www.youtube.com/watch?v=TmPfTpjtdgg)  \n",
    "[Multi-Agent Hide and Seek 2'58](https://www.youtube.com/watch?v=kopoLzvh5jY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750b6667-3621-48b1-b1f7-52bf7ed46bd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Some alarm bells..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84b0468-9328-4b0f-aeed-c2f0b23699d5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[DARPA's AI vs. Human in Virtual F-16 Aerial Dogfight (FINALS) ](https://www.youtube.com/watch?v=IOJhgC1ksNU)  \n",
    "[NOT AI: Boston Dynamics, Atlas | Partners in Parkour](https://www.youtube.com/watch?v=tF4DML7FIWk)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
